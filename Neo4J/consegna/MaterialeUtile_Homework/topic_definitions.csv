Topic,Count,Name,Representation,Representative_Docs
-1,2518,-1_the_of_and_to,"['the', 'of', 'and', 'to', 'in', 'we', 'is', 'for', 'that', 'with']","['The design of algorithms that leverage machine learning alongside\ncombinatorial optimization techniques is a young but thriving area of\noperations research. If trends emerge, the literature has still not converged\non the proper way of combining these two techniques or on the predictor\narchitectures that should be used. We focus on operations research problems for\nwhich no efficient algorithms are known, but that are variants of classic\nproblems for which ones efficient algorithm exist. Elaborating on recent\ncontributions that suggest using a machine learning predictor to approximate\nthe variant by the classic problem, we introduce the notion of structured\napproximation of an operations research problem by another. We provide a\ngeneric learning algorithm to fit these approximations. This algorithm requires\nonly instances of the variant in the training set, unlike previous learning\nalgorithms that also require the solution of these instances. Using tools from\nstatistical learning theory, we prove a result showing the convergence speed of\nthe estimator, and deduce an approximation ratio guarantee on the performance\nof the algorithm obtained for the variant. Numerical experiments on a single\nmachine scheduling and a stochastic vehicle scheduling problem from the\nliterature show that our learning algorithm is competitive with algorithms that\nhave access to optimal solutions, leading to state-of-the-art algorithms for\nthe variant considered.', ""Spreading processes on graphs arise in a host of application domains, from\nthe study of online social networks to viral marketing to epidemiology. Various\ndiscrete-time probabilistic models for spreading processes have been proposed.\nThese are used for downstream statistical estimation and prediction problems,\noften involving messages or other information that is transmitted along with\ninfections caused by the process. It is thus important to design models of\ncascade observation that take into account phenomena that lead to uncertainty\nabout the process state at any given time. We highlight one such phenomenon --\ntemporal distortion -- caused by a misalignment between the rate at which\nobservations of a cascade process are made and the rate at which the process\nitself operates, and argue that failure to correct for it results in\ndegradation of performance on downstream statistical tasks. To address these\nissues, we formulate the clock estimation problem in terms of a natural\ndistortion measure. We give a clock estimation algorithm, which we call\nFastClock, that runs in linear time in the size of its input and is provably\nstatistically accurate for a broad range of model parameters when cascades are\ngenerated from the independent cascade process with known parameters and when\nthe underlying graph is Erd\\H{o}s-R\\'enyi. We further give empirical results on\nthe performance of our algorithm in comparison to the state of the art\nestimator, a likelihood proxy maximization-based estimator implemented via\ndynamic programming. We find that, in a broad parameter regime, our algorithm\nsubstantially outperforms the dynamic programming algorithm in terms of both\nrunning time and accuracy."", 'Topology Optimization is the process of finding the optimal arrangement of\nmaterials within a design domain by minimizing a cost function, subject to some\nperformance constraints. Robust topology optimization (RTO) also incorporates\nthe effect of input uncertainties and produces a design with the best average\nperformance of the structure while reducing the response sensitivity to input\nuncertainties. It is computationally expensive to carry out RTO using finite\nelement and Monte Carlo sampling. In this work, we use neural network\nsurrogates to enable a faster solution approach via surrogate-based\noptimization and build a Variational Autoencoder (VAE) to transform the the\nhigh dimensional design space into a low dimensional one. Furthermore, finite\nelement solvers will be replaced by a neural network surrogate. Also, to\nfurther facilitate the design exploration, we limit our search to a subspace,\nwhich consists of designs that are solutions to deterministic topology\noptimization problems under different realizations of input uncertainties. With\nthese neural network approximations, a gradient-based optimization approach is\nformed to minimize the predicted objective function over the low dimensional\ndesign subspace. We demonstrate the effectiveness of the proposed approach on\ntwo compliance minimization problems and show that VAE performs well on\nlearning the features of the design from minimal training data, and that\nconverting the design space into a low dimensional latent space makes the\nproblem computationally efficient. The resulting gradient-based optimization\nalgorithm produces optimal designs with lower robust compliances than those\nobserved in the training set.']"
0,282,0_language_dialogue_knowledge_models,"['language', 'dialogue', 'knowledge', 'models', 'text', 'pre', 'tasks', 'model', 'task', 'question']","['Knowledge enriched language representation learning has shown promising\nperformance across various knowledge-intensive NLP tasks. However, existing\nknowledge based language models are all trained with monolingual knowledge\ngraph data, which limits their application to more languages. In this work, we\npresent a novel framework to pretrain knowledge based multilingual language\nmodels (KMLMs). We first generate a large amount of code-switched synthetic\nsentences and reasoning-based multilingual training data using the Wikidata\nknowledge graphs. Then based on the intra- and inter-sentence structures of the\ngenerated data, we design pretraining tasks to facilitate knowledge learning,\nwhich allows the language models to not only memorize the factual knowledge but\nalso learn useful logical patterns. Our pretrained KMLMs demonstrate\nsignificant performance improvements on a wide range of knowledge-intensive\ncross-lingual NLP tasks, including named entity recognition, factual knowledge\nretrieval, relation classification, and a new task designed by us, namely,\nlogic reasoning. Our code and pretrained language models will be made publicly\navailable.', 'Incorporating factual knowledge into pre-trained language models (PLM) such\nas BERT is an emerging trend in recent NLP studies. However, most of the\nexisting methods combine the external knowledge integration module with a\nmodified pre-training loss and re-implement the pre-training process on the\nlarge-scale corpus. Re-pretraining these models is usually resource-consuming,\nand difficult to adapt to another domain with a different knowledge graph (KG).\nBesides, those works either cannot embed knowledge context dynamically\naccording to textual context or struggle with the knowledge ambiguity issue. In\nthis paper, we propose a novel knowledge-aware language model framework based\non fine-tuning process, which equips PLM with a unified knowledge-enhanced text\ngraph that contains both text and multi-relational sub-graphs extracted from\nKG. We design a hierarchical relational-graph-based message passing mechanism,\nwhich can allow the representations of injected KG and text to mutually update\neach other and can dynamically select ambiguous mentioned entities that share\nthe same text. Our empirical results show that our model can efficiently\nincorporate world knowledge from KGs into existing language models such as\nBERT, and achieve significant improvement on the machine reading comprehension\n(MRC) task compared with other knowledge-enhanced models.', 'The Transformer architecture and transfer learning have marked a quantum leap\nin natural language processing, improving the state of the art across a range\nof text-based tasks. This paper examines how these advancements can be applied\nto and improve code search. To this end, we pre-train a BERT-based model on\ncombinations of natural language and source code data and evaluate it on pairs\nof StackOverflow question titles and code answers. Our results show that the\npre-trained models consistently outperform the models that were not\npre-trained. In cases where the model was pre-trained on natural language ""and""\nsource code data, it also outperforms an information retrieval baseline based\non Lucene. Also, we demonstrated that combined use of an information\nretrieval-based approach followed by a Transformer, leads to the best results\noverall, especially when searching into a large search pool. Furthermore,\ntransfer learning is particularly effective when much pre-training data is\navailable and fine-tuning data is limited. We demonstrate that natural language\nprocessing models based on the Transformer architecture can be directly applied\nto source code analysis tasks, such as code search. With the development of\nTransformer models designed more specifically for dealing with source code\ndata, we believe the results on source code analysis tasks can be further\nimproved.']"
1,154,1_control_controller_system_feedback,"['control', 'controller', 'system', 'feedback', 'nonlinear', 'linear', 'systems', 'time', 'observer', 'dynamics']","['This paper investigates the distributed optimal output consensus problem of\nsecond-order uncertain nonlinear multi-agent systems over weight-unbalanced\ndirected networks. Under the standard assumption that local cost functions are\nstrongly convex with globally Lipschitz gradients, a novel distributed dynamic\nstate feedback controller is developed such that the outputs of all the agents\nreach the optimal solution to minimize the global cost function which is the\nsum of all the local cost functions. The controller design is based on a\ntwo-layer strategy, where a distributed optimal coordinator and a\nreference-tracking controller are proposed to address the challenges arising\nfrom unbalanced directed networks and uncertain nonlinear functions\nrespectively. A key feature of the proposed controller is that the nonlinear\nfunctions containing the uncertainties and disturbances are not required to be\nglobally Lipschitz. Furthermore, by exploiting adaptive control technique, no\nprior knowledge of the uncertainties or disturbances is required either. Two\nsimulation examples are finally provided to illustrate the effectiveness of the\nproposed control scheme.', 'One of the most important branches of nonlinear control theory is the\nso-called sliding-mode. Its aim is the design of a (nonlinear) feedback law\nthat brings and maintains the state trajectory of a dynamic system on a given\nsliding surface. Here, dynamics becomes completely independent of the model\nparameters and can be tuned accordingly to the desired target. In this paper we\nstudy this problem when the feedback law is subject to strong structural\nconstraints. In particular, we assume that the control input may take values\nonly over two bounded and disjoint sets. Such sets could be also non perfectly\nknown a priori. An example is a control input allowed to switch only between\ntwo values. Under these peculiarities, we derive the necessary and sufficient\nconditions that guarantee sliding-mode control effectiveness for a class of\ntime-varying continuous-time linear systems that includes all the stationary\nstate-space linear models. Our analysis covers several scientific fields. It is\nonly apparently confined to the linear setting and allows also to study an\nimportant set of nonlinear models. We describe fundamental examples related to\nepidemiology where the control input is the level of contact rate among people\nand the sliding surface permits to control the number of infected. For popular\nepidemiological models we prove the global convergence of control schemes based\non the introduction of severe restrictions, like lockdowns, to contain\nepidemic. This greatly generalizes previous results obtained in the literature\nby casting them within a general sliding-mode theory.', 'In this note, a novel observer-based output feedback control approach is\nproposed to address the distributed optimal output consensus problem of\nuncertain nonlinear multi-agent systems in the normal form over unbalanced\ndirected graphs. The main challenges of the concerned problem lie in unbalanced\ndirected graphs and nonlinearities of multi-agent systems with their agent\nstates not available for feedback control. Based on a two-layer controller\nstructure, a distributed optimal coordinator is first designed to convert the\nconsidered problem into a reference-tracking problem. Then a decentralized\noutput feedback controller is developed to stabilize the resulting augmented\nsystem. A high-gain observer is exploited in controller design to estimate the\nagent states in the presence of uncertainties and disturbances so that the\nproposed controller relies only on agent outputs. The semi-global convergence\nof the agent outputs toward the optimal solution that minimizes the sum of all\nlocal cost functions is proved under standard assumptions. A key feature of the\nobtained results is that the nonlinear agents under consideration are only\nrequired to be locally Lipschitz and possess globally asymptotically stable and\nlocally exponentially stable zero dynamics.']"
2,138,2_speech_speaker_audio_music,"['speech', 'speaker', 'audio', 'music', 'asr', 'voice', 'model', 'recognition', 'acoustic', 'to']","['End-to-end intent classification using speech has numerous advantages\ncompared to the conventional pipeline approach using automatic speech\nrecognition (ASR), followed by natural language processing modules. It attempts\nto predict intent from speech without using an intermediate ASR module.\nHowever, such end-to-end framework suffers from the unavailability of large\nspeech resources with higher acoustic variation in spoken language\nunderstanding. In this work, we exploit the scope of the transformer\ndistillation method that is specifically designed for knowledge distillation\nfrom a transformer based language model to a transformer based speech model. In\nthis regard, we leverage the reliable and widely used bidirectional encoder\nrepresentations from transformers (BERT) model as a language model and transfer\nthe knowledge to build an acoustic model for intent classification using the\nspeech. In particular, a multilevel transformer based teacher-student model is\ndesigned, and knowledge distillation is performed across attention and hidden\nsub-layers of different transformer layers of the student and teacher models.\nWe achieve an intent classification accuracy of 99.10% and 88.79% for Fluent\nspeech corpus and ATIS database, respectively. Further, the proposed method\ndemonstrates better performance and robustness in acoustically degraded\ncondition compared to the baseline method.', 'We present a direct speech-to-speech translation (S2ST) model that translates\nspeech from one language to speech in another language without relying on\nintermediate text generation. Previous work addresses the problem by training\nan attention-based sequence-to-sequence model that maps source speech\nspectrograms into target spectrograms. To tackle the challenge of modeling\ncontinuous spectrogram features of the target speech, we propose to predict the\nself-supervised discrete representations learned from an unlabeled speech\ncorpus instead. When target text transcripts are available, we design a\nmultitask learning framework with joint speech and text training that enables\nthe model to generate dual mode output (speech and text) simultaneously in the\nsame inference pass. Experiments on the Fisher Spanish-English dataset show\nthat predicting discrete units and joint speech and text training improve model\nperformance by 11 BLEU compared with a baseline that predicts spectrograms and\nbridges 83% of the performance gap towards a cascaded system. When trained\nwithout any text transcripts, our model achieves similar performance as a\nbaseline that predicts spectrograms and is trained with text data.', 'This study aims at designing an environment-aware text-to-speech (TTS) system\nthat can generate speech to suit specific acoustic environments. It is also\nmotivated by the desire to leverage massive data of speech audio from\nheterogeneous sources in TTS system development. The key idea is to model the\nacoustic environment in speech audio as a factor of data variability and\nincorporate it as a condition in the process of neural network based speech\nsynthesis. Two embedding extractors are trained with two purposely constructed\ndatasets for characterization and disentanglement of speaker and environment\nfactors in speech. A neural network model is trained to generate speech from\nextracted speaker and environment embeddings. Objective and subjective\nevaluation results demonstrate that the proposed TTS system is able to\neffectively disentangle speaker and environment factors and synthesize speech\naudio that carries designated speaker characteristics and environment\nattribute. Audio samples are available online for demonstration\nhttps://daxintan-cuhk.github.io/Environment-Aware-TTS/ .']"
3,136,3_privacy_fl_federated_learning,"['privacy', 'fl', 'federated', 'learning', 'server', 'data', 'private', 'clients', 'communication', 'distributed']","[""Federated Learning (FL) allows edge devices (or clients) to keep data locally\nwhile simultaneously training a shared high-quality global model. However,\ncurrent research is generally based on an assumption that the training data of\nlocal clients have ground-truth. Furthermore, FL faces the challenge of\nstatistical heterogeneity, i.e., the distribution of the client's local\ntraining data is non-independent identically distributed (non-IID). In this\npaper, we present a robust semi-supervised FL system design, where the system\naims to solve the problem of data availability and non-IID in FL. In\nparticular, this paper focuses on studying the labels-at-server scenario where\nthere is only a limited amount of labeled data on the server and only unlabeled\ndata on the clients. In our system design, we propose a novel method to tackle\nthe problems, which we refer to as Federated Mixing (FedMix). FedMix improves\nthe naive combination of FL and semi-supervised learning methods and designs\nparameter decomposition strategies for disjointed learning of labeled,\nunlabeled data, and global models. To alleviate the non-IID problem, we propose\na novel aggregation rule based on the frequency of the client's participation\nin training, namely the FedFreq aggregation algorithm, which can adjust the\nweight of the corresponding local model according to this frequency. Extensive\nevaluations conducted on CIFAR-10 dataset show that the performance of our\nproposed method is significantly better than those of the current baseline. It\nis worth noting that our system is robust to different non-IID levels of client\ndata."", 'Federated learning (FL) is recognized as a key enabling technology to support\ndistributed artificial intelligence (AI) services in future 6G. By supporting\ndecentralized data training and collaborative model training among devices, FL\ninherently tames privacy leakage and reduces transmission costs. Whereas, the\nperformance of the wireless FL is typically restricted by the communication\nlatency. Multiple-input multiple-output (MIMO) technique is one promising\nsolution to build up a communication-efficient edge FL system with limited\nradio resources. In this paper, we propose a novel joint device scheduling and\nreceive beamforming design approach to reduce the FL convergence gap over\nshared wireless MIMO networks. Specifically, we theoretically establish the\nconvergence analysis of the FL process, and then apply the proposed device\nscheduling policy to maximize the number of weighted devices under the FL\nsystem latency and sum power constraints. Numerical results verify the\ntheoretical analysis of the FL convergence and exhibit the appealing learning\nperformance of the proposed approach.', ""Federated Learning (FL) allows multiple participating clients to train\nmachine learning models collaboratively by keeping their datasets local and\nonly exchanging model updates. Existing FL protocol designs have been shown to\nbe vulnerable to attacks that aim to compromise data privacy and/or model\nrobustness. Recently proposed defenses focused on ensuring either privacy or\nrobustness, but not both. In this paper, we develop a framework called PRECAD,\nwhich simultaneously achieves differential privacy (DP) and enhances robustness\nagainst model poisoning attacks with the help of cryptography. Using secure\nmulti-party computation (MPC) techniques (e.g., secret sharing), noise is added\nto the model updates by the honest-but-curious server(s) (instead of each\nclient) without revealing clients' inputs, which achieves the benefit of\ncentralized DP in terms of providing a better privacy-utility tradeoff than\nlocal DP based solutions. Meanwhile, a crypto-aided secure validation protocol\nis designed to verify that the contribution of model update from each client is\nbounded without leaking privacy. We show analytically that the noise added to\nensure DP also provides enhanced robustness against malicious model\nsubmissions. We experimentally demonstrate that our PRECAD framework achieves\nhigher privacy-utility tradeoff and enhances robustness for the trained models.""]"
4,133,4_galaxy_galaxies_survey_redshift,"['galaxy', 'galaxies', 'survey', 'redshift', 'stars', 'stellar', 'mass', 'of', 'surveys', 'the']","['Galaxy redshift surveys are designed to map cosmic structures in three\ndimensions for large-scale structure studies. Nevertheless, limitations due to\nsampling and the survey window are unavoidable and degrade the cosmological\nconstraints. We present an analysis of the VIMOS Public Extragalactic Redshift\nSurvey (VIPERS) over the redshift range $0.6 < z < 1$ that is optimised to\nextract the cosmological parameters while fully accounting for the complex\nsurvey geometry. We employ the Gibbs sampling algorithm to iteratively draw\nsamples of the galaxy density field in redshift space, the galaxy bias, the\nmatter density, baryon fraction and growth-rate parameter $f\\sigma_8$ based on\na multivariate Gaussian likelihood and prior on the density field. Despite the\nhigh number of degrees of freedom, the samples converge to the joint posterior\ndistribution and give self-consistent constraints on the model parameters. We\nvalidate the approach using VIPERS mock galaxy catalogues. Although the\nuncertainty is underestimated by the Gaussian likelihood on the scales that we\nconsider by $50\\%$, the dispersion of the results from the mock catalogues\ngives a robust error estimate. We find that the precision of the results\nmatches those of the traditional analyses applied to the VIPERS data that use\nmore constrained models. By relaxing the model assumptions, we confirm that the\ndata deliver consistent constraints on the $\\Lambda$CDM model. This work\nprovides a case-study for the application of maximum-likelihood analyses for\nthe next generation of galaxy redshift surveys.', 'We analyze the clustering of galaxies using the z=1.006 snapshot of the\nCosmoDC2 simulation, a high-fidelity synthetic galaxy catalog designed to\nvalidate analysis methods for the Large Synoptic Survey Telescope (LSST). We\npresent sub-percent measurements of the galaxy auto-correlation and galaxy-dark\nmatter cross correlations in Fourier space and configuration space for a\nmagnitude-limited galaxy sample. At these levels of precision, the statistical\nerrors of the measurement are comparable to the systematic effects present in\nthe simulation and measurement procedure; nevertheless, using a hybrid-PT\nmodel, we are able to model non-linear galaxy bias with 0.5% precision up to\nscales of $k_{\\rm max}= 0.5 \\ h/{\\rm Mpc}$ and $r_{\\rm min}= 4 \\ {\\rm Mpc}/h$.\nWhile the linear bias parameter is measured with 0.01% precision, other bias\nparameters are determined with considerably weaker constraints and sometimes\nbimodal posterior distributions. We compare our fiducial model with lower\ndimensional models where the higher-order bias parameters are fixed at their\nco-evolution values and find that leaving these parameters free provides\nsignificant improvements in our ability to model small scale information. We\nalso compare bias parameters for galaxy samples defined using different\nmagnitude bands and find agreement between samples containing equal numbers of\ngalaxies. Finally, we compare bias parameters between Fourier space and\nconfiguration space and find moderate tension between the two approaches.\nAlthough our model is often unable to fit the CosmoDC2 galaxy samples within\nthe 0.1% precision of our measurements, our results suggest that the hybrid-PT\nmodel used in this analysis is capable of modeling non-linear galaxy bias\nwithin the percent level precision needed for upcoming galaxy surveys.', 'The calibration and validation of scientific analysis in simulations is a\nfundamental tool to ensure unbiased and robust results in observational\ncosmology. In particular, mock galaxy catalogs are a crucial resource to\nachieve these goals in the measurement of Baryon Acoustic Oscillations (BAO) in\nthe clustering of galaxies. Here we present a set of 1952 galaxy mock catalogs\ndesigned to mimic the Dark Energy Survey (DES) Year 3 BAO sample over its full\nphotometric redshift range $0.6 < z_{\\rm photo} < 1.1$. The mocks are based\nupon 488 ICE-COLA fast $N$-body simulations of full-sky light-cones and are\ncreated by populating halos with galaxies, using a hybrid Halo Occupation\nDistribution - Halo Abundance Matching model. This model has 10 free\nparameters, which are determined, for the first time, using an automatic\nlikelihood minimization procedure. We also introduce a novel technique to\nassign photometric redshift for simulated galaxies, following a two-dimensional\nprobability distribution with VIMOS Public Extragalactic Redshift Survey\n(VIPERS) data. The calibration was designed to match the observed abundance of\ngalaxies as a function of photometric redshift, the distribution of photometric\nredshift errors, and the clustering amplitude on scales smaller than those used\nfor BAO measurements. An exhaustive analysis is done to ensure that the mocks\nreproduce the input properties. Finally, mocks are tested by comparing the\nangular correlation function $w(\\theta)$, angular power spectrum $C_\\ell$ and\nprojected clustering $\\xi_p(r_\\perp)$ to theoretical predictions and data. The\nsuccess in reproducing accurately the photometric redshift uncertainties and\nthe galaxy clustering as a function of redshift render this mock creation\npipeline as a benchmark for future analyses of photometric galaxy surveys.']"
5,124,5_origami_particles_surface_of,"['origami', 'particles', 'surface', 'of', 'water', 'assembly', 'structures', 'dna', 'the', 'self']","['Traditionally, origami has been categorized into two groups according to\ntheir kinematics design: rigid and non-rigid origami. However, such\ncategorization can be superficial, and rigid origami can obtain new mechanical\nproperties by intentionally relaxing the rigid-folding kinematics. Based on\nnumerical simulations using the bar-hinge approach and experiments, this study\nexamines the multi-stability of a stacked Miura-origami cellular structure with\ndifferent levels of facet compliance. The simulation and experiment results\nshow that a unit cell in such cellular solid exhibits only two stable states if\nit follows the rigid origami kinematics; however, two more stable states are\nreachable if the origami facets become sufficiently compliant. Moreover, the\nswitch between two certain stable states shows an asymmetric energy barrier,\nmeaning that the unit cell follows fundamentally different deformation paths\nwhen it extends from one state to another compared to the opposite compression\nswitch. As a result, the reaction force required for extending this unit cell\nbetween these two states can be higher than the compression switch. Such\nasymmetric multi-stability can be fine-tuned by tailoring the underlying\norigami design, and it can be extended into cellular solids with carefully\nplaced voids. By showing the benefits of exploiting facet compliance, this\nstudy could foster multi-functional structures and material systems that\ntraditional rigid origami cannot create.', 'Colloidal molecules are designed to mimic their molecular analogues through\ntheir anisotropic shape and interactions. However, current experimental\nrealizations are missing the structural flexibility present in real molecules\nthereby restricting their use as model systems. We overcome this limitation by\nassembling reconfigurable colloidal molecules from silica particles\nfunctionalized with mobile DNA linkers in high yields. We achieve this by\nsteering the self-assembly pathway towards the formation of finite-sized\nclusters by employing high number ratios of particles functionalized with\ncomplementary DNA strands. The size ratio of the two species of particles\nprovides control over the overall cluster size, or, ""valence"", as well as the\ndegree of reconfigurability. The bond flexibility provided by the mobile\nlinkers allows the successful assembly of colloidal clusters with the\ngeometrically expected maximum valence. We quantitatively examine the\nself-assembly dynamics of these flexible colloidal molecules by a combination\nof experiments, molecular dynamics simulations, and an analytical model. Our\n""flexible colloidal molecules"" are exciting building blocks for investigating\nand exploiting the self-assembly of complex hierarchical structures, photonic\ncrystals and colloidal meta-materials.', ""Just like atoms combine into molecules, colloids can self-organize into\npredetermined structures according to a set of design principles. Controlling\nvalence -- the number of inter-particle bonds -- is a prerequisite for the\nassembly of complex architectures. The assembly can be directed via solid\n`patchy' particles with prescribed geometries to make, for example, a colloidal\ndiamond. We demonstrate here that the nanoscale ordering of individual\nmolecular linkers can combine to program the structure of microscopic\nassemblies. Specifically, we experimentally show that covering initially\nisotropic microdroplets with $N$ mobile DNA linkers results in spontaneous and\nreversible self-organization of the DNA into $Z(N)$ binding patches, selecting\na predictable valence. We understand this valence thermodynamically, deriving a\nfree energy functional for droplet-droplet adhesion that accurately predicts\nthe equilibrium size of and molecular organization within patches, as well as\nthe observed valence transitions with $N$. Thus, microscopic self-organization\ncan be programmed by choosing the molecular properties and concentration of\nbinders. These results are widely applicable to the assembly of any particle\nwith mobile linkers, such as functionalized liposomes or protein interactions\nin cell-cell adhesion.""]"
6,120,6_graphs_problem_graph_log,"['graphs', 'problem', 'graph', 'log', 'algorithm', 'time', 'codes', 'polynomial', 'tree', 'distance']","['We first design an $\\mathcal{O}(n^2)$ solution for finding a maximum induced\nmatching in permutation graphs given their permutation models, based on a\ndynamic programming algorithm with the aid of the sweep line technique. With\nthe support of the disjoint-set data structure, we improve the complexity to\n$\\mathcal{O}(m + n)$. Consequently, we extend this result to give an\n$\\mathcal{O}(m + n)$ algorithm for the same problem in trapezoid graphs. By\ncombining our algorithms with the current best graph identification algorithms,\nwe can solve the MIM problem in permutation and trapezoid graphs in linear and\n$\\mathcal{O}(n^2)$ time, respectively. Our results are far better than the best\nknown $\\mathcal{O}(mn)$ algorithm for the maximum induced matching problem in\nboth graph classes, which was proposed by Habib et al.', 'We consider a classical scheduling problem on $m$ identical machines. For an\narbitrary constant $q>1$, the aim is to assign jobs to machines such that\n$\\sum_{i=1}^m C_i^q$ is minimized, where $C_i$ is the total processing time of\njobs assigned to machine $i$. It is well known that this problem is strongly\nNP-hard.\n  Under mild assumptions, the running time of an $(1+\\epsilon)$-approximation\nalgorithm for a strongly NP-hard problem cannot be polynomial on $1/\\epsilon$,\nunless $\\text{P}=\\text{NP}$. For most problems in the literature, this\ntranslates into algorithms with running time at least as large as\n$2^{\\Omega(1/\\varepsilon)}+n^{O(1)}$. For the natural scheduling problem above,\nwe establish the existence of an algorithm which violates this threshold. More\nprecisely, we design a PTAS that runs in\n$2^{\\tilde{O}(\\sqrt{1/\\epsilon})}+n^{O(1)}$ time. This result is in sharp\ncontrast to the closely related minimum makespan variant, where an exponential\nlower bound is known under the exponential time hypothesis (ETH). We complement\nour result with an essentially matching lower bound on the running time,\nshowing that our algorithm is best-possible under ETH. The lower bound proof\nexploits new number-theoretical constructions for variants of progression-free\nsets, which might be of independent interest.\n  Furthermore, we provide a fine-grained characterization on the running time\nof a PTAS for this problem depending on the relation between $\\epsilon$ and the\nnumber of machines $m$. More precisely, our lower bound only holds when\n$m=\\Theta(\\sqrt{1/\\epsilon})$. Better algorithms, that go beyond the lower\nbound, exist for other values of $m$. In particular, there even exists an\nalgorithm with running time polynomial in $1/\\epsilon$ if we restrict ourselves\nto instances with $m=\\Omega(1/\\epsilon\\log^21/\\epsilon)$.', 'The maximum independent set problem is one of the most important problems in\ngraph algorithms and has been extensively studied in the line of research on\nthe worst-case analysis of exact algorithms for NP-hard problems. In the\nweighted version, each vertex in the graph is associated with a weight and we\nare going to find an independent set of maximum total vertex weight. In this\npaper, we design several reduction rules and a fast exact algorithm for the\nmaximum weighted independent set problem, and use the measure-and-conquer\ntechnique to analyze the running time bound of the algorithm. Our algorithm\nworks on general weighted graphs and it has a good running time bound on sparse\ngraphs. If the graph has an average degree at most 3, our algorithm runs in\n$O^*(1.1443^n)$ time and polynomial space, improving previous running time\nbounds for the problem in cubic graphs using polynomial space.']"
7,107,7_adversarial_attacks_attack_robustness,"['adversarial', 'attacks', 'attack', 'robustness', 'malware', 'image', 'backdoor', 'examples', 'against', 'robust']","['Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nvarious graph structure related tasks such as node classification and graph\nclassification. However, GNNs are vulnerable to adversarial attacks. Existing\nworks mainly focus on attacking GNNs for node classification; nevertheless, the\nattacks against GNNs for graph classification have not been well explored.\n  In this work, we conduct a systematic study on adversarial attacks against\nGNNs for graph classification via perturbing the graph structure. In\nparticular, we focus on the most challenging attack, i.e., hard label black-box\nattack, where an attacker has no knowledge about the target GNN model and can\nonly obtain predicted labels through querying the target model.To achieve this\ngoal, we formulate our attack as an optimization problem, whose objective is to\nminimize the number of edges to be perturbed in a graph while maintaining the\nhigh attack success rate. The original optimization problem is intractable to\nsolve, and we relax the optimization problem to be a tractable one, which is\nsolved with theoretical convergence guarantee. We also design a coarse-grained\nsearching algorithm and a query-efficient gradient computation algorithm to\ndecrease the number of queries to the target GNN model. Our experimental\nresults on three real-world datasets demonstrate that our attack can\neffectively attack representative GNNs for graph classification with less\nqueries and perturbations. We also evaluate the effectiveness of our attack\nunder two defenses: one is well-designed adversarial graph detector and the\nother is that the target GNN model itself is equipped with a defense to prevent\nadversarial graph generation. Our experimental results show that such defenses\nare not effective enough, which highlights more advanced defenses.', 'Recent work shows that deep neural networks are vulnerable to adversarial\nexamples. Much work studies adversarial example generation, while very little\nwork focuses on more critical adversarial defense. Existing adversarial\ndetection methods usually make assumptions about the adversarial example and\nattack method (e.g., the word frequency of the adversarial example, the\nperturbation level of the attack method). However, this limits the\napplicability of the detection method. To this end, we propose TREATED, a\nuniversal adversarial detection method that can defend against attacks of\nvarious perturbation levels without making any assumptions. TREATED identifies\nadversarial examples through a set of well-designed reference models. Extensive\nexperiments on three competitive neural networks and two widely used datasets\nshow that our method achieves better detection performance than baselines. We\nfinally conduct ablation studies to verify the effectiveness of our method.', 'In spite of the enormous success of neural networks, adversarial examples\nremain a relatively weakly understood feature of deep learning systems. There\nis a considerable effort in both building more powerful adversarial attacks and\ndesigning methods to counter the effects of adversarial examples. We propose a\nmethod to transform the adversarial input data through a mixture of generators\nin order to recover the correct class obfuscated by the adversarial attack. A\ncanonical set of images is used to generate adversarial examples through\npotentially multiple attacks. Such transformed images are processed by a set of\ngenerators, which are trained adversarially as a whole to compete in inverting\nthe initial transformations. To our knowledge, this is the first use of a\nmixture-based adversarially trained system as a defense mechanism. We show that\nit is possible to train such a system without supervision, simultaneously on\nmultiple adversarial attacks. Our system is able to recover class information\nfor previously-unseen examples with neither attack nor data labels on the MNIST\ndataset. The results demonstrate that this multi-attack approach is competitive\nwith adversarial defenses tested in single-attack settings.']"
8,103,8_ris_irs_channel_mimo,"['ris', 'irs', 'channel', 'mimo', 'beamforming', 'antenna', 'wireless', 'bs', 'intelligent', 'reconfigurable']","[""To mitigate the effects of shadow fading and obstacle blocking,\nreconfigurable intelligent surface (RIS) has become a promising technology to\nimprove the signal transmission quality of wireless communications by\ncontrolling the reconfigurable passive elements with less hardware cost and\nlower power consumption. However, accurate, low-latency and low-pilot-overhead\nchannel state information (CSI) acquisition remains a considerable challenge in\nRIS-assisted systems due to the large number of RIS passive elements. In this\npaper, we propose a three-stage joint channel decomposition and prediction\nframework to require CSI. The proposed framework exploits the two-timescale\nproperty that the base station (BS)-RIS channel is quasi-static and the\nRIS-user equipment (UE) channel is fast time-varying. Specifically, in the\nfirst stage, we use the full-duplex technique to estimate the channel between a\nBS's specific antenna and the RIS, addressing the critical scaling ambiguity\nproblem in the channel decomposition. We then design a novel deep neural\nnetwork, namely, the sparse-connected long short-term memory (SCLSTM), and\npropose a SCLSTM-based algorithm in the second and third stages, respectively.\nThe algorithm can simultaneously decompose the BS-RIS channel and RIS-UE\nchannel from the cascaded channel and capture the temporal relationship of the\nRIS-UE channel for prediction. Simulation results show that our proposed\nframework has lower pilot overhead than the traditional channel estimation\nalgorithms, and the proposed SCLSTM-based algorithm can also achieve more\naccurate CSI acquisition robustly and effectively."", 'This paper investigates the two-timescale transmission design for\nreconfigurable intelligent surface (RIS)-aided massive multiple-input\nmultiple-output (MIMO) systems, where the beamforming at the base station (BS)\nis adapted to the rapidly-changing instantaneous channel state information\n(CSI), while the passive beamforming at the RIS is adapted to the\nslowly-changing statistical CSI.\n  Specifically, we first propose a linear minimum mean square error (LMMSE)\nestimator to obtain the aggregated channel from the users to the BS in each\nchannel coherence interval. Based on the estimated channel, we apply the\nlow-complexity maximal ratio combining (MRC) beamforming at the BS, and then\nderive the ergodic achievable rate in a closed form expression.\n  To draw design insights, we perform a detailed theoretical analysis departing\nfrom the derived ergodic achievable rate. If the BS-RIS channel is Rician\ndistributed, we prove that the transmit power can be scaled proportionally to\n$1/M$, as the number of BS antennas, $M$, grows to infinity while maintaining a\nnon-zero rate.\n  If the BS-RIS channel is Rayleigh distributed, the transmit power can be\nscaled either proportionally to $1/\\sqrt{M}$ as $M$ grows large, or\nproportionally to $1/N$ as the number of reflecting elements, $N$, grows large,\nwhile still maintaining a non-zero rate.\n  By capitalizing on the derived expression of the data rate under the\nstatistical knowledge of the CSI, we maximize the minimum user rate by\ndesigning the passive beamforming at the RIS.\n  Numerical results confirm that, even in the presence of imperfect CSI, the\nintegration of an RIS in massive MIMO systems results in promising performance\ngains. In addition, the obtained results reveal that it is favorable to place\nthe RIS close to the users rather than close to the BS.', 'Reconfigurable intelligent surfaces (RISs) have been proposed recently as new\ntechnology to tune the wireless propagation channels in real-time. However,\nmost of the current works assume single-RIS (S-RIS)-aided systems, which can be\nlimited in some application scenarios where a transmitter might need a\nmulti-RIS-aided channel to communicate with a receiver. In this paper, we\nconsider a double-RIS (D-RIS)-aided MIMO system and propose an alternating\nleast-squared-based channel estimation method by exploiting the Tucker2 tensor\nstructure of the received signals. Using the proposed method, the cascaded MIMO\nchannel parts can be estimated separately, up to trivial scaling factors.\nCompared with the S-RIS systems, we show that if the RIS elements of a S-RIS\nsystem are distributed carefully between the two RISs in a D-RIS system, the\ntraining overhead can be reduced and the estimation accuracy can also be\nincreased. Therefore, D-RIS systems can be seen as an appealing approach to\nfurther increase the coverage, capacity, and efficiency of future wireless\nnetworks compared to S-RIS systems.']"
9,102,9_image_images_3d_style,"['image', 'images', '3d', 'style', 'generative', 'gan', 'scene', 'editing', 'color', 'sketch']","['Recent advances in generative adversarial networks (GANs) have led to\nremarkable achievements in face image synthesis. While methods that use\nstyle-based GANs can generate strikingly photorealistic face images, it is\noften difficult to control the characteristics of the generated faces in a\nmeaningful and disentangled way. Prior approaches aim to achieve such semantic\ncontrol and disentanglement within the latent space of a previously trained\nGAN. In contrast, we propose a framework that a priori models physical\nattributes of the face such as 3D shape, albedo, pose, and lighting explicitly,\nthus providing disentanglement by design. Our method, MOST-GAN, integrates the\nexpressive power and photorealism of style-based GANs with the physical\ndisentanglement and flexibility of nonlinear 3D morphable models, which we\ncouple with a state-of-the-art 2D hair manipulation network. MOST-GAN achieves\nphotorealistic manipulation of portrait images with fully disentangled 3D\ncontrol over their physical attributes, enabling extreme manipulation of\nlighting, facial expression, and pose variations up to full profile view.', 'In many applications of computer graphics, art and design, it is desirable\nfor a user to provide intuitive non-image input, such as text, sketch, stroke,\ngraph or layout, and have a computer system automatically generate\nphoto-realistic images that adhere to the input content. While classic works\nthat allow such automatic image content generation have followed a framework of\nimage retrieval and composition, recent advances in deep generative models such\nas generative adversarial networks (GANs), variational autoencoders (VAEs), and\nflow-based methods have enabled more powerful and versatile image generation\ntasks. This paper reviews recent works for image synthesis given intuitive user\ninput, covering advances in input versatility, image generation methodology,\nbenchmark datasets, and evaluation metrics. This motivates new perspectives on\ninput representation and interactivity, cross pollination between major image\ngeneration paradigms, and evaluation and comparison of generation methods.', 'We present a method for creating 3D indoor scenes with a generative model\nlearned from a collection of semantic-segmented depth images captured from\ndifferent unknown scenes. Given a room with a specified size, our method\nautomatically generates 3D objects in a room from a randomly sampled latent\ncode. Different from existing methods that represent an indoor scene with the\ntype, location, and other properties of objects in the room and learn the scene\nlayout from a collection of complete 3D indoor scenes, our method models each\nindoor scene as a 3D semantic scene volume and learns a volumetric generative\nadversarial network (GAN) from a collection of 2.5D partial observations of 3D\nscenes. To this end, we apply a differentiable projection layer to project the\ngenerated 3D semantic scene volumes into semantic-segmented depth images and\ndesign a new multiple-view discriminator for learning the complete 3D scene\nvolume from 2.5D semantic-segmented depth images. Compared to existing methods,\nour method not only efficiently reduces the workload of modeling and acquiring\n3D scenes for training, but also produces better object shapes and their\ndetailed layouts in the scene. We evaluate our method with different indoor\nscene datasets and demonstrate the advantages of our method. We also extend our\nmethod for generating 3D indoor scenes from semantic-segmented depth images\ninferred from RGB images of real scenes.']"
10,100,10_segmentation_ct_covid_images,"['segmentation', 'ct', 'covid', 'images', 'image', '19', 'medical', 'diagnosis', 'cancer', 'clinical']","['The COVID-19 (coronavirus disease 2019) pandemic affected more than 186\nmillion people with over 4 million deaths worldwide by June 2021. The magnitude\nof which has strained global healthcare systems. Chest Computed Tomography (CT)\nscans have a potential role in the diagnosis and prognostication of COVID-19.\nDesigning a diagnostic system which is cost-efficient and convenient to operate\non resource-constrained devices like mobile phones would enhance the clinical\nusage of chest CT scans and provide swift, mobile, and accessible diagnostic\ncapabilities. This work proposes developing a novel Android application that\ndetects COVID-19 infection from chest CT scans using a highly efficient and\naccurate deep learning algorithm. It further creates an attention heatmap,\naugmented on the segmented lung parenchyma region in the CT scans through an\nalgorithm developed as a part of this work, which shows the regions of\ninfection in the lungs. We propose a selection approach combined with\nmulti-threading for a faster generation of heatmaps on Android Device, which\nreduces the processing time by about 93%. The neural network trained to detect\nCOVID-19 in this work is tested with F1 score and accuracy, both of 99.58% and\nsensitivity of 99.69%, which is better than most of the results in the domain\nof COVID diagnosis from CT scans. This work will be beneficial in high volume\npractices and help doctors triage patients in the early diagnosis of the\nCOVID-19 quickly and efficiently.', 'Significant work has been done towards deep learning (DL) models for\nautomatic lung and lesion segmentation and classification of COVID-19 on chest\nCT data. However, comprehensive visualization systems focused on supporting the\ndual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a\nvisualization application specially tailored for radiologists to diagnose\nCOVID-19 from chest CT data. The system incorporates a complete pipeline of\nautomatic lungs segmentation, localization/ isolation of lung abnormalities,\nfollowed by visualization, visual and DL analysis, and\nmeasurement/quantification tools. Our system combines the traditional 2D\nworkflow of radiologists with newer 2D and 3D visualization techniques with DL\nsupport for a more comprehensive diagnosis. COVID-view incorporates a novel DL\nmodel for classifying the patients into positive/negative COVID-19 cases, which\nacts as a reading aid for the radiologist using COVID-view and provides the\nattention heatmap as an explainable DL for the model output. We designed and\nevaluated COVID-view through suggestions, close feedback and conducting case\nstudies of real-world patient data by expert radiologists who have substantial\nexperience diagnosing chest CT scans for COVID-19, pulmonary embolism, and\nother forms of lung infections. We present requirements and task analysis for\nthe diagnosis of COVID-19 that motivate our design choices and results in a\npractical system which is capable of handling real-world patient cases.', 'Automatic segmentation of infected regions in computed tomography (CT) images\nis necessary for the initial diagnosis of COVID-19. Deep-learning-based methods\nhave the potential to automate this task but require a large amount of data\nwith pixel-level annotations. Training a deep network with annotated lung\ncancer CT images, which are easier to obtain, can alleviate this problem to\nsome extent. However, this approach may suffer from a reduction in performance\nwhen applied to unseen COVID-19 images during the testing phase due to the\ndomain shift. In this paper, we propose a novel unsupervised method for\nCOVID-19 infection segmentation that aims to learn the domain-invariant\nfeatures from lung cancer and COVID-19 images to improve the generalization\nability of the segmentation network for use with COVID-19 CT images. To\novercome the intensity shift, our method first transforms annotated lung cancer\ndata into the style of unlabeled COVID-19 data using an effective augmentation\napproach via a Fourier transform. Furthermore, to reduce the distribution\nshift, we design a teacher-student network to learn rotation-invariant features\nfor segmentation. Experiments demonstrate that even without getting access to\nthe annotations of COVID-19 CT during training, the proposed network can\nachieve a state-of-the-art segmentation performance on COVID-19 images.']"
11,95,11_quantum_circuits_qubits_classical,"['quantum', 'circuits', 'qubits', 'classical', 'qubit', 'circuit', 'algorithm', 'gates', 'gate', 'computing']","['Variational quantum circuits are used in quantum machine learning and\nvariational quantum simulation tasks. Designing good variational circuits or\npredicting how well they perform for given learning or optimization tasks is\nstill unclear. Here we discuss these problems, analyzing variational quantum\ncircuits using the theory of neural tangent kernels. We define quantum neural\ntangent kernels, and derive dynamical equations for their associated loss\nfunction in optimization and learning tasks. We analytically solve the dynamics\nin the frozen limit, or lazy training regime, where variational angles change\nslowly and a linear perturbation is good enough. We extend the analysis to a\ndynamical setting, including quadratic corrections in the variational angles.\nWe then consider hybrid quantum-classical architecture and define a large-width\nlimit for hybrid kernels, showing that a hybrid quantum-classical neural\nnetwork can be approximately Gaussian. The results presented here show limits\nfor which analytical understandings of the training dynamics for variational\nquantum circuits, used for quantum machine learning and optimization problems,\nare possible. These analytical results are supported by numerical simulations\nof quantum machine learning experiments.', 'Quantum computing is one of the most promising technology advances of the\nlatest years. Once only a conceptual idea to solve physics simulations, quantum\ncomputation is today a reality, with numerous machines able to execute quantum\nalgorithms. One of the hardest challenges in quantum computing is reliability.\nQubits are highly sensitive to noise, which can make the output useless.\nMoreover, lately it has been shown that superconducting qubits are extremely\nsusceptible to external sources of faults, such as ionizing radiation. When\nadopted in large scale, radiation-induced errors are expected to become a\nserious challenge for qubits reliability. In this paper, we propose an\nevaluation of the impact of transient faults in the execution of quantum\ncircuits. Inspired by the Architectural and Program Vulnerability Factors,\nwidely adopted to characterize the reliability of classical computing\narchitectures and algorithms, we propose the Quantum Vulnerability Factor (QVF)\nas a metric to measure the impact that the corruption of a qubit has on the\ncircuit output probability distribution. First, we model faults based on the\nlatest studies on real machines and recently performed radiation experiments.\nThen, we design a quantum fault injector, built over Qiskit, and characterize\nthe propagation of faults in quantum circuits. We report the finding of more\nthan 15,000,000 fault injections, evaluating the reliability of three quantum\ncircuits and identifying the faults and qubits that are more likely than others\nto impact the output. With our results, we give guidelines on how to map the\nqubits in the real quantum computer to reduce the output error and to reduce\nthe probability of having a radiation-induced corruption to modify the output.\nFinally, we compare the simulation results with experiments on physical quantum\ncomputers.', 'With the constant increase of the number of quantum bits (qubits) in the\nactual quantum computers, implementing and accelerating the prevalent deep\nlearning on quantum computers are becoming possible. Along with this trend,\nthere emerge quantum neural architectures based on different designs of quantum\nneurons. A fundamental question in quantum deep learning arises: what is the\nbest quantum neural architecture? Inspired by the design of neural\narchitectures for classical computing which typically employs multiple types of\nneurons, this paper makes the very first attempt to mix quantum neuron designs\nto build quantum neural architectures. We observe that the existing quantum\nneuron designs may be quite different but complementary, such as neurons from\nvariational quantum circuits (VQC) and Quantumflow. More specifically, VQC can\napply real-valued weights but suffer from being extended to multiple layers,\nwhile QuantumFlow can build a multi-layer network efficiently, but is limited\nto use binary weights. To take their respective advantages, we propose to mix\nthem together and figure out a way to connect them seamlessly without\nadditional costly measurement. We further investigate the design principles to\nmix quantum neurons, which can provide guidance for quantum neural architecture\nexploration in the future. Experimental results demonstrate that the identified\nquantum neural architectures with mixed quantum neurons can achieve 90.62% of\naccuracy on the MNIST dataset, compared with 52.77% and 69.92% on the VQC and\nQuantumFlow, respectively.']"
12,94,12_graph_gnns_node_gnn,"['graph', 'gnns', 'node', 'gnn', 'graphs', 'networks', 'neural', 'gcn', 'clustering', 'learning']","['Spectral graph convolutional networks (SGCNs) have been attracting increasing\nattention in graph representation learning partly due to their interpretability\nthrough the prism of the established graph signal processing framework.\nHowever, existing SGCNs are limited in implementing graph convolutions with\nrigid transforms that could not adapt to signals residing on graphs and tasks\nat hand. In this paper, we propose a novel class of spectral graph\nconvolutional networks that implement graph convolutions with adaptive graph\nwavelets. Specifically, the adaptive graph wavelets are learned with neural\nnetwork-parameterized lifting structures, where structure-aware attention-based\nlifting operations are developed to jointly consider graph structures and node\nfeatures. We propose to lift based on diffusion wavelets to alleviate the\nstructural information loss induced by partitioning non-bipartite graphs. By\ndesign, the locality and sparsity of the resulting wavelet transform as well as\nthe scalability of the lifting structure for large and varying-size graphs are\nguaranteed. We further derive a soft-thresholding filtering operation by\nlearning sparse graph representations in terms of the learned wavelets, which\nimproves the scalability and interpretablity, and yield a localized, efficient\nand scalable spectral graph convolution. To ensure that the learned graph\nrepresentations are invariant to node permutations, a layer is employed at the\ninput of the networks to reorder the nodes according to their local topology\ninformation. We evaluate the proposed networks in both node-level and\ngraph-level representation learning tasks on benchmark citation and\nbioinformatics graph datasets. Extensive experiments demonstrate the\nsuperiority of the proposed networks over existing SGCNs in terms of accuracy,\nefficiency and scalability.', 'Graph neural networks (GNNs) have drawn increasing attention in recent years\nand achieved remarkable performance in many graph-based tasks, especially in\nsemi-supervised learning on graphs. However, most existing GNNs excessively\nrely on topological structures and aggregate multi-hop neighborhood information\nby simply stacking network layers, which may introduce superfluous noise\ninformation, limit the expressive power of GNNs and lead to the over-smoothing\nproblem ultimately. In light of this, we propose a novel Dual-Perception Graph\nNeural Network (DPGNN) to address these issues. In DPGNN, we utilize node\nfeatures to construct a feature graph, and perform node representations\nlearning based on the original topology graph and the constructed feature graph\nsimultaneously, which conduce to capture the structural neighborhood\ninformation and the feature-related information. Furthermore, we design a\nMulti-Hop Graph Generator (MHGG), which applies a node-to-hop attention\nmechanism to aggregate node-specific multi-hop neighborhood information\nadaptively. Finally, we apply self-ensembling to form a consistent prediction\nfor unlabeled node representations. Experimental results on five datasets with\ndifferent topological structures demonstrate that our proposed DPGNN\noutperforms all the latest state-of-the-art models on all datasets, which\nproves the superiority and versatility of our model. The source code of our\nmodel is available at https://github.com.', 'Graph Neural Networks have become one of the indispensable tools to learn\nfrom graph-structured data, and their usefulness has been shown in wide variety\nof tasks. In recent years, there have been tremendous improvements in\narchitecture design, resulting in better performance on various prediction\ntasks. In general, these neural architectures combine node feature aggregation\nand feature transformation using learnable weight matrix in the same layer.\nThis makes it challenging to analyze the importance of node features aggregated\nfrom various hops and the expressiveness of the neural network layers. As\ndifferent graph datasets show varying levels of homophily and heterophily in\nfeatures and class label distribution, it becomes essential to understand which\nfeatures are important for the prediction tasks without any prior information.\nIn this work, we decouple the node feature aggregation step and depth of graph\nneural network, and empirically analyze how different aggregated features play\na role in prediction performance. We show that not all features generated via\naggregation steps are useful, and often using these less informative features\ncan be detrimental to the performance of the GNN model. Through our\nexperiments, we show that learning certain subsets of these features can lead\nto better performance on wide variety of datasets. We propose to use softmax as\na regularizer and ""soft-selector"" of features aggregated from neighbors at\ndifferent hop distances; and L2-Normalization over GNN layers. Combining these\ntechniques, we present a simple and shallow model, Feature Selection Graph\nNeural Network (FSGNN), and show empirically that the proposed model achieves\ncomparable or even higher accuracy than state-of-the-art GNN models in nine\nbenchmark datasets for the node classification task, with remarkable\nimprovements up to 51.1%.']"
13,91,13_power_wind_energy_control,"['power', 'wind', 'energy', 'control', 'system', 'turbine', 'charging', 'grid', 'the', 'flexibility']","['Power systems are shifted from conventional bulk generation toward renewable\ngeneration. This trend leads to the frequency security problem due to the\ndecline of system inertia. On the other hand, natural gas-fired units are\nfrequently scheduled to provide operational flexibility due to their fast\nadjustment ability. The interdependence between power and natural gas systems\nis thus intensified. In this paper, we study the frequency constrained\nscheduling problem from the perspective of an integrated electricity-gas system\nunder variable wind power. We propose a distributionally robust (DR) chance\nconstrained optimization model to co-optimize the unit commitment and virtual\ninertia provision from wind farm systems. This model incorporates both\nfrequency constraints and natural gas system (NGS) operational constraints and\naddresses the wind power uncertainty by designing DR joint chance constraints.\nWe show that this model admits a mixed-integer second-order cone programming.\nCase studies demonstrate that the proposed approach can provide a highly\nreliable and computationally efficient solution and show the importance of\nincorporating NGS operational constraints in the frequency constrained\nscheduling problem.', 'The grid-forming converter is an important unit in the future power system\nwith more inverter-interfaced generators. However, improving its performance is\nstill a key challenge. This paper proposes a generalized architecture of the\ngrid-forming converter from the view of multivariable feedback control. As a\nresult, many of the existing popular control strategies, i.e., droop control,\npower synchronization control, virtual synchronous generator control, matching\ncontrol, dispatchable virtual oscillator control, and their improved forms are\nunified into a multivariable feedback control transfer matrix working on\nseveral linear and nonlinear error signals. Meanwhile, unlike the traditional\nassumptions of decoupling between AC and DC control, active power and reactive\npower control, the proposed configuration simultaneously takes all of them into\nconsideration, which therefore can provide better performance. As an example, a\nnew multi-input-multi-output-based grid-forming (MIMO-GFM) control is proposed\nbased on the generalized configuration. To cope with the multivariable\nfeedback, an optimal and structured $H_{\\infty}$ synthesis is used to design\nthe control parameters. At last, simulation and experimental results show\nsuperior performance and robustness of the proposed configuration and control.', 'A charging system is required to convert ac electricity from the grid to dc\nelectricity to charge an electric vehicle (EV) battery. According to the\nSociety of Automatic Engineers (SAE) standard, EV chargers can be divided into\nthree levels based on power rating: Level 1, Level 2, and Level 3. This paper\ninvestigates the circuit topologies and control principles of EV charging\nsystems at each level. Three high-fidelity testbeds of EV charging systems for\na 10 kWh battery are designed and implemented in real-time digital simulator\nRT-Lab. The testbeds include modeling details such as switching of\nsemiconductors. Twenty-five minutes real-time simulation is conducted for each\ntestbed. Detailed dynamic performance of the circuits and the controls at every\nstage are presented to demonstrate the charging process. All three level EV\ncharging systems employ high-frequency transformer embedded dual active bridge\n(DAB) dc/dc converter to regulate battery side dc voltage and current. Hence,\naverage model-based linear system analysis is given to configure the parameters\nof the phase shift control adopted by the DAB dc/dc converter. In addition,\npower factor control (PFC) that is employed for Level 1 and Level 2\nsingle-phase ac charging systems, three-phase voltage source converter control\nthat is employed for Level 3 three-phase ac charging systems, are all analyzed.\nThe three testbeds, with their detailed circuit parameters and control\nparameters presented, can be used as reference testbeds for EV grid integration\nresearch.']"
14,90,14_auction_mechanism_games_agents,"['auction', 'mechanism', 'games', 'agents', 'equilibrium', 'game', 'auctions', 'mechanisms', 'optimal', 'matching']","[""We consider a sequential blocked matching (SBM) model where strategic agents\nrepeatedly report ordinal preferences over a set of services to a central\nmechanism. The central mechanism's goal is to elicit agents' true preferences\nand design a policy that matches services to agents in order to maximize the\nexpected social welfare with the added constraint that each matched service can\nbe \\emph{blocked} or unavailable for a number of time periods. Naturally, SBM\nmodels the repeated allocation of reusable services to a set of agents where\neach allocated service becomes unavailable for a fixed duration. We first\nconsider the offline SBM setting, where the the strategic agents are aware of\nthe true preferences. We measure the performance of any policy by\n\\emph{distortion}, the worst-case multiplicative approximation guaranteed by\nany policy. For the setting with $S$ services, we establish lower bounds of\n$\\Omega(S)$ and $\\Omega(\\sqrt{S})$ on the distortions of any deterministic and\nrandomised mechanisms, respectively. We complement these results by providing\napproximately truthful, measured by \\emph{incentive ratio}, deterministic and\nrandomised policies based on the repeated application of random serial\ndictatorship that match the lower bounds. Our results show that there is a\nsignificant improvement if one considers the class of randomised policies.\n  Finally, we consider the online SBM setting with bandit feedback where each\nagent is unaware of her true preference, and the center must facilitate each\nagent in the learning of agent's preference through the matching of services\nover time. We design an approximately truthful mechanism based on the\nExplore-then-Commit paradigm, which achieves logarithmic dynamic approximate\nregret."", ""We study revenue maximization in multi-item multi-bidder auctions under the\nnatural item-independence assumption - a classical problem in Multi-Dimensional\nBayesian Mechanism Design. One of the biggest challenges in this area is\ndeveloping algorithms to compute (approximately) optimal mechanisms that are\nnot brute-force in the size of the bidder type space, which is usually\nexponential in the number of items in multi-item auctions. Unfortunately, such\nalgorithms were only known for basic settings of our problem when bidders have\nunit-demand [CHMS10,CMS15] or additive valuations [Yao15].\n  In this paper, we significantly improve the previous results and design the\nfirst algorithm that runs in time polynomial in the number of items and the\nnumber of bidders to compute mechanisms that are $O(1)$-approximations to the\noptimal revenue when bidders have XOS valuations, resolving the open problem\nraised in [CM16,CZ17]. Moreover, the computed mechanism has a simple structure:\nIt is either a posted price mechanism or a two-part tariff mechanism. As a\ncorollary of our result, we show how to compute an approximately optimal and\nsimple mechanism efficiently using only sample access to the bidders' value\ndistributions. Our algorithm builds on two innovations that allow us to search\nover the space of mechanisms efficiently: (i) a new type of succinct\nrepresentation of mechanisms - the marginal reduced forms, and (ii) a novel\nLift-and-Round procedure that concavifies the problem."", ""We consider the single-item interdependent value setting, where there is a\nmonopolist, $n$ buyers, and each buyer has a private signal $s_i$ describing a\npiece of information about the item. Each bidder $i$ also has a valuation\nfunction $v_i(s_1,\\ldots,s_n)$ mapping the (private) signals of all buyers to a\npositive real number representing their value for the item. This setting\ncaptures scenarios where the item's information is asymmetric or dispersed\namong agents, such as in competitions for oil drilling rights, or in auctions\nfor art pieces. Due to the increased complexity of this model compared to\nstandard private values, it is generally assumed that each bidder's valuation\nfunction $v_i$ is public knowledge. But in many situations, the seller may not\nknow how a bidder aggregates signals into a valuation. In this paper, we design\nmechanisms that guarantee approximately-optimal social welfare while satisfying\nex-post incentive compatibility and individual rationality for the case where\nthe valuation functions are private to the bidders.\n  When the valuations are public, it is possible for optimal social welfare to\nbe attained by a deterministic mechanism under a single-crossing condition. In\ncontrast, when the valuations are the bidders' private information, we show\nthat no finite bound can be achieved by any deterministic mechanism even under\nsingle-crossing.\n  Moreover, no randomized mechanism can guarantee better than an\n$n$-approximation. We thus consider valuation functions that are submodular\nover signals (SOS), introduced in the context of combinatorial auctions in a\nrecent breakthrough paper by Eden et al. [EC'19]. Our main result is an\n$O(\\log^2 n)$-approximation for buyers with private signals and valuations\nunder the SOS condition. We also give a tight $\\Theta(k)$-approximation for the\ncase each agent's valuation depends on at most $k$ other signals even for\nunknown $k$.""]"
15,90,15_recommendation_item_user_recommender,"['recommendation', 'item', 'user', 'recommender', 'items', 'users', 'graph', 'interactions', 'click', 'preference']","[""Social recommendation task aims to predict users' preferences over items with\nthe incorporation of social connections among users, so as to alleviate the\nsparse issue of collaborative filtering. While many recent efforts show the\neffectiveness of neural network-based social recommender systems, several\nimportant challenges have not been well addressed yet: (i) The majority of\nmodels only consider users' social connections, while ignoring the\ninter-dependent knowledge across items; (ii) Most of existing solutions are\ndesigned for singular type of user-item interactions, making them infeasible to\ncapture the interaction heterogeneity; (iii) The dynamic nature of user-item\ninteractions has been less explored in many social-aware recommendation\ntechniques. To tackle the above challenges, this work proposes a\nKnowledge-aware Coupled Graph Neural Network (KCGN) that jointly injects the\ninter-dependent knowledge across items and users into the recommendation\nframework. KCGN enables the high-order user- and item-wise relation encoding by\nexploiting the mutual information for global graph structure awareness.\nAdditionally, we further augment KCGN with the capability of capturing dynamic\nmulti-typed user-item interactive patterns. Experimental studies on real-world\ndatasets show the effectiveness of our method against many strong baselines in\na variety of settings. Source codes are available at:\nhttps://github.com/xhcdream/KCGN."", 'Sequential recommendation has been a widely popular topic of recommender\nsystems. Existing works have contributed to enhancing the prediction ability of\nsequential recommendation systems based on various methods, such as recurrent\nnetworks and self-attention mechanisms. However, they fail to discover and\ndistinguish various relationships between items, which could be underlying\nfactors which motivate user behaviors. In this paper, we propose an\nEdge-Enhanced Global Disentangled Graph Neural Network (EGD-GNN) model to\ncapture the relation information between items for global item representation\nand local user intention learning. At the global level, we build a global-link\ngraph over all sequences to model item relationships. Then a channel-aware\ndisentangled learning layer is designed to decompose edge information into\ndifferent channels, which can be aggregated to represent the target item from\nits neighbors. At the local level, we apply a variational auto-encoder\nframework to learn user intention over the current sequence. We evaluate our\nproposed method on three real-world datasets. Experimental results show that\nour model can get a crucial improvement over state-of-the-art baselines and is\nable to distinguish item features.', ""Recommender systems, a pivotal tool to alleviate the information overload\nproblem, aim to predict user's preferred items from millions of candidates by\nanalyzing observed user-item relations. As for tackling the sparsity and cold\nstart problems encountered by recommender systems, uncovering hidden (indirect)\nuser-item relations by employing side information and knowledge to enrich\nobserved information for the recommendation has been proven promising recently;\nand its performance is largely determined by the scalability of recommendation\nmodels in the face of the high complexity and large scale of side information\nand knowledge. Making great strides towards efficiently utilizing complex and\nlarge-scale data, research into graph embedding techniques is a major topic.\nEquipping recommender systems with graph embedding techniques contributes to\noutperforming the conventional recommendation implementing directly based on\ngraph topology analysis and has been widely studied these years. This article\nsystematically retrospects graph embedding-based recommendation from embedding\ntechniques for bipartite graphs, general graphs, and knowledge graphs, and\nproposes a general design pipeline of that. In addition, comparing several\nrepresentative graph embedding-based recommendation models with the most\ncommon-used conventional recommendation models, on simulations, manifests that\nthe conventional models overall outperform the graph embedding-based ones in\npredicting implicit user-item interactions, revealing the relative weakness of\ngraph embedding-based recommendation in these tasks. To foster future research,\nthis article proposes constructive suggestions on making a trade-off between\ngraph embedding-based recommendation and the conventional recommendation in\ndifferent tasks as well as some open questions.""]"
16,83,16_spin_magnetic_topological_hall,"['spin', 'magnetic', 'topological', 'hall', 'materials', 'valley', 'states', 'effect', 'orbit', 'ferromagnetic']","['The combination of topology and magnetism is attractive to produce exotic\nquantum matters, such as the quantum anomalous Hall state, axion insulators and\nthe magnetic Weyl semimetals. MnBi2Te4, as an intrinsic magnetic topological\ninsulator, provides a platform for the realization of various topological\nphases. Here we report the intermediate Hall steps in the magnetic hysteresis\nof MnBi2Te4, where four distinguishable magnetic memory states at zero magnetic\nfield are revealed. The gate and temperature dependence of the magnetic\nintermediate states indicates the noncollinear spin structure in MnBi2Te4,\nwhich can be attributed to the Dzyaloshinskii-Moriya interaction as the\ncoexistence of strong spin-orbit coupling and local inversion symmetry breaking\non the surface. Moreover, these multiple magnetic memory states can be\nprogrammatically switched among each other through applying designed pulses of\nmagnetic field. Our results provide new insights of the influence of bulk\ntopology on the magnetic states, and the multiple memory states should be\npromising for spintronic devices.', 'The manipulation of magnetic properties using either electrical currents or\ngate bias is the key of future high-impact nanospintronics applications such as\nspin-valve read heads, non-volatile logic, and random-access memories. The\ncurrent technology for magnetic switching with spin-transfer torque requires\nhigh current densities, whereas gate-tunable magnetic materials such as\nferromagnetic semiconductors and multiferroic materials are still far from\npractical applications. Recently, magnetic switching induced by pure spin\ncurrents using the spin Hall and Rashba effects in heavy metals, called\nspin-orbit torque (SOT), has emerged as a candidate for designing\nnext-generation magnetic memory with low current densities. The recent\ndiscovery of topological materials and two-dimensional (2D) van der Waals (vdW)\nmaterials provides opportunities to explore versatile 3D-2D and 2D-2D\nheterostructures with interesting characteristics. In this review, we introduce\nthe emerging approaches to realizing SOT nanodevices including techniques to\nevaluate the SOT efficiency as well as the opportunities and challenges of\nusing 2D topological materials and vdW materials in such applications.', ""We propose a realizable device design for an all-electrical robust valley\nfilter that utilizes spin protected topological interface states hosted on\nmonolayer 2D-Xene materials with large intrinsic spin-orbit coupling. In\ncontrast with conventional quantum spin-Hall edge states localized around the\n$X$-points, the interface states appearing at the domain wall between\ntopologically distinct phases are either from the $K$ or $K^{'}$ points, making\nthem suitable prospects for serving as valley-polarized channels. We show that\nthe presence of a large band-gap quantum spin Hall effect enables the spatial\nseparation of the spin-valley locked helical interface states with the valley\nstates being protected by spin conservation, leading to a robustness against\nshort-range non-magnetic disorder. By adopting the scattering matrix formalism\non a suitably designed device structure, valley-resolved transport in the\npresence of non-magnetic short-range disorder for different 2D-Xene materials\nis also analyzed in detail. Our numerical simulations confirm the role of\nspin-orbit coupling in achieving an improved valley filter performance with a\nperfect quantum of conductance attributed to the topologically protected\ninterface states. Our analysis further elaborates clearly the right choice of\nmaterial, device geometry and other factors that need to be considered while\ndesigning an optimized valleytronic filter device.""]"
17,82,17_rl_reinforcement_policy_reward,"['rl', 'reinforcement', 'policy', 'reward', 'learning', 'agent', 'policies', 'algorithms', 'environment', 'environments']","['To achieve sample efficiency in reinforcement learning (RL), it necessitates\nefficiently exploring the underlying environment. Under the offline setting,\naddressing the exploration challenge lies in collecting an offline dataset with\nsufficient coverage. Motivated by such a challenge, we study the reward-free RL\nproblem, where an agent aims to thoroughly explore the environment without any\npre-specified reward function. Then, given any extrinsic reward, the agent\ncomputes the policy via a planning algorithm with offline data collected in the\nexploration phase. Moreover, we tackle this problem under the context of\nfunction approximation, leveraging powerful function approximators.\n  Specifically, we propose to explore via an optimistic variant of the\nvalue-iteration algorithm incorporating kernel and neural function\napproximations, where we adopt the associated exploration bonus as the\nexploration reward. Moreover, we design exploration and planning algorithms for\nboth single-agent MDPs and zero-sum Markov games and prove that our methods can\nachieve $\\widetilde{\\mathcal{O}}(1 /\\varepsilon^2)$ sample complexity for\ngenerating a $\\varepsilon$-suboptimal policy or $\\varepsilon$-approximate Nash\nequilibrium when given an arbitrary extrinsic reward. To the best of our\nknowledge, we establish the first provably efficient reward-free RL algorithm\nwith kernel and neural function approximators.', 'Recent advances in reinforcement learning (RL) have led to a growing interest\nin applying RL to classical planning domains or applying classical planning\nmethods to some complex RL domains. However, the long-horizon goal-based\nproblems found in classical planning lead to sparse rewards for RL, making\ndirect application inefficient. In this paper, we propose to leverage\ndomain-independent heuristic functions commonly used in the classical planning\nliterature to improve the sample efficiency of RL. These classical heuristics\nact as dense reward generators to alleviate the sparse-rewards issue and enable\nour RL agent to learn domain-specific value functions as residuals on these\nheuristics, making learning easier. Correct application of this technique\nrequires consolidating the discounted metric used in RL and the non-discounted\nmetric used in heuristics. We implement the value functions using Neural Logic\nMachines, a neural network architecture designed for grounded first-order logic\ninputs. We demonstrate on several classical planning domains that using\nclassical heuristics for RL allows for good sample efficiency compared to\nsparse-reward RL. We further show that our learned value functions generalize\nto novel problem instances in the same domain.', 'Reinforcement learning (RL) provides a framework for learning goal-directed\npolicies given user-specified rewards. However, since designing rewards often\nrequires substantial engineering effort, we are interested in the problem of\nlearning without rewards, where agents must discover useful behaviors in the\nabsence of task-specific incentives. Intrinsic motivation is a family of\nunsupervised RL techniques which develop general objectives for an RL agent to\noptimize that lead to better exploration or the discovery of skills. In this\npaper, we propose a new unsupervised RL technique based on an adversarial game\nwhich pits two policies against each other to compete over the amount of\nsurprise an RL agent experiences. The policies each take turns controlling the\nagent. The Explore policy maximizes entropy, putting the agent into surprising\nor unfamiliar situations. Then, the Control policy takes over and seeks to\nrecover from those situations by minimizing entropy. The game harnesses the\npower of multi-agent competition to drive the agent to seek out increasingly\nsurprising parts of the environment while learning to gain mastery over them.\nWe show empirically that our method leads to the emergence of complex skills by\nexhibiting clear phase transitions. Furthermore, we show both theoretically\n(via a latent state space coverage argument) and empirically that our method\nhas the potential to be applied to the exploration of stochastic,\npartially-observed environments. We show that Adversarial Surprise learns more\ncomplex behaviors, and explores more effectively than competitive baselines,\noutperforming intrinsic motivation methods based on active inference,\nnovelty-seeking (Random Network Distillation (RND)), and multi-agent\nunsupervised RL (Asymmetric Self-Play (ASP)) in MiniGrid, Atari and VizDoom\nenvironments.']"
18,77,18_detector_proton_muon_mu,"['detector', 'proton', 'muon', 'mu', 'beam', 'lhc', 'energy', 'collider', 'neutron', 'at']","[""The ePix detector family provides multiple variants of hybrid pixel detectors\nto support a wide range of applications at free electron laser facilities. We\npresent the results of a systematic study of the influence of radiation induced\ndamage on the performance and lifetime of an ePix100a detector module using a\ndirect attenuated beam of the EuXFEL at 9 keV photon energy and an average\npower of 10 $\\mu$W. An area of 20 x 20 pixels was irradiated with an average\nphoton flux of approx. 7 x $10^{9}$ photons/s to a dose of approximately\n760$\\pm$65 kGy at the location of the Si/SiO$_2$ interfaces in the sensor. A\ndose dependent increase in both offset and noise of the ePix100a detector have\nbeen observed originating from an increase of the sensor leakage current.\nMoreover, we observed an effect directly after irradiation resulting in the\nsaturation of individual pixels by their dark current. Changes in gain are\nevaluated one and half hours post irradiation and suggest damage to occur also\non the ASIC level. Based on the obtained results, thresholds for beam\nparameters are deduced and the detector lifetime is estimated with respect to\nthe requirements to the data quality in order to satisfy the scientific\nstandards defined by the experiments. We conclude the detector can withstand a\nbeam with an energy up to 1 $\\mu$J at a photon energy of 9 keV impacting on an\narea of 1 mm$^2$. The detector can be used without significant degradation of\nits performance for several years if the incident photon beam intensities do\nnot exceed the detector's dynamic range by at least three orders of magnitude.\nOur results provide valuable input for the operation of the ePix100a detector\nat FEL facilities and for the design of future detector technology."", 'The proposed high-luminosity high-energy Electron-Ion Collider (EIC) will\nprovide a clean environment to precisely study several fundamental questions in\nthe fields of high-energy and nuclear physics . A low material budget and high\ngranularity silicon vertex/tracking detector is critical to carry out a series\nof hadron and jet measurements at the future EIC especially for the heavy\nflavor product reconstruction or tagging. The conceptual design of a proposed\nforward silicon tracking detector with the pseudorapidity coverage from 1.2 to\n3.5 has been developed in integration with different magnet options and the\nother EIC detector sub-systems. The tracking performance of this detector\nenables precise heavy flavor hadron and jet measurements in the hadron beam\ngoing direction. The detector R$\\&$D for the proposed silicon technology\ncandidates: Low Gain Avalanche Diode (LGAD) and radiation hard depleted\nMonolithic Active Pixel Sensor (MALTA), which can provide good spatial and\ntiming resolutions, is underway. Bench test results of the LGAD and MALTA\nprototype sensors will be discussed.', ""The Muon $g-2$ Experiment at Fermilab uses a gaseous straw tracking detector\nto make detailed measurements of the stored muon beam profile, which are\nessential for the experiment to achieve its uncertainty goals. Positrons from\nmuon decays spiral inward and pass through the tracking detector before\nstriking an electromagnetic calorimeter. The tracking detector is therefore\nlocated inside the vacuum chamber in a region where the magnetic field is large\nand non-uniform. As such, the tracking detector must have a low leak rate to\nmaintain a high-quality vacuum, must be non-magnetic so as not to perturb the\nmagnetic field and, to minimize energy loss, must have a low radiation length.\nThe performance of the tracking detector has met or surpassed the design\nrequirements, with adequate electronic noise levels, an average straw hit\nresolution of $(110 \\pm 20)$ $\\mu$m, a detection efficiency of 97\\% or higher,\nand no performance degradation or signs of aging. The tracking detector's\nmeasurements result in an otherwise unachievable understanding of the muon's\nbeam motion, particularly at early times in the experiment's measurement period\nwhen there are a significantly greater number of muons decaying. This is vital\nto the statistical power of the experiment, as well as facilitating the precise\nextraction of several systematic corrections and uncertainties. This paper\ndescribes the design, construction, testing, commissioning, and performance of\nthe tracking detector.""]"
19,74,19_visualization_visualizations_data_visual,"['visualization', 'visualizations', 'data', 'visual', 'design', 'and', 'user', 'interactive', 'we', 'charts']","[""Problem-driven visualization work is rooted in deeply understanding the data,\nactors, processes, and workflows of a target domain. However, an individual's\npersonality traits and cognitive abilities may also influence visualization\nuse. Diverse user needs and abilities raise natural questions for specificity\nin visualization design: Could individuals from different domains exhibit\nperformance differences when using visualizations? Are any systematic\nvariations related to their cognitive abilities? This study bridges\ndomain-specific perspectives on visualization design with those provided by\ncognition and perception. We measure variations in visualization task\nperformance across chemistry, computer science, and education, and relate these\ndifferences to variations in spatial ability. We conducted an online study with\nover 60 domain experts consisting of tasks related to pie charts, isocontour\nplots, and 3D scatterplots, and grounded by a well-documented spatial ability\ntest. Task performance (correctness) varied with profession across more complex\nvisualizations, but not pie charts, a comparatively common visualization. We\nfound that correctness correlates with spatial ability, and the professions\ndiffer in terms of spatial ability. These results indicate that domains differ\nnot only in the specifics of their data and tasks, but also in terms of how\neffectively their constituent members engage with visualizations and their\ncognitive traits. Analyzing participants' confidence and strategy comments\nsuggests that focusing on performance neglects important nuances, such as\ndiffering approaches to engage with even common visualizations and potential\nskill transference. Our findings offer a fresh perspective on\ndiscipline-specific visualization with recommendations to help guide\nvisualization design that celebrates the uniqueness of the disciplines and\nindividuals we seek to serve."", 'Visualization recommendation or automatic visualization generation can\nsignificantly lower the barriers for general users to rapidly create effective\ndata visualizations, especially for those users without a background in data\nvisualizations. However, existing rule-based approaches require tedious manual\nspecifications of visualization rules by visualization experts. Other machine\nlearning-based approaches often work like black-box and are difficult to\nunderstand why a specific visualization is recommended, limiting the wider\nadoption of these approaches. This paper fills the gap by presenting KG4Vis, a\nknowledge graph (KG)-based approach for visualization recommendation. It does\nnot require manual specifications of visualization rules and can also guarantee\ngood explainability. Specifically, we propose a framework for building\nknowledge graphs, consisting of three types of entities (i.e., data features,\ndata columns and visualization design choices) and the relations between them,\nto model the mapping rules between data and effective visualizations. A\nTransE-based embedding technique is employed to learn the embeddings of both\nentities and relations of the knowledge graph from existing\ndataset-visualization pairs. Such embeddings intrinsically model the desirable\nvisualization rules. Then, given a new dataset, effective visualizations can be\ninferred from the knowledge graph with semantically meaningful rules. We\nconducted extensive evaluations to assess the proposed approach, including\nquantitative comparisons, case studies and expert interviews. The results\ndemonstrate the effectiveness of our approach.', 'A growing body of research focuses on helping users explore complex datasets\nfaster by automatically suggesting visualization designs of possible interest.\nHowever, existing visualization recommendation systems only enumerate, rank,\nand recommend a small group of visualization designs. Our goal is to understand\nwhether there is enough theoretical and experimental knowledge in current\nliterature to inform visualization recommendation systems to assess the entire\nvisualization design space. Thus, in this paper, we present a literature review\ncomparing and ranking the quality of visualization designs in visual perception\nand human performance. We structure our review by first defining the\nvisualization design space where visualizations must be compared to recommend\neffective visualization designs. We then perform the review by using a\ncomprehensive schema to record the theoretical and experimental results of\nvisualization comparison, which can also be used to guide the future\nconstruction of visualization recommendation systems. To analyze the literature\ncoverage, we develop an interactive tool that can help explore current\nliterature coverage of visualization comparison and identify gaps efficiently\nand effectively. Based on our findings, we highlight new opportunities and\nchallenges for the community in working towards a comprehensive visualization\nranking for informing visualization recommendation systems.']"
20,72,20_molecular_drug_molecules_protein,"['molecular', 'drug', 'molecules', 'protein', 'chemical', 'docking', 'molecule', 'binding', 'generative', 'proteins']","['Recently, utilizing reinforcement learning (RL) to generate molecules with\ndesired properties has been highlighted as a promising strategy for drug\ndesign. A molecular docking program - a physical simulation that estimates\nprotein-small molecule binding affinity - can be an ideal reward scoring\nfunction for RL, as it is a straightforward proxy of the therapeutic potential.\nStill, two imminent challenges exist for this task. First, the models often\nfail to generate chemically realistic and pharmacochemically acceptable\nmolecules. Second, the docking score optimization is a difficult exploration\nproblem that involves many local optima and less smooth surfaces with respect\nto molecular structure. To tackle these challenges, we propose a novel RL\nframework that generates pharmacochemically acceptable molecules with large\ndocking scores. Our method - Fragment-based generative RL with Explorative\nExperience replay for Drug design (FREED) - constrains the generated molecules\nto a realistic and qualified chemical space and effectively explores the space\nto find drugs by coupling our fragment-based generation method and a novel\nerror-prioritized experience replay (PER). We also show that our model performs\nwell on both de novo and scaffold-based schemes. Our model produces molecules\nof higher quality compared to existing methods while achieving state-of-the-art\nperformance on two of three targets in terms of the docking scores of the\ngenerated molecules. We further show with ablation studies that our method,\npredictive error-PER (FREED(PE)), significantly improves the model performance.', 'Structure-based drug design involves finding ligand molecules that exhibit\nstructural and chemical complementarity to protein pockets. Deep generative\nmethods have shown promise in proposing novel molecules from scratch (de-novo\ndesign), avoiding exhaustive virtual screening of chemical space. Most\ngenerative de-novo models fail to incorporate detailed ligand-protein\ninteractions and 3D pocket structures. We propose a novel supervised model that\ngenerates molecular graphs jointly with 3D pose in a discretised molecular\nspace. Molecules are built atom-by-atom inside pockets, guided by structural\ninformation from crystallographic data. We evaluate our model using a docking\nbenchmark and find that guided generation improves predicted binding affinities\nby 8% and drug-likeness scores by 10% over the baseline. Furthermore, our model\nproposes molecules with binding scores exceeding some known ligands, which\ncould be useful in future wet-lab studies.', 'Graph neural networks (GNNs) have been used extensively for addressing\nproblems in drug design and discovery. Both ligand and target molecules are\nrepresented as graphs with node and edge features encoding information about\natomic elements and bonds respectively. Although existing deep learning models\nperform remarkably well at predicting physicochemical properties and binding\naffinities, the generation of new molecules with optimized properties remains\nchallenging. Inherently, most GNNs perform poorly in whole-graph representation\ndue to the limitations of the message-passing paradigm. Furthermore,\nstep-by-step graph generation frameworks that use reinforcement learning or\nother sequential processing can be slow and result in a high proportion of\ninvalid molecules with substantial post-processing needed in order to satisfy\nthe principles of stoichiometry. To address these issues, we propose a\nrepresentation-first approach to molecular graph generation. We guide the\nlatent representation of an autoencoder by capturing graph structure\ninformation with the geometric scattering transform and apply penalties that\nstructure the representation also by molecular properties. We show that this\nhighly structured latent space can be directly used for molecular graph\ngeneration by the use of a GAN. We demonstrate that our architecture learns\nmeaningful representations of drug datasets and provides a platform for\ngoal-directed drug synthesis.']"
21,71,21_treatment_trial_trials_randomized,"['treatment', 'trial', 'trials', 'randomized', 'covariates', 'causal', 'effect', 'sample', 'patients', 'covariate']","['Methods for extending -- generalizing or transporting -- inferences from a\nrandomized trial to a target population involve conditioning on a large set of\ncovariates that is sufficient for rendering the randomized and non-randomized\ngroups exchangeable. Yet, decision-makers are often interested in examining\ntreatment effects in subgroups of the target population defined in terms of\nonly a few discrete covariates. Here, we propose methods for estimating\nsubgroup-specific potential outcome means and average treatment effects in\ngeneralizability and transportability analyses, using outcome model-based\n(g-formula), weighting, and augmented weighting estimators. We consider\nestimating subgroup-specific average treatment effects in the target population\nand its non-randomized subset, and provide methods that are appropriate both\nfor nested and non-nested trial designs. As an illustration, we apply the\nmethods to data from the Coronary Artery Surgery Study to compare the effect of\nsurgery plus medical therapy versus medical therapy alone for chronic coronary\nartery disease in subgroups defined by history of myocardial infarction.', 'Clinical trials with a hybrid control arm (a control arm constructed from a\ncombination of randomized patients and real-world data on patients receiving\nusual care in standard clinical practice) have the potential to decrease the\ncost of randomized trials while increasing the proportion of trial patients\ngiven access to novel therapeutics. However, due to stringent trial inclusion\ncriteria and differences in care and data quality between trials and community\npractice, trial patients may have systematically different outcomes compared to\ntheir real-world counterparts. We propose a new method for analyses of trials\nwith a hybrid control arm that efficiently controls bias and type I error.\nUnder our proposed approach, selected real-world patients are weighted by a\nfunction of the ""on-trial score,"" which reflects their similarity to trial\npatients. In contrast to previously developed hybrid control designs that\nassign the same weight to all real-world patients, our approach upweights of\nreal-world patients who more closely resemble randomized control patients while\ndissimilar patients are discounted. Estimates of the treatment effect are\nobtained via Cox proportional hazards models. We compare our approach to\nexisting approaches via simulations and apply these methods to a study using\nelectronic health record data. Our proposed method is able to control type I\nerror, minimize bias, and decrease variance when compared to using only trial\ndata in nearly all scenarios examined. Therefore, our new approach can be used\nwhen conducting clinical trials by augmenting the standard-of-care arm with\nweighted patients from the EHR to increase power without inducing bias.', 'N-of-1 trials, single participant trials in which multiple treatments are\nsequentially randomized over the study period, can give direct estimates of\nindividual-specific treatment effects. Combining n-of-1 trials gives extra\ninformation for estimating the population average treatment effect compared\nwith randomized controlled trials and increases precision for\nindividual-specific treatment effect estimates. In this paper, we present a\nprocedure for designing n-of-1 trials. We formally define the design components\nfor determining the sample size of a series of n-of-1 trials, present models\nfor analyzing these trials and use them to derive the sample size formula for\nestimating the population average treatment effect and the standard error of\nthe individual-specific treatment effect estimates. We recommend first finding\nthe possible designs that will satisfy the power requirement for estimating the\npopulation average treatment effect and then, if of interest, finalizing the\ndesign to also satisfy the standard error requirements for the\nindividual-specific treatment effect estimates. The procedure is implemented\nand illustrated in the paper and through a Shiny app.']"
22,70,22_vision_mlp_transformer_object,"['vision', 'mlp', 'transformer', 'object', 'attention', 'architecture', 'transformers', 'detection', 'tasks', 'computer']","['Recently, transformer and multi-layer perceptron (MLP) architectures have\nachieved impressive results on various vision tasks. A few works investigated\nmanually combining those operators to design visual network architectures, and\ncan achieve satisfactory performances to some extent. In this paper, we propose\nto jointly search the optimal combination of convolution, transformer, and MLP\nfor building a series of all-operator network architectures with high\nperformances on visual tasks. We empirically identify that the widely-used\nstrided convolution or pooling based down-sampling modules become the\nperformance bottlenecks when the operators are combined to form a network. To\nbetter tackle the global context captured by the transformer and MLP operators,\nwe propose two novel context-aware down-sampling modules, which can better\nadapt to the global information encoded by transformer and MLP operators. To\nthis end, we jointly search all operators and down-sampling modules in a\nunified search space. Notably, Our searched network UniNet (Unified Network)\noutperforms state-of-the-art pure convolution-based architecture, EfficientNet,\nand pure transformer-based architecture, Swin-Transformer, on multiple public\nvisual benchmarks, ImageNet classification, COCO object detection, and ADE20K\nsemantic segmentation.', 'An Axial Shifted MLP architecture (AS-MLP) is proposed in this paper.\nDifferent from MLP-Mixer, where the global spatial feature is encoded for the\ninformation flow through matrix transposition and one token-mixing MLP, we pay\nmore attention to the local features communication. By axially shifting\nchannels of the feature map, AS-MLP is able to obtain the information flow from\ndifferent axial directions, which captures the local dependencies. Such an\noperation enables us to utilize a pure MLP architecture to achieve the same\nlocal receptive field as CNN-like architecture. We can also design the\nreceptive field size and dilation of blocks of AS-MLP, etc, just like designing\nthose of convolution kernels. With the proposed AS-MLP architecture, our model\nobtains 83.3% Top-1 accuracy with 88M parameters and 15.2 GFLOPs on the\nImageNet-1K dataset. Such a simple yet effective architecture outperforms all\nMLP-based architectures and achieves competitive performance compared to the\ntransformer-based architectures (e.g., Swin Transformer) even with slightly\nlower FLOPs. In addition, AS-MLP is also the first MLP-based architecture to be\napplied to the downstream tasks (e.g., object detection and semantic\nsegmentation). The experimental results are also impressive. Our proposed\nAS-MLP obtains 51.5 mAP on the COCO validation set and 49.5 MS mIoU on the\nADE20K dataset, which is competitive compared to the transformer-based\narchitectures. Code is available at https://github.com/svip-lab/AS-MLP.', 'For the past ten years, CNN has reigned supreme in the world of computer\nvision, but recently, Transformer is on the rise. However, the quadratic\ncomputational cost of self-attention has become a severe problem of practice.\nThere has been much research on architectures without CNN and self-attention in\nthis context. In particular, MLP-Mixer is a simple idea designed using MLPs and\nhit an accuracy comparable to the Vision Transformer. However, the only\ninductive bias in this architecture is the embedding of tokens. Thus, there is\nstill a possibility to build a non-convolutional inductive bias into the\narchitecture itself, and we built in an inductive bias using two simple ideas.\nA way is to divide the token-mixing block vertically and horizontally. Another\nway is to make spatial correlations denser among some channels of token-mixing.\nWith this approach, we were able to improve the accuracy of the MLP-Mixer while\nreducing its parameters and computational complexity. Compared to other\nMLP-based models, the proposed model, named RaftMLP has a good balance of\ncomputational complexity, the number of parameters, and actual memory usage. In\naddition, our work indicates that MLP-based models have the potential to\nreplace CNNs by adopting inductive bias. The source code in PyTorch version is\navailable at \\url{https://github.com/okojoalg/raft-mlp}.']"
23,63,23_neural_pinns_physics_equations,"['neural', 'pinns', 'physics', 'equations', 'models', 'equation', 'pdes', 'flow', 'differential', 'network']","['Physics-informed neural networks (PINNs) show great advantages in solving\npartial differential equations. In this paper, we for the first time propose to\nstudy conformable time fractional diffusion equations by using PINNs. By\nsolving the supervise learning task, we design a new spatio-temporal function\napproximator with high data efficiency. L-BFGS algorithm is used to optimize\nour loss function, and back propagation algorithm is used to update our\nparameters to give our numerical solutions. For the forward problem, we can\ntake IC/BCs as the data, and use PINN to solve the corresponding partial\ndifferential equation. Three numerical examples are are carried out to\ndemonstrate the effectiveness of our methods. In particular, when the order of\nthe conformable fractional derivative $\\alpha$ tends to $1$, a class of\nweighted PINNs is introduced to overcome the accuracy degradation caused by the\nsingularity of solutions. For the inverse problem, we use the data obtained to\ntrain the neural network, and the estimation of parameter $\\lambda$ in the\nequation is elaborated. Similarly, we give three numerical examples to show\nthat our method can accurately identify the parameters, even if the training\ndata is corrupted with 1\\% uncorrelated noise.', 'Physics-informed Machine Learning has recently become attractive for learning\nphysical parameters and features from simulation and observation data. However,\nmost existing methods do not ensure that the physics, such as balance laws\n(e.g., mass, momentum, energy conservation), are constrained. Some recent works\n(e.g., physics-informed neural networks) softly enforce physics constraints by\nincluding partial differential equation (PDE)-based loss functions but need\nre-discretization of the PDEs using auto-differentiation. Training these neural\nnets on observational data showed that one could solve forward and inverse\nproblems in one shot. They evaluate the state variables and the parameters in a\nPDE. This re-discretization of PDEs is not necessarily an attractive option for\ndomain scientists that work with physics-based codes that have been developed\nfor decades with sophisticated discretization techniques to solve complex\nprocess models and advanced equations of state. This paper proposes a physics\nconstrained machine learning framework, AdjointNet, allowing domain scientists\nto embed their physics code in neural network training workflows. This\nembedding ensures that physics is constrained everywhere in the domain.\nAdditionally, the mathematical properties such as consistency, stability, and\nconvergence vital to the numerical solution of a PDE are still satisfied. We\nshow that the proposed AdjointNet framework can be used for parameter\nestimation (and uncertainty quantification by extension) and experimental\ndesign using active learning. The applicability of our framework is\ndemonstrated for four flow cases. Results show that AdjointNet-based inversion\ncan estimate process model parameters with reasonable accuracy. These examples\ndemonstrate the applicability of using existing software with no changes in\nsource code to perform accurate and reliable inversion of model parameters.', 'Recently, surrogate models based on deep learning have attracted much\nattention for engineering analysis and optimization. As the construction of\ndata pairs in most engineering problems is time-consuming, data acquisition is\nbecoming the predictive capability bottleneck of most deep surrogate models,\nwhich also exists in surrogate for thermal analysis and design. To address this\nissue, this paper develops a physics-informed convolutional neural network\n(CNN) for the thermal simulation surrogate. The network can learn a mapping\nfrom heat source layout to the steady-state temperature field without labeled\ndata, which equals solving an entire family of partial difference equations\n(PDEs). To realize the physics-guided training without labeled data, we employ\nthe heat conduction equation and finite difference method to construct the loss\nfunction. Since the solution is sensitive to boundary conditions, we properly\nimpose hard constraints by padding in the Dirichlet and Neumann boundary\nconditions. In addition, the neural network architecture is well-designed to\nimprove the prediction precision of the problem at hand, and pixel-level online\nhard example mining is introduced to overcome the imbalance of optimization\ndifficulty in the computation domain. The experiments demonstrate that the\nproposed method can provide comparable predictions with numerical method and\ndata-driven deep learning models. We also conduct various ablation studies to\ninvestigate the effectiveness of the network component and training methods\nproposed in this paper.']"
24,63,24_blockchain_security_transaction_chain,"['blockchain', 'security', 'transaction', 'chain', 'ledger', 'consensus', 'bft', 'payment', 'transactions', 'protocol']","[""In the given technology-driven era, smart cities are the next frontier of\ntechnology, aiming at improving the quality of people's lives. Many research\nworks focus on future smart cities with a holistic approach towards smart city\ndevelopment. In this paper, we introduce such future smart cities that leverage\nblockchain technology in areas like data security, energy and waste management,\ngovernance, transport, supply chain, including emergency events, and\nenvironmental monitoring. Blockchain, being a decentralized immutable ledger,\nhas the potential to promote the development of smart cities by guaranteeing\ntransparency, data security, interoperability, and privacy. Particularly, using\nblockchain in emergency events will provide interoperability between many\nparties involved in the response, will increase timeliness of services, and\nestablish transparency. In that case, if a current fee-based or\nfirst-come-first-serve-based processing is used, emergency events may get\ndelayed in being processed due to competition, and thus, threatening people's\nlives. Thus, there is a need for transaction prioritization based on the\npriority of information and quick creation of blocks (variable interval block\ncreation mechanism). Also, since the leaders ensure transaction prioritization\nwhile generating blocks, leader rotation and proper election procedure become\nimportant for the transaction prioritization process to take place honestly and\nefficiently. In our consensus protocol, we deploy a machine learning (ML)\nalgorithm to achieve efficient leader election and design a novel dynamic block\ncreation algorithm. Also, to ensure honest assessment from the followers on the\nblocks generated by the leaders, a peer-prediction-based verification mechanism\nis proposed. Both security analysis and simulation experiments are carried out\nto demonstrate the robustness and accuracy of our proposed scheme."", 'In recent years, blockchain has gained widespread attention as an emerging\ntechnology for decentralization, transparency, and immutability in advancing\nonline activities over public networks. As an essential market process,\nauctions have been well studied and applied in many business fields due to\ntheir efficiency and contributions to fair trade. Complementary features\nbetween blockchain and auction models trigger a great potential for research\nand innovation. On the one hand, the decentralized nature of blockchain can\nprovide a trustworthy, secure, and cost-effective mechanism to manage the\nauction process; on the other hand, auction models can be utilized to design\nincentive and consensus protocols in blockchain architectures. These\nopportunities have attracted enormous research and innovation activities in\nboth academia and industry; however, there is a lack of an in-depth review of\nexisting solutions and achievements. In this paper, we conduct a comprehensive\nstate-of-the-art survey of these two research topics. We review the existing\nsolutions for integrating blockchain and auction models, with some\napplication-oriented taxonomies generated. Additionally, we highlight some open\nresearch challenges and future directions towards integrated blockchain-auction\nmodels.', 'Blockchain has been applied to data sharing to ensure the integrity of data\nand chain of custody. Sharing big data such as large biomedical data files is a\nchallenge to blockchain systems since the ledger is not designed to maintain\nbig files, access control is an issue, and users may be dishonest. We call big\ndata such as big files stored outside of a ledger that includes the blockchain\nand world state at a blockchain node as ""off-state"" and propose an off-state\nsharing protocol for a blockchain system to share big data between pairs of\nnodes. In our protocol, only encrypted files are transferred. The cryptographic\nkey is stored in the world state in a secure way and can be accessed only by\nauthorized parties. A receiver has to request the corresponding cryptographic\nkey from the sender to decrypt such encrypted files. All requests are run\nthrough transactions to establish reliable chain of custody. We design and\nimplement a prototypical blockchain off-state sharing system, BOSS, with\nHyperledger Fabric. Extensive experiments were performed to validate the\nfeasibility and performance of BOSS.']"
25,62,25_materials_phonon_thermal_ferroelectric,"['materials', 'phonon', 'thermal', 'ferroelectric', '2d', 'properties', 'conductivity', 'electron', '_2', 'thermoelectric']","['Semiconductors with very low lattice thermal conductivities are highly\ndesired for applications relevant to thermal energy conversion and management,\nsuch as thermoelectrics and thermal barrier coatings. Although the crystal\nstructure and chemical bonding are known to play vital roles in shaping heat\ntransfer behavior, material design approaches of lowering lattice thermal\nconductivity using chemical bonding principles are uncommon. In this work, we\npresent an effective strategy of weakening interatomic interactions and\ntherefore suppressing lattice thermal conductivity based on chemical bonding\nprinciples and develop a high-efficiency approach of discovering low\n$\\kappa_{\\rm L}$ materials by screening the local coordination environments of\ncrystalline compounds. The followed first-principles calculations uncover 30\nhitherto unexplored compounds with (ultra)low lattice thermal conductivities\nfrom thirteen prototype crystal structures contained in the inorganic crystal\nstructure database. Furthermore, we demonstrate an approach of rationally\ndesigning high-performance thermoelectrics by additionally incorporating\ncations with stereochemically active lone-pair electrons. Our results not only\nprovide fundamental insights into the physical origin of the low lattice\nthermal conductivity in a large family of copper-based compounds but also offer\nan efficient approach to discovery and design materials with targeted thermal\ntransport properties.', 'Interactions of charge carriers with lattice vibrations, or phonons, play a\ncritical role in unconventional electronic transport of metals and semimetals.\nRecent observations of phonon-mediated collective electron flow in bulk\nsemimetals, termed electron hydrodynamics, present new opportunities in the\nsearch for strong electron-electron interactions in high carrier density\nmaterials. Here we present the general transport signatures of such a\nsecond-order scattering mechanism, along with analytical limits at the\nEliashberg level of theory. We study electronic transport, using $ab$ $initio$\ncalculations, in finite-size channels of semimetallic ZrSiS and TaAs$_2$ with\nand without topological band crossings, respectively. The order of magnitude\nseparation between momentum-relaxing and momentum-conserving scattering\nlength-scales across a wide temperature range make both of them promising\ncandidates for further experimental observation of electron hydrodynamics. More\ngenerally, our calculations show that the hydrodynamic transport regime can be\nrealized in a much broader class of anisotropic metals and does not, to first\norder, rely on the topological nature of the bands. Finally, we discuss general\ndesign principles guiding future search for hydrodynamic candidates, based on\nthe analytical formulation and our $ab$ $initio$ predictions. We find that\nsystems with strong electron-phonon interactions, reduced electronic phase\nspace, and suppressed phonon-phonon scattering at temperatures of interest are\nlikely to feature hydrodynamic electron transport. We predict that layered\nand/or anisotropic semimetals composed of half-filled $d$-shells and light\ngroup V/VI elements with lower crystal symmetry are ideal candidates to observe\nhydrodynamic phenomena in future.', ""Understanding thermal transport in layered transition metal dichalcogenide\n(TMD) crystals is crucial for a myriad of applications exploiting these\nmaterials. Despite significant efforts, several basic thermal transport\nproperties of TMDs are currently not well understood. Here, we present a\ncombined experimental-theoretical study of the intrinsic lattice thermal\nconductivity of the representative TMD MoSe$_2$, focusing on the effect of\nmaterial thickness and the material's environment. We use Raman thermometry\nmeasurements on suspended crystals, where we identify and eliminate crucial\nartefacts, and perform $ab$ $initio$ simulations with phonons at finite, rather\nthan zero, temperature. We find that phonon dispersions and lifetimes change\nstrongly with thickness, yet (sub)nanometer thin TMD films exhibit a similar\nin-plane thermal conductivity ($\\sim$20~Wm$^{-1}$K$^{-1}$) as bulk crystals\n($\\sim$40~Wm$^{-1}$K$^{-1}$). This is the result of compensating phonon\ncontributions, in particular low-frequency modes with a surprisingly long mean\nfree path of several micrometers that contribute significantly to thermal\ntransport for monolayers. We furthermore demonstrate that out-of-plane heat\ndissipation to air is remarkably efficient, in particular for the thinnest\ncrystals. These results are crucial for the design of TMD-based applications in\nthermal management, thermoelectrics and (opto)electronics.""]"
26,61,26_point_3d_cloud_lidar,"['point', '3d', 'cloud', 'lidar', 'clouds', 'depth', 'registration', 'object', 'odometry', 'kitti']","['Recently deep learning has achieved significant progress on point cloud\nanalysis tasks. Learning good representations is of vital importance to these\ntasks. Most current methods rely on massive labelled data for training. We here\npropose a point discriminative learning method for unsupervised representation\nlearning on 3D point clouds, which can learn local and global geometry\nfeatures. We achieve this by imposing a novel point discrimination loss on the\nmiddle level and global level point features produced in the backbone network.\nThis point discrimination loss enforces the features to be consistent with\npoints belonging to the shape surface and inconsistent with randomly sampled\nnoisy points. Our method is simple in design, which works by adding an extra\nadaptation module and a point consistency module for unsupervised training of\nthe encoder in the backbone network. Once trained, these two modules can be\ndiscarded during supervised training of the classifier or decoder for\ndown-stream tasks. We conduct extensive experiments on 3D object\nclassification, 3D part segmentation and shape reconstruction in various\nunsupervised and transfer settings. Both quantitative and qualitative results\nshow that our method learns powerful representations and achieves new\nstate-of-the-art performance.', 'Point clouds captured in real-world applications are often incomplete due to\nthe limited sensor resolution, single viewpoint, and occlusion. Therefore,\nrecovering the complete point clouds from partial ones becomes an indispensable\ntask in many practical applications. In this paper, we present a new method\nthat reformulates point cloud completion as a set-to-set translation problem\nand design a new model, called PoinTr that adopts a transformer encoder-decoder\narchitecture for point cloud completion. By representing the point cloud as a\nset of unordered groups of points with position embeddings, we convert the\npoint cloud to a sequence of point proxies and employ the transformers for\npoint cloud generation. To facilitate transformers to better leverage the\ninductive bias about 3D geometric structures of point clouds, we further devise\na geometry-aware block that models the local geometric relationships\nexplicitly. The migration of transformers enables our model to better learn\nstructural knowledge and preserve detailed information for point cloud\ncompletion. Furthermore, we propose two more challenging benchmarks with more\ndiverse incomplete point clouds that can better reflect the real-world\nscenarios to promote future research. Experimental results show that our method\noutperforms state-of-the-art methods by a large margin on both the new\nbenchmarks and the existing ones. Code is available at\nhttps://github.com/yuxumin/PoinTr', 'State-of-the-art methods for driving-scene LiDAR-based perception (including\npoint cloud semantic segmentation, panoptic segmentation and 3D detection,\n\\etc) often project the point clouds to 2D space and then process them via 2D\nconvolution. Although this cooperation shows the competitiveness in the point\ncloud, it inevitably alters and abandons the 3D topology and geometric\nrelations. A natural remedy is to utilize the 3D voxelization and 3D\nconvolution network. However, we found that in the outdoor point cloud, the\nimprovement obtained in this way is quite limited. An important reason is the\nproperty of the outdoor point cloud, namely sparsity and varying density.\nMotivated by this investigation, we propose a new framework for the outdoor\nLiDAR segmentation, where cylindrical partition and asymmetrical 3D convolution\nnetworks are designed to explore the 3D geometric pattern while maintaining\nthese inherent properties. The proposed model acts as a backbone and the\nlearned features from this model can be used for downstream tasks such as point\ncloud semantic and panoptic segmentation or 3D detection. In this paper, we\nbenchmark our model on these three tasks. For semantic segmentation, we\nevaluate the proposed model on several large-scale datasets, \\ie,\nSemanticKITTI, nuScenes and A2D2. Our method achieves the state-of-the-art on\nthe leaderboard of SemanticKITTI (both single-scan and multi-scan challenge),\nand significantly outperforms existing methods on nuScenes and A2D2 dataset.\nFurthermore, the proposed 3D framework also shows strong performance and good\ngeneralization on LiDAR panoptic segmentation and LiDAR 3D detection.']"
27,61,27_explanations_prediction_temporal_mobility,"['explanations', 'prediction', 'temporal', 'mobility', 'clinical', 'traffic', 'forecasting', 'counterfactual', 'care', 'spatiotemporal']","['In this paper, we address the ""black-box"" problem in predictive process\nanalytics by building interpretable models that are capable to inform both what\nand why is a prediction. Predictive process analytics is a newly emerged\ndiscipline dedicated to providing business process intelligence in modern\norganisations. It uses event logs, which capture process execution traces in\nthe form of multi-dimensional sequence data, as the key input to train\npredictive models. These predictive models, often built upon deep learning\ntechniques, can be used to make predictions about the future states of business\nprocess execution. We apply attention mechanism to achieve model\ninterpretability. We propose i) two types of attentions: event attention to\ncapture the impact of specific process events on a prediction, and attribute\nattention to reveal which attribute(s) of an event influenced the prediction;\nand ii) two attention mechanisms: shared attention mechanism and specialised\nattention mechanism to reflect different design decisions in when to construct\nattribute attention on individual input features (specialised) or using the\nconcatenated feature tensor of all input feature vectors (shared). These lead\nto two distinct attention-based models, and both are interpretable models that\nincorporate interpretability directly into the structure of a process\npredictive model. We conduct experimental evaluation of the proposed models\nusing real-life dataset, and comparative analysis between the models for\naccuracy and interpretability, and draw insights from the evaluation and\nanalysis results.', 'Predictive process analytics often apply machine learning to predict the\nfuture states of a running business~process. However, the internal mechanisms\nof many existing predictive algorithms are opaque and a human decision-maker is\nunable to understand \\emph{why} a certain activity was predicted. Recently,\ncounterfactuals have been proposed in the literature to derive\nhuman-understandable explanations from predictive models. Current\ncounterfactual approaches consist of finding the minimum feature change that\ncan make a certain prediction flip its outcome. Although many algorithms have\nbeen proposed, their application to multi-dimensional sequence data like event\nlogs has not been explored in the literature.\n  In this paper, we explore the use of a recent, popular model-agnostic\ncounterfactual algorithm, DiCE, in the context of predictive process analytics.\nThe analysis reveals that DiCE is unable to derive explanations for process\npredictions, due to (1) process domain knowledge not being taken into account,\n(2) long traces of process execution that often tend to be less understandable,\nand (3) difficulties in optimising the counterfactual search with categorical\nvariables. We design an extension of DiCE, namely DiCE4EL (DiCE for Event\nLogs), that can generate counterfactual explanations for process prediction,\nand propose an approach that supports deriving milestone-aware counterfactual\nexplanations at key intermediate stages along process execution to promote\ninterpretability. We apply our approach to a publicly available real-life event\nlog and the analysis results demonstrate the effectiveness of the proposed\napproach.', 'Spatiotemporal forecasting plays an essential role in various applications in\nintelligent transportation systems (ITS), such as route planning, navigation,\nand traffic control and management. Deep Spatiotemporal graph neural networks\n(GNNs), which capture both spatial and temporal patterns, have achieved great\nsuccess in traffic forecasting applications. Understanding how GNNs-based\nforecasting work and the vulnerability and robustness of these models becomes\ncritical to real-world applications. For example, if spatiotemporal GNNs are\nvulnerable in real-world traffic prediction applications, a hacker can easily\nmanipulate the results and cause serious traffic congestion and even a\ncity-scale breakdown. However, despite that recent studies have demonstrated\nthat deep neural networks (DNNs) are vulnerable to carefully designed\nperturbations in multiple domains like objection classification and graph\nrepresentation, current adversarial works cannot be directly applied to\nspatiotemporal forecasting due to the causal nature and spatiotemporal\nmechanisms in forecasting models. To fill this gap, in this paper we design\nSpatially Focused Attack (SFA) to break spatiotemporal GNNs by attacking a\nsingle vertex. To achieve this, we first propose the inverse estimation to\naddress the causality issue; then, we apply genetic algorithms with a universal\nattack method as the evaluation function to locate the weakest vertex; finally,\nperturbations are generated by solving an inverse estimation-based optimization\nproblem. We conduct experiments on real-world traffic data and our results show\nthat perturbations in one vertex designed by SA can be diffused into a large\npart of the graph.']"
28,59,28_hardware_accelerator_memory_accelerators,"['hardware', 'accelerator', 'memory', 'accelerators', 'chip', 'fpga', 'inference', 'systolic', 'accuracy', 'edge']","['Realizing today\'s cloud-level artificial intelligence functionalities\ndirectly on devices distributed at the edge of the internet calls for edge\nhardware capable of processing multiple modalities of sensory data (e.g. video,\naudio) at unprecedented energy-efficiency. AI hardware architectures today\ncannot meet the demand due to a fundamental ""memory wall"": data movement\nbetween separate compute and memory units consumes large energy and incurs long\nlatency. Resistive random-access memory (RRAM) based compute-in-memory (CIM)\narchitectures promise to bring orders of magnitude energy-efficiency\nimprovement by performing computation directly within memory. However,\nconventional approaches to CIM hardware design limit its functional flexibility\nnecessary for processing diverse AI workloads, and must overcome hardware\nimperfections that degrade inference accuracy. Such trade-offs between\nefficiency, versatility and accuracy cannot be addressed by isolated\nimprovements on any single level of the design. By co-optimizing across all\nhierarchies of the design from algorithms and architecture to circuits and\ndevices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM\nto simultaneously deliver a high degree of versatility for diverse model\narchitectures, record energy-efficiency $5\\times$ - $8\\times$ better than prior\nart across various computational bit-precisions, and inference accuracy\ncomparable to software models with 4-bit weights on all measured standard AI\nbenchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image\nclassification, 84.7% accuracy on Google speech command recognition, and a 70%\nreduction in image reconstruction error on a Bayesian image recovery task. This\nwork paves a way towards building highly efficient and reconfigurable edge AI\nhardware platforms for the more demanding and heterogeneous AI applications of\nthe future.', ""Edge computing devices inherently face tight resource constraints, which is\nespecially apparent when deploying Deep Neural Networks (DNN) with high memory\nand compute demands. FPGAs are commonly available in edge devices. Since these\nreconfigurable circuits can achieve higher throughput and lower power\nconsumption than general purpose processors, they are especially well-suited\nfor DNN acceleration. However, existing solutions for designing FPGA-based DNN\naccelerators for edge devices come with high development overheads, given the\ncost of repeated FPGA synthesis passes, reimplementation in a Hardware\nDescription Language (HDL) of the simulated design, and accelerator system\nintegration.\n  In this paper we propose SECDA, a new hardware/software co-design methodology\nto reduce design time of optimized DNN inference accelerators on edge devices\nwith FPGAs. SECDA combines cost-effective SystemC simulation with hardware\nexecution, streamlining design space exploration and the development process\nvia reduced design evaluation time. As a case study, we use SECDA to\nefficiently develop two different DNN accelerator designs on a PYNQ-Z1 board, a\nplatform that includes an edge FPGA. We quickly and iteratively explore the\nsystem's hardware/software stack, while identifying and mitigating performance\nbottlenecks. We evaluate the two accelerator designs with four common DNN\nmodels, achieving an average performance speedup across models of up to\n3.5$\\times$ with a 2.9$\\times$ reduction in energy consumption over CPU-only\ninference. Our code is available at https://github.com/gicLAB/SECDA"", 'Deep Neural Networks (DNNs), as a subset of Machine Learning (ML) techniques,\nentail that real-world data can be learned and that decisions can be made in\nreal-time. However, their wide adoption is hindered by a number of software and\nhardware limitations. The existing general-purpose hardware platforms used to\naccelerate DNNs are facing new challenges associated with the growing amount of\ndata and are exponentially increasing the complexity of computations. An\nemerging non-volatile memory (NVM) devices and processing-in-memory (PIM)\nparadigm is creating a new hardware architecture generation with increased\ncomputing and storage capabilities. In particular, the shift towards\nReRAM-based in-memory computing has great potential in the implementation of\narea and power efficient inference and in training large-scale neural network\narchitectures. These can accelerate the process of the IoT-enabled AI\ntechnologies entering our daily life. In this survey, we review the\nstate-of-the-art ReRAM-based DNN many-core accelerators, and their superiority\ncompared to CMOS counterparts was shown. The review covers different aspects of\nhardware and software realization of DNN accelerators, their present\nlimitations, and future prospectives. In particular, comparison of the\naccelerators shows the need for the introduction of new performance metrics and\nbenchmarking standards. In addition, the major concerns regarding the efficient\ndesign of accelerators include a lack of accuracy in simulation tools for\nsoftware and hardware co-design.']"
29,59,29_photonic_mode_cavity_optical,"['photonic', 'mode', 'cavity', 'optical', 'waveguide', 'laser', 'soliton', 'coupling', 'integrated', 'photon']","[""We introduce a photonic crystal ring cavity that resembles an internal gear\nand unites photonic crystal (PhC) and whispering gallery mode (WGM) concepts.\nThis `microgear' photonic crystal ring (MPhCR) is created by applying a\nperiodic modulation to the inside boundary of a microring resonator to open a\nlarge bandgap, as in a PhC cavity, while maintaining the ring's circularly\nsymmetric outside boundary and high quality factor ($Q$), as in a WGM cavity.\nThe MPhCR targets a specific WGM to open a large PhC bandgap up to tens of free\nspectral ranges, compressing the mode spectrum while maintaining the high-$Q$,\nangular momenta, and waveguide coupling properties of the WGM modes. In\nparticular, near the dielectric band-edge, we observe modes whose group\nvelocity is slowed down by 10 times relative to conventional microring modes\nwhile supporting $Q~=~(1.1\\pm0.1)\\times10^6$. This $Q$ is $\\approx$50$\\times$\nthat of the previous record in slow light devices. Using the slow light design\nas a starting point, we further demonstrate the ability to localize WGMs into\nphotonic crystal defect (dPhC) modes for the first time, enabling a more than\n10$\\times$ reduction of mode volume compared to conventional WGMs while\nmaintaining high-$Q$ up to (5.6$\\pm$0.1)$\\times$10$^5$. Importantly, this\nadditional dPhC localization is achievable without requiring detailed\nelectromagnetic design. Moreover, controlling their frequencies and waveguide\ncoupling is straightforward in the MPhCR, thanks to its WGM heritage. By using\na PhC to strongly modify fundamental properties of WGMs, such as group velocity\nand localization, the MPhCR provides an exciting platform for a broad range of\nphotonics applications, including sensing/metrology, nonlinear optics, and\ncavity quantum electrodynamics."", 'Silicon carbide has recently emerged as a promising photonics material due to\nits unique properties, including possessing strong second- and third-order\nnonlinear coefficients and hosting various color centers that can be utilized\nfor a wealth of quantum applications. Here, we report the design and\ndemonstration of octave-spanning microcombs in a\n4H-silicon-carbide-on-insulator platform for the first time. Such broadband\noperation is enabled by optimized nanofabrication achieving >1 million\nintrinsic quality factors in a 36-$\\mu$m-radius microring resonator, and\ncareful dispersion engineering by investigating the dispersion properties of\ndifferent mode families. For example, for the fundamental transverse-electric\nmode whose dispersion can be tailored by simply varying the microring waveguide\nwidth, we realized a microcomb spectrum covering the wavelength range from 1100\nnm to 2400 nm with an on-chip power near 120 mW. While the observed comb state\nis verified to be chaotic and not soliton, attaining such a large bandwidth is\na crucial step towards realizing $f$-2$f$ self-referencing. In addition, we\nhave also observed coherent soliton-crystal state for the fundamental\ntransverse-magnetic mode, which exhibits stronger dispersion than the\nfundamental transverse-electric mode and hence a narrower bandwidth.', 'Portable mid-infrared (mid-IR) spectroscopy and sensing applications require\nwidely tunable, narrow linewidth, chip-scale, single-mode sources without\nsacrificing significant output power. However, no such lasers have been\ndemonstrated beyond 3 $\\mu$m due to the challenge of building tunable, high\nquality-factor (Q) on-chip cavities. We demonstrate a tunable, single-mode\nmid-IR laser at 3.4 $\\mu$m using a high-Q silicon microring cavity with\nintegrated heaters and a multi-mode Interband Cascade Laser (ICL). We show that\nthe multiple longitudinal modes of an ICL collapse into a single frequency via\nself-injection locking with an output power of 0.4 mW and achieve an oxide-clad\nhigh confinement waveguide microresonator with a loaded Q of $2.8\\times 10^5$.\nUsing integrated microheaters, our laser exhibits a wide tuning range of 54 nm\nat 3.4 $\\mu$m with 3 dB output power variation. We further measure an\nupper-bound effective linewidth of 9.1 MHz from the locked laser using a\nscanning Fabry-Perot interferometer. Our design of a single-mode laser based on\na tunable high-Q microresonator can be expanded to quantum-cascade lasers at\nhigher wavelengths and lead to the development of compact, portable,\nhigh-performance mid-IR sensors for spectroscopic and sensing applications.']"
30,58,30_order_schemes_equations_numerical,"['order', 'schemes', 'equations', 'numerical', 'scheme', 'equation', 'problems', 'navier', 'galerkin', 'convergence']","['In this paper, we use an implicit two-derivative deferred correction time\ndiscretization approach and combine it with a spatial discretization of the\ndiscontinuous Galerkin spectral element method to solve (non-)linear PDEs. The\nresulting numerical method is high order accurate in space and time. As the\nnovel scheme handles two time derivatives, the spatial operator for both\nderivatives has to be defined. This results in an extended system matrix of the\nscheme. We analyze this matrix regarding possible simplifications and an\nefficient way to solve the arising (non-)linear system of equations. It is\nshown how a carefully designed preconditioner and a matrix-free approach allow\nfor an efficient implementation and application of the novel scheme. For both,\nlinear advection and the compressible Euler equations, up to eighth order of\naccuracy in time is shown. Finally, it is illustrated how the method can be\nused to approximate solutions to the compressible Navier-Stokes equations.', 'This paper extends a new class of positivity-preserving, entropy stable\nspectral collocation schemes developed for the one-dimensional compressible\nNavier-Stokes equations in [1,2] to three spatial dimensions. The new\nhigh-order schemes are provably L2 stable, design-order accurate for smooth\nsolutions, and guarantee the pointwise positivity of thermodynamic variables\nfor 3-D compressible viscous flows. Similar to the 1-D counterpart, the\nproposed schemes for the 3-D Navier-Stokes equations are constructed by using a\nflux-limiting technique that combines a positivity-violating entropy stable\nmethod of arbitrary order of accuracy and a novel first-order\npositivity-preserving entropy stable finite volume-type scheme discretized on\nthe same Legendre-Gauss-Lobatto grid points used for constructing the\nhigh-order discrete operators. The positivity preservation and excellent\ndiscontinuity-capturing properties are achieved by adding an artificial\ndissipation in the form of the low- and high-order Brenner-Navier-Stokes\ndiffusion operators. To our knowledge, this is the first family of\npositivity-preserving, entropy stable schemes of arbitrary order of accuracy\nfor the 3-D compressible Navier-Stokes equations.', 'In this paper, we extend the positivity-preserving, entropy stable\nfirst-order finite volume-type scheme developed for the one-dimensional\ncompressible Navier-Stokes equations in [1] to three spatial dimensions. The\nnew first-order scheme is provably entropy stable, design-order accurate for\nsmooth solutions, and guarantees the pointwise positivity of thermodynamic\nvariables for 3-D compressible viscous flows. Similar to the 1-D counterpart,\nthe proposed scheme for the 3-D Navier-Stokes equations is discretized on\nLegendre-Gauss-Lobatto grids used for high-order spectral collocation methods.\nThe positivity of density is achieved by adding an artificial dissipation in\nthe form of the first-order Brenner-Navier-Stokes diffusion operator. Another\ndistinctive feature of the proposed scheme is that the Navier-Stokes viscous\nterms are discretized by high-order spectral collocation summation-by-parts\noperators. To eliminate time step stiffness caused by the high-order\napproximation of the viscous terms, the velocity and temperature limiters\ndeveloped for the 1-D compressible Navier-Stokes equations in [1] are\ngeneralized to three spatial dimensions. These limiters bound the magnitude of\nvelocity and temperature gradients and preserve the entropy stability and\npositivity properties of the baseline scheme. Numerical results are presented\nto demonstrate design-order accuracy and positivity-preserving properties of\nthe new first-order scheme for 2-D and 3-D inviscid and viscous flows with\nstrong shocks and contact discontinuities.']"
31,57,31_cloud_computing_network_traffic,"['cloud', 'computing', 'network', 'traffic', 'edge', 'resources', 'service', 'vr', 'application', 'data']","['Traffic management systems capture tremendous video data and leverage\nadvances in video processing to detect and monitor traffic incidents. The\ncollected data are traditionally forwarded to the traffic management center\n(TMC) for in-depth analysis and may thus exacerbate the network paths to the\nTMC. To alleviate such bottlenecks, we propose to utilize edge computing by\nequipping edge nodes that are close to cameras with computing resources (e.g.\ncloudlets). A cloudlet, with limited computing resources as compared to TMC,\nprovides limited video processing capabilities. In this paper, we focus on two\ncommon traffic monitoring tasks, congestion detection, and speed detection, and\npropose a two-tier edge computing based model that takes into account of both\nthe limited computing capability in cloudlets and the unstable network\ncondition to the TMC. Our solution utilizes two algorithms for each task, one\nimplemented at the edge and the other one at the TMC, which are designed with\nthe consideration of different computing resources. While the TMC provides\nstrong computation power, the video quality it receives depends on the\nunderlying network conditions. On the other hand, the edge processes very\nhigh-quality video but with limited computing resources. Our model captures\nthis trade-off. We evaluate the performance of the proposed two-tier model as\nwell as the traffic monitoring algorithms via test-bed experiments under\ndifferent weather as well as network conditions and show that our proposed\nhybrid edge-cloud solution outperforms both the cloud-only and edge-only\nsolutions.', ""Recently, the boosting growth of computation-heavy applications raises great\nchallenges for the Fifth Generation (5G) and future wireless networks. As\nresponding, the hybrid edge and cloud computing (ECC) system has been expected\nas a promising solution to handle the increasing computational applications\nwith low-latency and on-demand services of computation offloading, which\nrequires new computing resource sharing and access control technology\nparadigms. This work establishes a software-defined networking (SDN) based\narchitecture for edge/cloud computing services in 5G heterogeneous networks\n(HetNets), which can support efficient and on-demand computing resource\nmanagement to optimize resource utilization and satisfy the time-varying\ncomputational tasks uploaded by user devices. In addition, resulting from the\ninformation incompleteness, we design an evolutionary game based service\nselection for users, which can model the replicator dynamics of service\nsubscription. Based on this dynamic access model, a Stackelberg differential\ngame based cloud computing resource sharing mechanism is proposed to facilitate\nthe resource trading between the cloud computing service provider (CCP) and\ndifferent edge computing service providers (ECPs). Then we derive the optimal\npricing and allocation strategies of cloud computing resource based on the\nreplicator dynamics of users' service selection. These strategies can promise\nthe maximum integral utilities to all computing service providers (CPs),\nmeanwhile the user distribution can reach the evolutionary stable state at this\nStackelberg equilibrium. Furthermore, simulation results validate the\nperformance of the designed resource sharing mechanism, and reveal the\nconvergence and equilibrium states of user selection, and computing resource\npricing and allocation."", 'Edge computing is an emerging paradigm to enable low-latency applications,\nlike mobile augmented reality, because it takes the computation on processing\ndevices that are closer to the users. On the other hand, the need for highly\nscalable execution of stateless tasks for cloud systems is driving the\ndefinition of new technologies based on serverless computing. In this paper, we\npropose a novel architecture where the two converge to enable low-latency\napplications: this is achieved by offloading short-lived stateless tasks from\nthe user terminals to edge nodes. Furthermore, we design a distributed\nalgorithm that tackles the research challenge of selecting the best executor,\nbased on real-time measurements and simple, yet effective, prediction\nalgorithms. Finally, we describe a new performance evaluation framework\nspecifically designed for an accurate assessment of algorithms and protocols in\nedge computing environments, where the nodes may have very heterogeneous\nnetworking and processing capabilities. The proposed framework relies on the\nuse of real components on lightweight virtualization mixed with simulated\ncomputation and is well-suited to the analysis of several applications and\nnetwork environments. Using our framework, we evaluate our proposed\narchitecture and algorithms in small- and large-scale edge computing scenarios,\nshowing that our solution achieves similar or better delay performance than a\ncentralized solution, with far less network utilization.']"
32,57,32_memory_dram_cache_hardware,"['memory', 'dram', 'cache', 'hardware', 'gpu', 'accelerators', 'fpga', 'performance', 'workloads', 'applications']","[""Memory disaggregation has attracted great attention recently because of its\nbenefits in efficient memory utilization and ease of management. So far, memory\ndisaggregation research has all taken one of two approaches, building/emulating\nmemory nodes with either regular servers or raw memory devices with no\nprocessing power. The former incurs higher monetary cost and face tail latency\nand scalability limitations, while the latter introduce performance, security,\nand management problems.\n  Server-based memory nodes and memory nodes with no processing power are two\nextreme approaches. We seek a sweet spot in the middle by proposing a\nhardware-based memory disaggregation solution that has the right amount of\nprocessing power at memory nodes. Furthermore, we take a clean-slate approach\nby starting from the requirements of memory disaggregation and designing a\nmemory-disaggregation-native system.\n  We propose a hardware-based disaggregated memory system, Clio, that\nvirtualizes and manages disaggregated memory at the memory node. Clio includes\na new hardware-based virtual memory system, a customized network system, and a\nframework for computation offloading. In building Clio, we not only co-design\nOS functionalities, hardware architecture, and the network system, but also\nco-design the compute node and memory node. We prototyped Clio's memory node\nwith FPGA and implemented its client-node functionalities in a user-space\nlibrary. Clio achieves 100 Gbps throughput and an end-to-end latency of 2.5 us\nat median and 3.2 us at the 99th percentile. Clio scales much better and has\norders of magnitude lower tail latency than RDMA, and it has 1.1x to 3.4x\nenergy saving compared to CPU-based and SmartNIC-based disaggregated memory\nsystems and is 2.7x faster than software-based SmartNIC solutions."", 'Traditional graphics processing units (GPUs) suffer from the low memory\ncapacity and demand for high memory bandwidth. To address these challenges, we\npropose Ohm-GPU, a new optical network based heterogeneous memory design for\nGPUs. Specifically, Ohm-GPU can expand the memory capacity by combing a set of\nhigh-density 3D XPoint and DRAM modules as heterogeneous memory. To prevent\nmemory channels from throttling throughput of GPU memory system, Ohm-GPU\nreplaces the electrical lanes in the traditional memory channel with a\nhigh-performance optical network. However, the hybrid memory can introduce\nfrequent data migrations between DRAM and 3D XPoint, which can unfortunately\noccupy the memory channel and increase the optical network traffic. To prevent\nthe intensive data migrations from blocking normal memory services, Ohm-GPU\nrevises the existing memory controller and designs a new optical network\ninfrastructure, which enables the memory channel to serve the data migrations\nand memory requests, in parallel. Our evaluation results reveal that Ohm-GPU\ncan improve the performance by 181% and 27%, compared to a DRAM-based GPU\nmemory system and the baseline optical network based heterogeneous memory\nsystem, respectively.', 'Even with generational improvements in DRAM technology, memory access latency\nstill remains the major bottleneck for application accelerators, primarily due\nto limitations in memory interface IPs which cannot fully account for\nvariations in target applications, the algorithms used, and accelerator\narchitectures. Since developing memory controllers for different applications\nis time-consuming, this paper introduces a modular and programmable memory\ncontroller that can be configured for different target applications on\navailable hardware resources. The proposed memory controller efficiently\nsupports cache-line accesses along with bulk memory transfers. The user can\nconfigure the controller depending on the available logic resources on the\nFPGA, memory access pattern, and external memory specifications. The modular\ndesign supports various memory access optimization techniques including,\nrequest scheduling, internal caching, and direct memory access. These\ntechniques contribute to reducing the overall latency while maintaining high\nsustained bandwidth. We implement the system on a state-of-the-art FPGA and\nevaluate its performance using two widely studied domains: graph analytics and\ndeep learning workloads. We show improved overall memory access time up to 58%\non CNN and GCN workloads compared with commercial memory controller IPs.']"
33,56,33_signals_rf_har_eeg,"['signals', 'rf', 'har', 'eeg', 'recognition', 'hrnv', 'human', 'to', 'signal', 'wearable']","['In the field of neuroscience, Brain activity analysis is always considered as\nan important area. Schizophrenia(Sz) is a brain disorder that severely affects\nthe thinking, behaviour, and feelings of people all around the world.\nElectroencephalography (EEG) is proved to be an efficient biomarker in Sz\ndetection. EEG is a non-linear time-seriesi signal and utilizing it for\ninvestigation is rather crucial due to its non-linear structure. This paper\naims to improve the performance of EEG based Sz detection using a deep learning\napproach. A novel hybrid deep learning model known as SzHNN (Schizophrenia\nHybrid Neural Network), a combination of Convolutional Neural Networks (CNN)\nand Long Short-Term Memory (LSTM) has been proposed. CNN network is used for\nlocal feature extraction and LSTM has been utilized for classification. The\nproposed model has been compared with CNN only, LSTM only, and machine\nlearning-based models. All the models have been evaluated on two different\ndatasets wherein Dataset 1 consists of 19 subjects and Dataset 2 consists of 16\nsubjects. Several experiments have been conducted for the same using various\nparametric settings on different frequency bands and using different sets of\nelectrodes on the scalp. Based on all the experiments, it is evident that the\nproposed hybrid model (SzHNN) provides the highest classification accuracy of\n99.9% in comparison to other existing models. The proposed model overcomes the\ninfluence of different frequency bands and even showed a much better accuracy\nof 91% with only 5 electrodes. The proposed model is also evaluated on the\nInternet of Medical Things (IoMT) framework for smart healthcare and remote\nmonitoring applications.', 'Human Activity Recognition (HAR) plays a critical role in a wide range of\nreal-world applications, and it is traditionally achieved via wearable sensing.\nRecently, to avoid the burden and discomfort caused by wearable devices,\ndevice-free approaches exploiting RF signals arise as a promising alternative\nfor HAR. Most of the latest device-free approaches require training a large\ndeep neural network model in either time or frequency domain, entailing\nextensive storage to contain the model and intensive computations to infer\nactivities. Consequently, even with some major advances on device-free HAR,\ncurrent device-free approaches are still far from practical in real-world\nscenarios where the computation and storage resources possessed by, for\nexample, edge devices, are limited. Therefore, we introduce HAR-SAnet which is\na novel RF-based HAR framework. It adopts an original signal adapted\nconvolutional neural network architecture: instead of feeding the handcraft\nfeatures of RF signals into a classifier, HAR-SAnet fuses them adaptively from\nboth time and frequency domains to design an end-to-end neural network model.\nWe apply point-wise grouped convolution and depth-wise separable convolutions\nto confine the model scale and to speed up the inference execution time. The\nexperiment results show that the recognition accuracy of HAR-SAnet outperforms\nstate-of-the-art algorithms and systems.', 'Radio-Frequency (RF) based device-free Human Activity Recognition (HAR) rises\nas a promising solution for many applications. However, device-free (or\ncontactless) sensing is often more sensitive to environment changes than\ndevice-based (or wearable) sensing. Also, RF datasets strictly require on-line\nlabeling during collection, starkly different from image and text data\ncollections where human interpretations can be leveraged to perform off-line\nlabeling. Therefore, existing solutions to RF-HAR entail a laborious data\ncollection process for adapting to new environments. To this end, we propose\nRF-Net as a meta-learning based approach to one-shot RF-HAR; it reduces the\nlabeling efforts for environment adaptation to the minimum level. In\nparticular, we first examine three representative RF sensing techniques and two\nmajor meta-learning approaches. The results motivate us to innovate in two\ndesigns: i) a dual-path base HAR network, where both time and frequency domains\nare dedicated to learning powerful RF features including spatial and\nattention-based temporal ones, and ii) a metric-based meta-learning framework\nto enhance the fast adaption capability of the base network, including an\nRF-specific metric module along with a residual classification module. We\nconduct extensive experiments based on all three RF sensing techniques in\nmultiple real-world indoor environments; all results strongly demonstrate the\nefficacy of RF-Net compared with state-of-the-art baselines.']"
34,53,34_type_calculus_proof_language,"['type', 'calculus', 'proof', 'language', 'semantics', 'typed', 'verification', 'reasoning', 'languages', 'logic']","[""Gradually typed languages allow programmers to mix statically and dynamically\ntyped code, enabling them to incrementally reap the benefits of static typing\nas they add type annotations to their code. However, this type migration\nprocess is typically a manual effort with limited tool support. This paper\nexamines the problem of \\emph{automated type migration}: given a dynamic\nprogram, infer additional or improved type annotations.\n  Existing type migration algorithms prioritize different goals, such as\nmaximizing type precision, maintaining compatibility with unmigrated code, and\npreserving the semantics of the original program. We argue that the type\nmigration problem involves fundamental compromises: optimizing for a single\ngoal often comes at the expense of others. Ideally, a type migration tool would\nflexibly accommodate a range of user priorities.\n  We present TypeWhich, a new approach to automated type migration for the\ngradually-typed lambda calculus with some extensions. Unlike prior work, which\nrelies on custom solvers, TypeWhich produces constraints for an off-the-shelf\nMaxSMT solver. This allows us to easily express objectives, such as minimizing\nthe number of necessary syntactic coercions, and constraining the type of the\nmigration to be compatible with unmigrated code.\n  We present the first comprehensive evaluation of GTLC type migration\nalgorithms, and compare TypeWhich to four other tools from the literature. Our\nevaluation uses prior benchmarks, and a new set of ``challenge problems.''\nMoreover, we design a new evaluation methodology that highlights the subtleties\nof gradual type migration. In addition, we apply TypeWhich to a suite of\nbenchmarks for Grift, a programming language based on the GTLC. TypeWhich is\nable to reconstruct all human-written annotations on all but one program."", 'Proof assistants are getting more widespread use in research and industry to\nprovide certified and independently checkable guarantees about theories,\ndesigns, systems and implementations. However, proof assistant implementations\nthemselves are seldom verified, although they take a major share of the trusted\ncode base in any such certification effort. In this area, proof assistants\nbased on Higher-Order Logic enjoy stronger guarantees, as self-certified\nimplementations have been available for some years. One cause of this\ndifference is the inherent complexity of dependent type theories together with\ntheir extensions with inductive types, universe polymorphism and complex sort\nsystems, and the gap between theory on paper and practical implementations in\nefficient programming languages. MetaCoq is a collaborative project that aims\nto tackle these difficulties to provide the first fully-certified realistic\nimplementation of a type checker for the full calculus underlying the Coq proof\nassistant. To achieve this, we refined the sometimes blurry, if not incorrect,\nspecification and implementation of the system. We show how theoretical tools\nfrom this community such as bidirectional type-checking,\nTait-Martin-L\\""of/Takahashi\'s confluence proof technique and monadic and\ndependently-typed programming can help construct the following artefacts: a\nspecification of Coq\'s syntax and type theory, the Polymorphic Cumulative\nCalculus of (Co)-Inductive Constructions (PCUIC); a monad for the manipulation\nof raw syntax and interaction with the Coq system; a verification of PCUIC\'s\nmetatheory, whose main results are the confluence of reduction, type\npreservation and principality of typing; a realistic, correct and complete\ntype-checker for PCUIC; a sound type and proof erasure procedure from PCUIC to\nuntyped lambda-calculus, i.e., the core of the extraction mechanism of Coq.', 'Harnessing the power of dependently typed languages can be difficult.\nProgrammers must manually construct proofs to produce well-typed programs,\nwhich is not an easy task. In particular, migrating code to these languages is\nchallenging. Gradual typing can make dependently-typed languages easier to use\nby mixing static and dynamic checking in a principled way. With gradual types,\nprogrammers can incrementally migrate code to a dependently typed language.\n  However, adding gradual types to dependent types creates a new challenge:\nmixing decidable type-checking and incremental migration in a full-featured\nlanguage is a precarious balance. Programmers expect type-checking to\nterminate, but dependent type-checkers evaluate terms at compile time, which is\nproblematic because gradual types can introduce non-termination into an\notherwise terminating language. Steps taken to mitigate this non-termination\nmust not jeopardize the smooth transitions between dynamic and static.\n  We present a gradual dependently-typed language that supports inductive type\nfamilies, has decidable type-checking, and provably supports smooth migration\nbetween static and dynamic, as codified by the refined criteria for gradual\ntyping proposed by Siek et al. (2015). Like Eremondi et al. (2019), we use\napproximate normalization for terminating compile-time evaluation. Unlike\nEremondi et al., our normalization does not require comparison of variables,\nallowing us to show termination with a syntactic model that accommodates\ninductive types. Moreover, we design a novel a technique for tracking\nconstraints on type indices, so that dynamic constraint violations signal\nrun-time errors eagerly. To facilitate these checks, we define an algebraic\nnotion of gradual precision, axiomatizing certain semantic properties of\ngradual terms.']"
35,53,35_driving_traffic_vehicles_vehicle,"['driving', 'traffic', 'vehicles', 'vehicle', 'autonomous', 'safety', 'road', 'lane', 'intersection', 'control']","['Autonomous driving has attracted significant research interests in the past\ntwo decades as it offers many potential benefits, including releasing drivers\nfrom exhausting driving and mitigating traffic congestion, among others.\nDespite promising progress, lane-changing remains a great challenge for\nautonomous vehicles (AV), especially in mixed and dynamic traffic scenarios.\nRecently, reinforcement learning (RL), a powerful data-driven control method,\nhas been widely explored for lane-changing decision makings in AVs with\nencouraging results demonstrated. However, the majority of those studies are\nfocused on a single-vehicle setting, and lane-changing in the context of\nmultiple AVs coexisting with human-driven vehicles (HDVs) have received scarce\nattention. In this paper, we formulate the lane-changing decision making of\nmultiple AVs in a mixed-traffic highway environment as a multi-agent\nreinforcement learning (MARL) problem, where each AV makes lane-changing\ndecisions based on the motions of both neighboring AVs and HDVs. Specifically,\na multi-agent advantage actor-critic network (MA2C) is developed with a novel\nlocal reward design and a parameter sharing scheme. In particular, a\nmulti-objective reward function is proposed to incorporate fuel efficiency,\ndriving comfort, and safety of autonomous driving. Comprehensive experimental\nresults, conducted under three different traffic densities and various levels\nof human driver aggressiveness, show that our proposed MARL framework\nconsistently outperforms several state-of-the-art benchmarks in terms of\nefficiency, safety and driver comfort.', 'Autonomous driving at intersections is one of the most complicated and\naccident-prone traffic scenarios, especially with mixed traffic participants\nsuch as vehicles, bicycles and pedestrians. The driving policy should make safe\ndecisions to handle the dynamic traffic conditions and meet the requirements of\non-board computation. However, most of the current researches focuses on\nsimplified intersections considering only the surrounding vehicles and\nidealized traffic lights. This paper improves the integrated decision and\ncontrol framework and develops a learning-based algorithm to deal with complex\nintersections with mixed traffic flows, which can not only take account of\nrealistic characteristics of traffic lights, but also learn a safe policy under\ndifferent safety constraints. We first consider different velocity models for\ngreen and red lights in the training process and use a finite state machine to\nhandle different modes of light transformation. Then we design different types\nof distance constraints for vehicles, traffic lights, pedestrians, bicycles\nrespectively and formulize the constrained optimal control problems (OCPs) to\nbe optimized. Finally, reinforcement learning (RL) with value and policy\nnetworks is adopted to solve the series of OCPs. In order to verify the safety\nand efficiency of the proposed method, we design a multi-lane intersection with\nthe existence of large-scale mixed traffic participants and set practical\ntraffic light phases. The simulation results indicate that the trained decision\nand control policy can well balance safety and tracking performance. Compared\nwith model predictive control (MPC), the computational time is three orders of\nmagnitude lower.', ""Cooperative driving relies on communication among vehicles to create\nsituational awareness. One application of cooperative driving is Cooperative\nAdaptive Cruise Control (CACC) that aims at enhancing highway transportation\nsafety and capacity. Model-based communication (MBC) is a new paradigm with a\nflexible content structure for broadcasting joint vehicle-driver predictive\nbehavioral models. The vehicle's complex dynamics and diverse driving behaviors\nadd complexity to the modeling process. Gaussian process (GP) is a fully\ndata-driven and non-parametric Bayesian modeling approach which can be used as\na modeling component of MBC. The knowledge about the uncertainty is propagated\nthrough predictions by generating local GPs for vehicles and broadcasting their\nhyper-parameters as a model to the neighboring vehicles. In this research\nstudy, GP is used to model each vehicle's speed trajectory, which allows\nvehicles to access the future behavior of their preceding vehicle during\ncommunication loss and/or low-rate communication. Besides, to overcome the\nsafety issues in a vehicle platoon, two operating modes for each vehicle are\nconsidered; free following and emergency braking. This paper presents a\ndiscrete hybrid stochastic model predictive control, which incorporates system\nmodes as well as uncertainties captured by GP models. The proposed control\ndesign approach finds the optimal vehicle speed trajectory with the goal of\nachieving a safe and efficient platoon of vehicles with small inter-vehicle gap\nwhile reducing the reliance of the vehicles on a frequent communication.\nSimulation studies demonstrate the efficacy of the proposed controller\nconsidering the aforementioned communication paradigm with low-rate\nintermittent communication.""]"
36,52,36_materials_machine_properties_molecular,"['materials', 'machine', 'properties', 'molecular', 'learning', 'material', 'thermal', 'conductivity', 'structure', 'computational']","['Data-driven approaches to solve problems in materials science have gained\nimmense popularity in recent times due to their ability to predict unknown\nmaterial properties and uncover relationships between structure and property.\nMachine learning algorithms like GBRT, random forest and neural networks have\nhad tremendous success in predicting target properties of materials and design\nof structures for various applications. However, a major drawback for achieving\nresults within the required accuracy using these algorithms has been the need\nfor large datasets which can be challenging for problems when data is not\nsufficiently available for training the models. In this work, we propose the\nuse of a hierarchical clustering algorithm which can work considerably better\non materials science problems with small dataset constraints. We apply the\nalgorithm to screen out promising double perovskite materials as candidates for\nsolar cells.', 'The availability and easy access of large scale experimental and\ncomputational materials data have enabled the emergence of accelerated\ndevelopment of algorithms and models for materials property prediction,\nstructure prediction, and generative design of materials. However, lack of\nuser-friendly materials informatics web servers has severely constrained the\nwide adoption of such tools in the daily practice of materials screening,\ntinkering, and design space exploration by materials scientists. Herein we\nfirst survey current materials informatics web apps and then propose and\ndevelop MaterialsAtlas.org, a web based materials informatics toolbox for\nmaterials discovery, which includes a variety of routinely needed tools for\nexploratory materials discovery, including materials composition and structure\ncheck (e.g. for neutrality, electronegativity balance, dynamic stability,\nPauling rules), materials property prediction (e.g. band gap, elastic moduli,\nhardness, thermal conductivity), and search for hypothetical materials. These\nuser-friendly tools can be freely accessed at \\url{www.materialsatlas.org}. We\nargue that such materials informatics apps should be widely developed by the\ncommunity to speed up the materials discovery processes.', 'Quantitative descriptions of the structure-thermal property correlation have\nbeen a bottleneck in designing materials with superb thermal properties. In the\npast decade, the first-principles phonon calculations using density functional\ntheory and the Boltzmann transport equation have become a common practice for\npredicting the thermal conductivity of new materials. However, first-principles\ncalculations are too costly for high-throughput material screening and\nmulti-scale structural design. First-principles calculations also face several\nfundamental challenges in modeling thermal transport properties, e.g., of\ncrystalline materials with defects, of amorphous materials, and for materials\nat high temperatures. In the past five years, machine learning started to play\na role in solving these challenges. This review provides a comprehensive\nsummary and discussion on the state-of-the-art, future opportunities, and the\nremaining challenges in implementing machine learning for studying thermal\nconductivity. After an introduction to the working principles of machine\nlearning and descriptors of material structures, recent research using machine\nlearning to study thermal transport is discussed. Three major applications of\nmachine learning for predicting thermal properties are discussed. First,\nmachine learning is applied to solve the challenges in modeling phonon\ntransport of crystals with defects, in amorphous materials, and at high\ntemperatures. Machine learning is used to build high-fidelity interatomic\npotentials to bridge the gap between first-principles calculations and\nmolecular dynamics simulations. Second, machine learning can be used to study\nthe correlation between thermal conductivity and other properties for\nhigh-throughput materials screening. Finally, machine learning is a powerful\ntool for structural design to achieve target thermal conductance or thermal\nconductivity.']"
37,51,37_news_media_social_misinformation,"['news', 'media', 'social', 'misinformation', 'fake', 'users', 'content', 'fact', 'tweets', 'online']","['Recent years have witnessed an increasing use of coordinated accounts on\nsocial media, operated by misinformation campaigns to influence public opinion\nand manipulate social outcomes. Consequently, there is an urgent need to\ndevelop an effective methodology for coordinated group detection to combat the\nmisinformation on social media. However, existing works suffer from various\ndrawbacks, such as, either limited performance due to extreme reliance on\npredefined signatures of coordination, or instead an inability to address the\nnatural sparsity of account activities on social media with useful prior domain\nknowledge. Therefore, in this paper, we propose a coordination detection\nframework incorporating neural temporal point process with prior knowledge such\nas temporal logic or pre-defined filtering functions. Specifically, when\nmodeling the observed data from social media with neural temporal point\nprocess, we jointly learn a Gibbs-like distribution of group assignment based\non how consistent an assignment is to (1) the account embedding space and (2)\nthe prior knowledge. To address the challenge that the distribution is hard to\nbe efficiently computed and sampled from, we design a theoretically guaranteed\nvariational inference approach to learn a mean-field approximation for it.\nExperimental results on a real-world dataset show the effectiveness of our\nproposed method compared to the SOTA model in both unsupervised and\nsemi-supervised settings. We further apply our model on a COVID-19 Vaccine\nTweets dataset. The detection result suggests the presence of suspicious\ncoordinated efforts on spreading misinformation about COVID-19 vaccines.', 'Fake news has become omnipresent in digitalized areas such as social media\nplatforms. While being disseminated online, it also poses a threat to\nindividuals and societies offline, for example, in the context of democratic\nelections. Research and practice have investigated the detection of fake news\nwith behavioral science or method-related perspectives. However, to date, we\nlack design knowledge on presenting fake news warnings to users to support\ntheir individual news credibility assessment. We present the journey through\nthe first design cycle on developing a fake news detection service focusing on\nthe user interface design. The design is grounded in concepts from the field of\nsource credibility theory and instantiated in a prototype that was\nqualitatively evaluated. The 13 participants communicated their interest in a\nlightweight application that aids in the news credibility assessment and rated\nthe design features as useful as well as desirable.', ""In recent years, with the expansion of the Internet and attractive social\nmedia infrastructures, people prefer to follow the news through these media.\nDespite the many advantages of these media in the news field, the lack of any\ncontrol and verification mechanism has led to the spread of fake news, as one\nof the most important threats to democracy, economy, journalism and freedom of\nexpression. Designing and using automatic methods to detect fake news on social\nmedia has become a significant challenge. In this paper, we examine the\npublishers' role in detecting fake news on social media. We also suggest a high\naccurate multi-modal framework, namely FR-Detect, using user-related and\ncontent-related features with early detection capability. For this purpose, two\nnew user-related features, namely Activity Credibility and Influence, have been\nintroduced for publishers. Furthermore, a sentence-level convolutional neural\nnetwork is provided to combine these features with latent textual content\nfeatures properly. Experimental results have shown that the publishers'\nfeatures can improve the performance of content-based models by up to 13% and\n29% in accuracy and F1-score, respectively.""]"
38,51,38_nas_search_pruning_neural,"['nas', 'search', 'pruning', 'neural', 'architecture', 'architectures', 'networks', 'network', 'accuracy', 'training']","['Lots of effort in neural architecture search (NAS) research has been\ndedicated to algorithmic development, aiming at designing more efficient and\nless costly methods. Nonetheless, the investigation of the initialization of\nthese techniques remain scare, and currently most NAS methodologies rely on\nstochastic initialization procedures, because acquiring information prior to\nsearch is costly. However, the recent availability of NAS benchmarks have\nenabled low computational resources prototyping. In this study, we propose to\naccelerate a NAS algorithm using a data-driven initialization technique,\nleveraging the availability of NAS benchmarks. Particularly, we proposed a\ntwo-step methodology. First, a calibrated clustering analysis of the search\nspace is performed. Second, the centroids are extracted and used to initialize\na NAS algorithm. We tested our proposal using Aging Evolution, an evolutionary\nalgorithm, on NAS-bench-101. The results show that, compared to a random\ninitialization, a faster convergence and a better performance of the final\nsolution is achieved.', 'Network Architecture Search (NAS) methods have recently gathered much\nattention. They design networks with better performance and use a much shorter\nsearch time compared to traditional manual tuning. Despite their efficiency in\nmodel deployments, most NAS algorithms target a single task on a fixed hardware\nsystem. However, real-life few-shot learning environments often cover a great\nnumber of tasks (T ) and deployments on a wide variety of hardware platforms (H\n).\n  The combinatorial search complexity T times H creates a fundamental search\nefficiency challenge if one naively applies existing NAS methods to these\nscenarios. To overcome this issue, we show, for the first time, how to rapidly\nadapt model architectures to new tasks in a many-task many-hardware few-shot\nlearning setup by integrating Model Agnostic Meta Learning (MAML) into the NAS\nflow. The proposed NAS method (H-Meta-NAS) is hardware-aware and performs\noptimisation in the MAML framework. H-Meta-NAS shows a Pareto dominance\ncompared to a variety of NAS and manual baselines in popular few-shot learning\nbenchmarks with various hardware platforms and constraints. In particular, on\nthe 5-way 1-shot Mini-ImageNet classification task, the proposed method\noutperforms the best manual baseline by a large margin (5.21% in accuracy)\nusing 60% less computation.', 'Recently, Neural Architecture Search (NAS) has been widely applied to\nautomate the design of deep neural networks. Various NAS algorithms have been\nproposed to reduce the search cost and improve the generalization performance\nof those final selected architectures. However, these NAS algorithms aim to\nselect only a single neural architecture from the search spaces and thus have\noverlooked the capability of other candidate architectures in helping improve\nthe performance of their final selected architecture. To this end, we present\ntwo novel sampling algorithms under our Neural Ensemble Search via Sampling\n(NESS) framework that can effectively and efficiently select a well-performing\nensemble of neural architectures from NAS search space. Compared with\nstate-of-the-art NAS algorithms and other well-known ensemble search baselines,\nour NESS algorithms are shown to be able to achieve improved performance in\nboth classification and adversarial defense tasks on various benchmark datasets\nwhile incurring a comparable search cost to these NAS algorithms.']"
39,48,39_soft_robotic_robot_robots,"['soft', 'robotic', 'robot', 'robots', 'gripper', 'actuators', 'actuation', 'force', 'fish', 'grasping']","[""Soft robotics technologies have gained growing interest in recent years,\nwhich allows various applications from manufacturing to human-robot\ninteraction. Pneumatic artificial muscle (PAM), a typical soft actuator, has\nbeen widely applied to soft robots. The compliance and resilience of soft\nactuators allow soft robots to behave compliant when interacting with\nunstructured environments, while the utilization of soft actuators also\nintroduces nonlinearity and uncertainty. Inspired by Cerebellum's vital\nfunctions in control of human's physical movement, a neural network model of\nCerebellum based on spiking neuron networks (SNNs) is designed. This model is\nused as a feed-forward controller in controlling a 1-DOF robot arm driven by\nPAMs. The simulation results show that this Cerebellar-based system achieves\ngood performance and increases the system's response."", 'Soft grippers are receiving growing attention due to their compliance-based\ninteractive safety and dexterity. Hybrid gripper (soft actuators enhanced by\nrigid constraints) is a new trend in soft gripper design. With right structural\ncomponents actuated by soft actuators, they could achieve excellent grasping\nadaptability and payload, while also being easy to model and control with\nconventional kinematics. However, existing works were mostly focused on\nachieving superior payload and perception with simple planar workspaces,\nresulting in far less dexterity compared with conventional grippers. In this\nwork, we took inspiration from the human Metacarpophalangeal (MCP) joint and\nproposed a new hybrid gripper design with 8 independent muscles. It was shown\nthat adding the MCP complexity was critical in enabling a range of novel\nfeatures in the hybrid gripper, including in-hand manipulation, lateral passive\ncompliance, as well as new control modes. A prototype gripper was fabricated\nand tested on our proprietary dual-arm robot platform with vision guided\ngrasping. With very lightweight pneumatic bellows soft actuators, the gripper\ncould grasp objects over 25 times its own weight with lateral compliance. Using\nthe dual-arm platform, highly anthropomorphic dexterous manipulations were\ndemonstrated using two hybrid grippers, from Tug-of-war on a rigid rod, to\npassing a soft towel between two grippers using in-hand manipulation. Matching\nwith the novel features and performance specifications of the proposed hybrid\ngripper, the underlying modeling, actuation, control, and experimental\nvalidation details were also presented, offering a promising approach to\nachieving enhanced dexterity, strength, and compliance in robotic grippers.', ""Soft robots are made of compliant and deformable materials and can perform\ntasks challenging for conventional rigid robots. The inherent compliance of\nsoft robots makes them more suitable and adaptable for interactions with humans\nand the environment. However, this preeminence comes at a cost: their continuum\nnature makes it challenging to develop robust model-based control strategies.\nSpecifically, an adaptive control approach addressing this challenge has not\nyet been applied to physical soft robotic arms. This work presents a\nreformulation of dynamics for a soft continuum manipulator using the\nEuler-Lagrange method. The proposed model eliminates the simplifying assumption\nmade in previous works and provides a more accurate description of the robot's\ninertia. Based on our model, we introduce a task-space adaptive control scheme.\nThis controller is robust against model parameter uncertainties and unknown\ninput disturbances. The controller is implemented on a physical soft continuum\narm. A series of experiments were carried out to validate the effectiveness of\nthe controller in task-space trajectory tracking under different payloads. The\ncontroller outperforms the state-of-the-art method both in terms of accuracy\nand robustness. Moreover, the proposed model-based control design is flexible\nand can be generalized to any continuum robotic arm with an arbitrary number of\ncontinuum segments.""]"
40,46,40_students_course_teaching_learning,"['students', 'course', 'teaching', 'learning', 'remote', 'student', 'courses', 'education', 'university', 'school']","[""In this work, the teaching content of a theoretical-chemistry (TC) course is\nreformed, establishing a theoretical contents from micro- to macro-system, and\ncomprehensively introducing the theory of chemical reaction to undergraduate\nstudents in chemistry. In order to develop such TC course based on the general\nphysical-chemistry course, we focus on the last-mile problem between the\nphysics and chemistry courses to train the critical thinking of undergraduate\nstudents in chemistry. To clearly show this, a reduction scheme of polymer\nmolecular dynamics was discussed as an example, which shows a different\ntheoretical content in polymer chemistry. Moreover, we propose a series of\nexperiences and dependent measures that can provide information regarding\nstudents' levels of knowledge and understanding. This assessment quiz was\ndesigned to test students on the fundamental concepts and applications of TC,\nsuch as dynamics, statistical ensemble, kinetics, and so on. From the actual\nteaching for 36 students, it was found that these students performed\nsignificantly improvement from the present TC content. Further analysis of each\nindividual question revealed that approximately two-third of the students learn\nnew knowledge. Although the present TC course might be considered to be a\ncertain degree of difficulty for chemists, these analyses show that students\ncan effectively accept these complicated concepts."", ""In 2020, many instructors and students at colleges and universities were\nthrust into an unprecedented situation as a result of the COVID-19 pandemic\ndisruptions. Even though they typically engage in in-person teaching and\nlearning in brick and mortar classrooms, remote instruction was the only\npossibility. Many instructors at our institution who had to switch from\nin-person to remote instruction without any notice earlier in the year worked\nextremely hard to design and teach online courses to support their students\nduring the second half of 2020. Since different instructors chose different\npedagogical approaches for remote instruction, students taking multiple remote\nclasses simultaneously experienced a variety of instructional strategies. We\npresent an analysis of students' perceptions of remote learning in their\nlecture-based, active learning, and lab physics classes at a large research\nuniversity in the USA, focusing on positive and negative aspects including\ncollaboration, communication, and assessment. Student reflections emphasized\nthe importance of grade incentives for out-of-class and in-class work;\nfrequent, low-stakes assessments; community-building activities; and\nopportunities to study with peers. Reflecting on the challenges and successes\nof different types of remote instructional approaches from students'\nperspective could provide useful insight to guide the design of future online\ncourses as well as some aspects of in-person courses."", 'Computer-aided design (CAD) programs are essential to engineering as they\nallow for better designs through low-cost iterations. While CAD programs are\ntypically taught to undergraduate students as a job skill, such software can\nalso help students learn engineering concepts. A current limitation of CAD\nprograms (even those that are specifically designed for educational purposes)\nis that they are not capable of providing automated real-time help to students.\nTo encourage CAD programs to build in assistance to students, we used data\ngenerated from students using a free, open source CAD software called Aladdin\nto demonstrate how student data combined with machine learning techniques can\npredict how well a particular student will perform in a design task. We\nchallenged students to design a house that consumed zero net energy as part of\nan introductory engineering technology undergraduate course. Using data from\n128 students, along with the scikit-learn Python machine learning library, we\ntested our models using both total counts of design actions and sequences of\ndesign actions as inputs. We found that our models using early design sequence\nactions are particularly valuable for prediction. Our logistic regression model\nachieved a >60% chance of predicting if a student would succeed in designing a\nzero net energy house. Our results suggest that it would be feasible for\nAladdin to provide useful feedback to students when they are approximately\nhalfway through their design. Further improvements to these models could lead\nto earlier predictions and thus provide students feedback sooner to enhance\ntheir learning.']"
41,45,41_ai_ethical_xai_ethics,"['ai', 'ethical', 'xai', 'ethics', 'research', 'explainability', 'systems', 'intelligence', 'artificial', 'development']","['The widespread use of artificial intelligence (AI) in many domains has\nrevealed numerous ethical issues from data and design to deployment. In\nresponse, countless broad principles and guidelines for ethical AI have been\npublished, and following those, specific approaches have been proposed for how\nto encourage ethical outcomes of AI. Meanwhile, library and information\nservices too are seeing an increase in the use of AI-powered and machine\nlearning-powered information systems, but no practical guidance currently\nexists for libraries to plan for, evaluate, or audit the ethics of intended or\ndeployed AI. We therefore report on several promising approaches for promoting\nethical AI that can be adapted from other contexts to AI-powered information\nservices and in different stages of the software lifecycle.', 'AI, machine learning, and data science methods are already pervasive in our\nsociety and technology, affecting all of our lives in many subtle ways.\nTrustworthy AI has become an important topic because trust in AI systems and\ntheir creators has been lost, or was never present in the first place.\nResearchers, corporations, and governments have long and painful histories of\nexcluding marginalized groups from technology development, deployment, and\noversight. As a direct result of this exclusion, these technologies have long\nhistories of being less useful or even harmful to minoritized groups. This\ninfuriating history illustrates that industry cannot be trusted to\nself-regulate and why trust in commercial AI systems and development has been\nlost. We argue that any AI development, deployment, and monitoring framework\nthat aspires to trust must incorporate both feminist, non-exploitative\nparticipatory design principles and strong, outside, and continual monitoring\nand testing. We additionally explain the importance of considering aspects of\ntrustworthiness beyond just transparency, fairness, and accountability,\nspecifically, to consider justice and shifting power to the people and\ndisempowered as core values to any trustworthy AI system. Creating trustworthy\nAI starts by funding, supporting, and empowering groups like Queer in AI so the\nfield of AI has the diversity and inclusion to credibly and effectively develop\ntrustworthy AI. Through our years of work and advocacy, we have developed\nexpert knowledge around questions of if and how gender, sexuality, and other\naspects of identity should be used in AI systems and how harms along these\nlines should be mitigated. Based on this, we discuss a gendered approach to AI,\nand further propose a queer epistemology and analyze the benefits it can bring\nto AI.', 'Recently, the use of sound measures and metrics in Artificial Intelligence\nhas become the subject of interest of academia, government, and industry.\nEfforts towards measuring different phenomena have gained traction in the AI\ncommunity, as illustrated by the publication of several influential field\nreports and policy documents. These metrics are designed to help decision\ntakers to inform themselves about the fast-moving and impacting influences of\nkey advances in Artificial Intelligence in general and Machine Learning in\nparticular. In this paper we propose to use such newfound capabilities of AI\ntechnologies to augment our AI measuring capabilities. We do so by training a\nmodel to classify publications related to ethical issues and concerns. In our\nmethodology we use an expert, manually curated dataset as the training set and\nthen evaluate a large set of research papers. Finally, we highlight the\nimplications of AI metrics, in particular their contribution towards developing\ntrustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI\nFairness; AI Measurement. Ethics in Computer Science.']"
42,45,42_optimization_evolutionary_objective_pareto,"['optimization', 'evolutionary', 'objective', 'pareto', 'problems', 'algorithm', 'solutions', 'swarm', 'algorithms', 'multi']","['We consider the problem of black-box multi-objective optimization (MOO) using\nexpensive function evaluations (also referred to as experiments), where the\ngoal is to approximate the true Pareto set of solutions by minimizing the total\nresource cost of experiments. For example, in hardware design optimization, we\nneed to find the designs that trade-off performance, energy, and area overhead\nusing expensive computational simulations. The key challenge is to select the\nsequence of experiments to uncover high-quality solutions using minimal\nresources. In this paper, we propose a general framework for solving MOO\nproblems based on the principle of output space entropy (OSE) search: select\nthe experiment that maximizes the information gained per unit resource cost\nabout the true Pareto front. We appropriately instantiate the principle of OSE\nsearch to derive efficient algorithms for the following four MOO problem\nsettings: 1) The most basic em single-fidelity setting, where experiments are\nexpensive and accurate; 2) Handling em black-box constraints} which cannot be\nevaluated without performing experiments; 3) The discrete multi-fidelity\nsetting, where experiments can vary in the amount of resources consumed and\ntheir evaluation accuracy; and 4) The em continuous-fidelity setting, where\ncontinuous function approximations result in a huge space of experiments.\nExperiments on diverse synthetic and real-world benchmarks show that our OSE\nsearch based algorithms improve over state-of-the-art methods in terms of both\ncomputational-efficiency and accuracy of MOO solutions.', 'Many real-world optimization problems such as engineering design can be\neventually modeled as the corresponding multiobjective optimization problems\n(MOPs) which must be solved to obtain approximate Pareto optimal fronts.\nMultiobjective evolutionary algorithm based on decomposition (MOEA/D) has been\nregarded as a significantly promising approach for solving MOPs. Recent studies\nhave shown that MOEA/D with uniform weight vectors is well-suited to MOPs with\nregular Pareto optimal fronts, but its performance in terms of diversity\nusually deteriorates when solving MOPs with irregular Pareto optimal fronts. In\nthis way, the solution set obtained by the algorithm can not provide more\nreasonable choices for decision makers. In order to efficiently overcome this\ndrawback, we propose an improved MOEA/D algorithm by virtue of the well-known\nPascoletti-Serafini scalarization method and a new strategy of multi-reference\npoints. Specifically, this strategy consists of the setting and adaptation of\nreference points generated by the techniques of equidistant partition and\nprojection. For performance assessment, the proposed algorithm is compared with\nexisting four state-of-the-art multiobjective evolutionary algorithms on\nbenchmark test problems with various types of Pareto optimal fronts. According\nto the experimental results, the proposed algorithm exhibits better diversity\nperformance than that of the other compared algorithms. Finally, our algorithm\nis applied to two real-world MOPs in engineering optimization successfully.', 'Multi-objective optimization problems are ubiquitous in real-world science,\nengineering and design optimization problems. It is not uncommon that the\nobjective functions are as a black box, the evaluation of which usually involve\ntime-consuming and/or costly physical experiments. Data-driven evolutionary\noptimization can be used to search for a set of non-dominated trade-off\nsolutions, where the expensive objective functions are approximated as a\nsurrogate model. In this paper, we propose a framework for implementing batched\ndata-driven evolutionary multi-objective optimization. It is so general that\nany off-the-shelf evolutionary multi-objective optimization algorithms can be\napplied in a plug-in manner. In particular, it has two unique components: 1)\nbased on the Karush-Kuhn-Tucker conditions, a manifold interpolation approach\nthat explores more diversified solutions with a convergence guarantee along the\nmanifold of the approximated Pareto-optimal set; and 2) a batch recommendation\napproach that reduces the computational time of the optimization process by\nevaluating multiple samples at a time in parallel. Experiments on 136 benchmark\ntest problem instances with irregular Pareto-optimal front shapes against six\nstate-of-the-art surrogate-assisted EMO algorithms fully demonstrate the\neffectiveness and superiority of our proposed framework. In particular, our\nproposed framework is featured with a faster convergence and a stronger\nresilience to various PF shapes.']"
43,44,43_regret_bandit_bandits_arm,"['regret', 'bandit', 'bandits', 'arm', 'algorithm', 'armed', 'optimal', 'problem', 'reward', 'setting']","['We study nonstochastic bandits and experts in a delayed setting where delays\ndepend on both time and arms. While the setting in which delays only depend on\ntime has been extensively studied, the arm-dependent delay setting better\ncaptures real-world applications at the cost of introducing new technical\nchallenges. In the full information (experts) setting, we design an algorithm\nwith a first-order regret bound that reveals an interesting trade-off between\ndelays and losses. We prove a similar first-order regret bound also for the\nbandit setting, when the learner is allowed to observe how many losses are\nmissing. These are the first bounds in the delayed setting that depend on the\nlosses and delays of the best arm only. When in the bandit setting no\ninformation other than the losses is observed, we still manage to prove a\nregret bound through a modification to the algorithm of Zimmert and Seldin\n(2020). Our analyses hinge on a novel bound on the drift, measuring how much\nbetter an algorithm can perform when given a look-ahead of one round.', 'We consider a continuous-time multi-arm bandit problem (CTMAB), where the\nlearner can sample arms any number of times in a given interval and obtain a\nrandom reward from each sample, however, increasing the frequency of sampling\nincurs an additive penalty/cost. Thus, there is a tradeoff between obtaining\nlarge reward and incurring sampling cost as a function of the sampling\nfrequency. The goal is to design a learning algorithm that minimizes regret,\nthat is defined as the difference of the payoff of the oracle policy and that\nof the learning algorithm. CTMAB is fundamentally different than the usual\nmulti-arm bandit problem (MAB), e.g., even the single-arm case is non-trivial\nin CTMAB, since the optimal sampling frequency depends on the mean of the arm,\nwhich needs to be estimated. We first establish lower bounds on the regret\nachievable with any algorithm and then propose algorithms that achieve the\nlower bound up to logarithmic factors. For the single-arm case, we show that\nthe lower bound on the regret is $\\Omega((\\log T)^2/\\mu)$, where $\\mu$ is the\nmean of the arm, and $T$ is the time horizon. For the multiple arms case, we\nshow that the lower bound on the regret is $\\Omega((\\log T)^2 \\mu/\\Delta^2)$,\nwhere $\\mu$ now represents the mean of the best arm, and $\\Delta$ is the\ndifference of the mean of the best and the second-best arm. We then propose an\nalgorithm that achieves the bound up to constant terms.', ""Much of the literature on optimal design of bandit algorithms is based on\nminimization of expected regret. It is well known that designs that are optimal\nover certain exponential families can achieve expected regret that grows\nlogarithmically in the number of arm plays, at a rate governed by the\nLai-Robbins lower bound. In this paper, we show that when one uses such\noptimized designs, the associated algorithms necessarily have the undesirable\nfeature that the tail of the regret distribution behaves like that of a\ntruncated Cauchy distribution. Furthermore, for $p>1$, the $p$'th moment of the\nregret distribution grows much faster than poly-logarithmically, in particular\nas a power of the number of sub-optimal arm plays. We show that optimized\nThompson sampling and UCB bandit designs are also fragile, in the sense that\nwhen the problem is even slightly mis-specified, the regret can grow much\nfaster than the conventional theory suggests. Our arguments are based on\nstandard change-of-measure ideas, and indicate that the most likely way that\nregret becomes larger than expected is when the optimal arm returns\nbelow-average rewards in the first few arm plays that make that arm appear to\nbe sub-optimal, thereby causing the algorithm to sample a truly sub-optimal arm\nmuch more than would be optimal.""]"
44,43,44_digital_innovation_research_health,"['digital', 'innovation', 'research', 'health', 'study', 'covid', '19', 'technology', 'creation', 'government']","['Objective: This study aims to identify the social determinants of mental\nhealth among undergraduate students in Bangladesh, a developing nation in South\nAsia. Our goal is to identify the broader social determinants of mental health\namong this population, study the manifestation of these determinants in their\nday-to-day life, and explore the feasibility of self-monitoring tools in\nhelping them identify the specific factors or relationships that impact their\nmental health. Methods: We conducted a 21-day study with 38 undergraduate\nstudents from seven universities in Bangladesh. We conducted two\nsemi-structured interviews: one pre-study and one post-study. During the 21-day\nstudy, participants used an Android application to self-report and self-monitor\ntheir mood after each phone conversation. The app prompted participants to\nreport their mood after each phone conversation and provided graphs and charts\nso that participants could independently review their mood and conversation\npatterns.\n  Results: Our results show that academics, family, job and economic condition,\nromantic relationships, and religion are the major social determinants of\nmental health among undergraduate students in Bangladesh. Our app helped the\nparticipants pinpoint the specific issues related to these factors as\nparticipants could review the pattern of their moods and emotions from past\nconversation history. Although our app does not provide any explicit\nrecommendation, participants took certain steps on their own to improve their\nmental health (e.g., reduced the frequency of communication with certain\npersons).\n  Conclusions: Overall, the findings from this study would provide better\ninsights for the researchers to design better solutions to help the younger\npopulation from this part of the world.', 'Digital government innovation is being recognised as a solution to many\nproblems faced by governments in providing services to their citizens. It is\nespecially important for low-income countries where there are resource\nconstraints. This research was aimed at exploring the moderating effect of\ngender on the adoption of a digital government innovation in Ethiopia based on\nthe UTAUT model (n=270) and using structural equation modeling (SEM). The\nresults reveal that gender only moderates the relationship between facilitating\nconditions and usage behavior of government employees to adopt the digital\ngovernment innovation which is inconsistent with other findings. Another key\nfinding was that even though the innovation was regarded as not being easy to\nuse, women identified that they would still use it because of the social\ninfluence from the peers and the bosses. This finding suggests that women\ngovernment employees who obtain external support are more likely to use digital\ngovernment innovations compared with men who are unlikely to use it even if\nthey were facilitated. The paper recommends that governments of low-income\ncountries like Ethiopia should design appropriate policies that encourage women\nin digital government.', ""The primary objective of our exploratory research is to contribute to the\nongoing conversation on Digital Afterlife from the lenses of Global South\nduring the COVID-19 period. Digital Afterlife is fast becoming a challenge for\nour increasingly connected society. Moreover, the situation got worse with the\nCOVID-19 pandemic. The on-going research is to address the disparity in the\nGlobal South, specifically in countries like Indonesia, India and The\nPhilippines compared to the Global North for Digital Afterlife services such as\npolicies and digital mourning services. By addressing the research question,\n'What services and policy frameworks are available for Digital Afterlife in the\nGlobal South during COVID-19?', we aim to find the multitude of ways people in\nthe Global South are managing their digital footprints. Our preliminary\nfindings show that some considerable research and death related digital\nservices and innovation have taken place during the pandemic. However,\noverwhelming majority of these works are western-centric and mainly dealing\nwith post-mortem personal asset management. Cultural nuances, socio-economic\nperspectives, religion, political climate, regional infrastructures are mostly\nsidelined. We found significant disparity in Digital Afterlife product and\nservice designs, which got worse during the global pandemic. Our goal is to\ncollect further in-depth data within the three big ICT powerhouses of global\nsouth (Indonesia, India and The Philippines), identify the challenges as well\nas the innovations around Digital Afterlife.We envision proposing a set of\nrecommendations, based on our findings, for developing a more inclusive and\nequitable digital space in this pandemic-stricken world.""]"
45,43,45_resolution_image_super_sr,"['resolution', 'image', 'super', 'sr', 'images', 'deblurring', 'denoising', 'network', 'hdr', 'rain']","['Since convolutional neural networks (CNNs) perform well at learning\ngeneralizable image priors from large-scale data, these models have been\nextensively applied to image restoration and related tasks. Recently, another\nclass of neural architectures, Transformers, have shown significant performance\ngains on natural language and high-level vision tasks. While the Transformer\nmodel mitigates the shortcomings of CNNs (i.e., limited receptive field and\ninadaptability to input content), its computational complexity grows\nquadratically with the spatial resolution, therefore making it infeasible to\napply to most image restoration tasks involving high-resolution images. In this\nwork, we propose an efficient Transformer model by making several key designs\nin the building blocks (multi-head attention and feed-forward network) such\nthat it can capture long-range pixel interactions, while still remaining\napplicable to large images. Our model, named Restoration Transformer\n(Restormer), achieves state-of-the-art results on several image restoration\ntasks, including image deraining, single-image motion deblurring, defocus\ndeblurring (single-image and dual-pixel data), and image denoising (Gaussian\ngrayscale/color denoising, and real image denoising). The source code and\npre-trained models are available at https://github.com/swz30/Restormer.', ""Convolutional Neural Networks (CNNs) have achieved impressive results across\nmany super-resolution (SR) and image restoration tasks. While many such\nnetworks can upscale low-resolution (LR) images using just the raw pixel-level\ninformation, the ill-posed nature of SR can make it difficult to accurately\nsuper-resolve an image which has undergone multiple different degradations.\nAdditional information (metadata) describing the degradation process (such as\nthe blur kernel applied, compression level, etc.) can guide networks to\nsuper-resolve LR images with higher fidelity to the original source. Previous\nattempts at informing SR networks with degradation parameters have indeed been\nable to improve performance in a number of scenarios. However, due to the\nfully-convolutional nature of many SR networks, most of these metadata fusion\nmethods either require a complete architectural change, or necessitate the\naddition of significant extra complexity. Thus, these approaches are difficult\nto introduce into arbitrary SR networks without considerable design\nalterations. In this paper, we introduce meta-attention, a simple mechanism\nwhich allows any SR CNN to exploit the information available in relevant\ndegradation parameters. The mechanism functions by translating the metadata\ninto a channel attention vector, which in turn selectively modulates the\nnetwork's feature maps. Incorporating meta-attention into SR networks is\nstraightforward, as it requires no specific type of architecture to function\ncorrectly. Extensive testing has shown that meta-attention can consistently\nimprove the pixel-level accuracy of state-of-the-art (SOTA) networks when\nprovided with relevant degradation metadata. For PSNR, the gain on\nblurred/downsampled (X4) images is of 0.2969 dB (on average) and 0.3320 dB for\nSOTA general and face SR models, respectively."", 'For deep learning methods of image super-resolution, the most critical issue\nis whether the paired low and high resolution images for training accurately\nreflect the sampling process of real cameras. Low and high resolution\n(LR$\\sim$HR) image pairs synthesized by existing degradation models (e.g.\nbicubic downsampling) deviate from those in reality; thus the super-resolution\nCNN trained by these synthesized LR$\\sim$HR image pairs does not perform well\nwhen being applied to real images. In this paper, we propose a novel method to\ncapture a large set of realistic LR$\\sim$HR image pairs using real cameras. The\ndata acquisition is carried out under controllable lab conditions with minimum\nhuman intervention and at high throughput (about 500 image pairs per hour). The\nhigh level of automation makes it easy to produce a set of real LR$\\sim$HR\ntraining image pairs for each camera.Our innovation is to shoot images\ndisplayed on an ultra-high quality screen at different resolutions. There are\nthree distinctive advantages of our method for image super-resolution. First,\nas the LR and HR images are taken of a 3D planar surface (the screen) the\nregistration problem fits exactly to a homography model and we can display\nspecially designed markers on the image to improve the registration precision.\nSecond, the displayed digital image file can be exploited as a reference to\noptimize the high frequency content of the restored image. Third, this\nhigh-efficiency data collection method makes it possible to collect a\ncustomized dataset for each camera sensor, for which one can train a specific\nmodel for the intended camera sensor. Experimental results show that training a\nsuper-resolution CNN by our LR$\\sim$HR dataset has superior restoration\nperformance than training it by existing datasets on real world images at the\ninference stage.']"
46,42,46_icecube_neutrino_neutrinos_ice,"['icecube', 'neutrino', 'neutrinos', 'ice', 'gen2', 'optical', 'will', 'cosmic', 'km3net', 'detector']","['The IceCube Neutrino Observatory at the South Pole has measured the diffuse\nastrophysical neutrino flux up to ~PeV energies and is starting to identify\nfirst point source candidates. The next generation facility, IceCube-Gen2, aims\nat extending the accessible energy range to EeV in order to measure the\ncontinuation of the astrophysical spectrum, to identify neutrino sources, and\nto search for a cosmogenic neutrino flux. As part of IceCube-Gen2, a radio\narray is foreseen that is sensitive to detect Askaryan emission of neutrinos\nbeyond ~30 PeV. Surface and deep antenna stations have different benefits in\nterms of effective area, resolution, and the capability to reject backgrounds\nfrom cosmic-ray air showers and may be combined to reach the best sensitivity.\nThe optimal detector configuration is still to be identified.\n  This contribution presents the full-array simulation efforts for a\ncombination of deep and surface antennas, and compares different design options\nwith respect to their sensitivity to fulfill the science goals of IceCube-Gen2.', 'The IceCube Neutrino Observatory opened the window on neutrino astronomy by\ndiscovering high-energy astrophysical neutrinos in 2013 and identifying the\nfirst compelling astrophysical neutrino source, the blazar TXS0506+056, in\n2017. In this talk, we will discuss the science reach and ongoing development\nof the IceCube-Gen2 facility---a planned extension to IceCube. IceCube-Gen2\nwill increase the rate of observed cosmic neutrinos by an order of magnitude,\nbe able to detect five-times fainter neutrino sources, and extend the\nmeasurement of astrophysical neutrinos several orders of magnitude higher in\nenergy. We will discuss the envisioned design of the instrument, which will\ninclude an enlarged in-ice optical array, a surface array for the study of\ncosmic-rays, and a shallow radio array to detect ultra-high energy (>100 PeV)\nneutrinos. we will also highlight ongoing efforts to develop and test new\ninstrumentation for IceCube-Gen2.', 'The IceCube Neutrino Observatory opened the window on high-energy neutrino\nastronomy by confirming the existence of PeV astrophysical neutrinos and\nidentifying the first compelling astrophysical neutrino source in the blazar\nTXS0506+056. Planning is underway to build an enlarged detector, IceCube-Gen2,\nwhich will extend measurements to higher energies, increase the rate of\nobserved cosmic neutrinos and provide improved prospects for detecting fainter\nsources. IceCube-Gen2 is planned to have an extended in-ice optical array, a\nradio array at shallower depths for detecting ultra-high-energy (>100 PeV)\nneutrinos, and a surface component studying cosmic rays. In this contribution,\nwe will discuss the simulation of the in-ice optical component of the baseline\ndesign of the IceCube-Gen2 detector, which foresees the deployment of an\nadditional ~120 new detection strings to the existing 86 in IceCube over ~7\nAntarctic summer seasons. Motivated by the phased construction plan for\nIceCube-Gen2, we discuss how the reconstruction capabilities and sensitivities\nof the instrument are expected to progress throughout its deployment.']"
47,39,47_captioning_visual_image_text,"['captioning', 'visual', 'image', 'text', 'captions', 'language', 'semantic', 'scene', 'caption', 'word']","['Dense video captioning (DVC) aims to generate multi-sentence descriptions to\nelucidate the multiple events in the video, which is challenging and demands\nvisual consistency, discoursal coherence, and linguistic diversity. Existing\nmethods mainly generate captions from individual video segments, lacking\nadaptation to the global visual context and progressive alignment between the\nfast-evolved visual content and textual descriptions, which results in\nredundant and spliced descriptions. In this paper, we introduce the concept of\ninformation flow to model the progressive information changing across video\nsequence and captions. By designing a Cross-modal Information Flow Alignment\nmechanism, the visual and textual information flows are captured and aligned,\nwhich endows the captioning process with richer context and dynamics on\nevent/topic evolution. Based on the Cross-modal Information Flow Alignment\nmodule, we further put forward DVCFlow framework, which consists of a\nGlobal-local Visual Encoder to capture both global features and local features\nfor each video segment, and a pre-trained Caption Generator to produce\ncaptions. Extensive experiments on the popular ActivityNet Captions and\nYouCookII datasets demonstrate that our method significantly outperforms\ncompetitive baselines, and generates more human-like text according to subject\nand objective tests.', 'Image captioning is shown to be able to achieve a better performance by using\nscene graphs to represent the relations of objects in the image. The current\ncaptioning encoders generally use a Graph Convolutional Net (GCN) to represent\nthe relation information and merge it with the object region features via\nconcatenation or convolution to get the final input for sentence decoding.\nHowever, the GCN-based encoders in the existing methods are less effective for\ncaptioning due to two reasons. First, using the image captioning as the\nobjective (i.e., Maximum Likelihood Estimation) rather than a relation-centric\nloss cannot fully explore the potential of the encoder. Second, using a\npre-trained model instead of the encoder itself to extract the relationships is\nnot flexible and cannot contribute to the explainability of the model. To\nimprove the quality of image captioning, we propose a novel architecture\nReFormer -- a RElational transFORMER to generate features with relation\ninformation embedded and to explicitly express the pair-wise relationships\nbetween objects in the image. ReFormer incorporates the objective of scene\ngraph generation with that of image captioning using one modified Transformer\nmodel. This design allows ReFormer to generate not only better image captions\nwith the bene-fit of extracting strong relational image features, but also\nscene graphs to explicitly describe the pair-wise relation-ships. Experiments\non publicly available datasets show that our model significantly outperforms\nstate-of-the-art methods on image captioning and scene graph generation', 'For an image with multiple scene texts, different people may be interested in\ndifferent text information. Current text-aware image captioning models are not\nable to generate distinctive captions according to various information needs.\nTo explore how to generate personalized text-aware captions, we define a new\nchallenging task, namely Question-controlled Text-aware Image Captioning\n(Qc-TextCap). With questions as control signals, this task requires models to\nunderstand questions, find related scene texts and describe them together with\nobjects fluently in human language. Based on two existing text-aware captioning\ndatasets, we automatically construct two datasets, ControlTextCaps and\nControlVizWiz to support the task. We propose a novel Geometry and Question\nAware Model (GQAM). GQAM first applies a Geometry-informed Visual Encoder to\nfuse region-level object features and region-level scene text features with\nconsidering spatial relationships. Then, we design a Question-guided Encoder to\nselect the most relevant visual features for each question. Finally, GQAM\ngenerates a personalized text-aware caption with a Multimodal Decoder. Our\nmodel achieves better captioning performance and question answering ability\nthan carefully designed baselines on both two datasets. With questions as\ncontrol signals, our model generates more informative and diverse captions than\nthe state-of-the-art text-aware captioning model. Our code and datasets are\npublicly available at https://github.com/HAWLYQ/Qc-TextCap.']"
48,39,48_face_recognition_faces_masks,"['face', 'recognition', 'faces', 'masks', 'facial', 'fingerprint', 'pad', 'masked', 'methods', 'dataset']","['In this paper, we develop face.evoLVe -- a comprehensive library that\ncollects and implements a wide range of popular deep learning-based methods for\nface recognition. First of all, face.evoLVe is composed of key components that\ncover the full process of face analytics, including face alignment, data\nprocessing, various backbones, losses, and alternatives with bags of tricks for\nimproving performance. Later, face.evoLVe supports multi-GPU training on top of\ndifferent deep learning platforms, such as PyTorch and PaddlePaddle, which\nfacilitates researchers to work on both large-scale datasets with millions of\nimages and low-shot counterparts with limited well-annotated data. More\nimportantly, along with face.evoLVe, images before & after alignment in the\ncommon benchmark datasets are released with source codes and trained models\nprovided. All these efforts lower the technical burdens in reproducing the\nexisting methods for comparison, while users of our library could focus on\ndeveloping advanced approaches more efficiently. Last but not least,\nface.evoLVe is well designed and vibrantly evolving, so that new face\nrecognition approaches can be easily plugged into our framework. Note that we\nhave used face.evoLVe to participate in a number of face recognition\ncompetitions and secured the first place. The version that supports PyTorch is\npublicly available at https://github.com/ZhaoJ9014/face.evoLVe.PyTorch and the\nPaddlePaddle version is available at\nhttps://github.com/ZhaoJ9014/face.evoLVe.PyTorch/tree/master/paddle.\nFace.evoLVe has been widely used for face analytics, receiving 2.4K stars and\n622 forks.', 'SARS-CoV-2 has presented direct and indirect challenges to the scientific\ncommunity. One of the most prominent indirect challenges advents from the\nmandatory use of face masks in a large number of countries. Face recognition\nmethods struggle to perform identity verification with similar accuracy on\nmasked and unmasked individuals. It has been shown that the performance of\nthese methods drops considerably in the presence of face masks, especially if\nthe reference image is unmasked. We propose FocusFace, a multi-task\narchitecture that uses contrastive learning to be able to accurately perform\nmasked face recognition. The proposed architecture is designed to be trained\nfrom scratch or to work on top of state-of-the-art face recognition methods\nwithout sacrificing the capabilities of a existing models in conventional face\nrecognition tasks. We also explore different approaches to design the\ncontrastive learning module. Results are presented in terms of masked-masked\n(M-M) and unmasked-masked (U-M) face verification performance. For both\nsettings, the results are on par with published methods, but for M-M\nspecifically, the proposed method was able to outperform all the solutions that\nit was compared to. We further show that when using our method on top of\nalready existing methods the training computational costs decrease\nsignificantly while retaining similar performances. The implementation and the\ntrained models are available at GitHub.', 'With the recent advancement of deep convolutional neural networks,\nsignificant progress has been made in general face recognition. However, the\nstate-of-the-art general face recognition models do not generalize well to\noccluded face images, which are exactly the common cases in real-world\nscenarios. The potential reasons are the absences of large-scale occluded face\ndata for training and specific designs for tackling corrupted features brought\nby occlusions. This paper presents a novel face recognition method that is\nrobust to occlusions based on a single end-to-end deep neural network. Our\napproach, named FROM (Face Recognition with Occlusion Masks), learns to\ndiscover the corrupted features from the deep convolutional neural networks,\nand clean them by the dynamically learned masks. In addition, we construct\nmassive occluded face images to train FROM effectively and efficiently. FROM is\nsimple yet powerful compared to the existing methods that either rely on\nexternal detectors to discover the occlusions or employ shallow models which\nare less discriminative. Experimental results on the LFW, Megaface challenge 1,\nRMF2, AR dataset and other simulated occluded/masked datasets confirm that FROM\ndramatically improves the accuracy under occlusions, and generalizes well on\ngeneral face recognition.']"
49,39,49_gravitational_black_hole_wave,"['gravitational', 'black', 'hole', 'wave', 'ligo', 'binaries', 'virgo', 'mass', 'gw', 'binary']","[""The detection of ~50 coalescing compact binaries with the Advanced LIGO and\nVirgo detectors has allowed us to test general relativity, constrain merger\nrates, and look for evidence of tidal effects, compact object spins, higher\nwaveform modes, and black hole ringdowns. An effect that has not yet been\nconfidently detected is binary eccentricity, which might be present in a small\nfraction of binaries formed dynamically. Here we discuss general limits on\neccentricity that can, in-principle, be placed on all types of compact object\nbinaries by a detector operating at the design sensitivity of Advanced LIGO.\nUsing a post-Newtonian model for gravitational-wave phasing valid in the small\neccentricity regime, we assess the relative measurement error for eccentricity\nfor a variety of spinning and non-spinning binaries. Errors and correlations\ninvolving the mass and spin parameters are also investigated. We find that\ndecreasing the low frequency limit of a detector's observational frequency band\nis one of the key design factors for increasing the odds of measuring binary\neccentricity. We also introduce and analytically explore the eccentric chirp\nmass parameter, which replaces the chirp mass as the key measurable parameter\ncombination in eccentric gravitational waveform models. The eccentric chirp\nmass parameter explains a degeneracy between the chirp mass and the\neccentricity. This degeneracy leads to a bias in the standard chirp mass\nparameter. We also investigate the systematic parameter bias that arises when\neccentric systems are recovered using circular waveform templates. We use both\nFisher matrix and Bayesian-inference-based Markov Chain Monte Carlo (MCMC)\nmethods to investigate these parameter estimation issues, and we find good\nagreement between the two approaches (for both statistical and systematic\nerrors) in the appropriate signal-to-noise ratio regime. (abridged)"", 'Gravitational-wave (GW) measurements of physical effects such as spin-induced\nquadrupole moments can distinguish binaries consisting of black holes from\nnon-black hole binaries. While these effects may be poorly constrained for\nsingle-event inferences with the second-generation detectors, combining\ninformation from multiple detections can help uncover features of non-black\nhole binaries. The spin-induced quadrupole moment has specific predictions for\ndifferent types of compact objects, and a generalized formalism must consider a\npopulation where different types of compact objects co-exist. In this study, we\nintroduce a hierarchical mixture-likelihood formalism to estimate the {\\it\nfraction of non-binary black holes in the population}. We demonstrate the\napplicability of this method using simulated GW signals injected into Gaussian\nnoise following the design sensitivities of the Advanced LIGO Advanced Virgo\ndetectors. We compare the performance of this method with a\ntraditionally-followed hierarchical inference approach. Both the methods are\nequally effective to hint at inhomogeneous populations, however, we find the\nmixture-likelihood approach to be more natural for mixture populations\ncomprising compact objects of diverse classes. We also discuss the possible\nsystematics in the mixture-likelihood approach, caused by several reasons,\nincluding the limited sensitivity of the second-generation detectors, specific\nfeatures of the astrophysical population distributions, and the limitations\nposed by the waveform models employed. Finally, we apply this method to the\nLIGO-Virgo detections published in the second GW transient catalog (GWTC-2) and\nfind them consistent with a binary black hole population within the statistical\nprecision.', 'Primordial black holes (PBHs) with a wide mass distribution imprinted by the\nthermal history of the Universe, which naturally produces a high peak at the\nsolar mass scale, could explain the gravitational-wave events seen by\nLIGO/Virgo and up to the totality of the dark matter. We show that compared to\nmonochromatic or log-normal mass functions, the gravitational wave backgrounds\n(GWBs) from early PBH binaries and from late binaries in clusters are strongly\nenhanced at low frequency and could even explain the NANOGrav observations.\nThis enhancement comes from binaries with very low mass ratios, involving\nsolar-mass and intermediate-mass PBHs at low frequency, solar-mass and\nsubsolar-mass at higher frequency. LISA could distinguish the various models,\nwhile in the frequency band of ground-based detectors, we find that the GWB\nfrom early binaries is just below the current LIGO/Virgo limits and above the\nastrophysical background, if they also explain black hole mergers. The GWB from\nbinaries in clusters is less boosted but has a different spectral index than\nfor neutron stars, astrophysical black holes or early PBH binaries. It is\ndetectable with Einstein Telescope or even with the LIGO/Virgo design\nsensitivity.']"
50,38,50_attacks_security_execution_side,"['attacks', 'security', 'execution', 'side', 'speculative', 'isolation', 'locking', 'cache', 'versioning', 'hardware']","['Shared cache resources in multi-core processors are vulnerable to cache\nside-channel attacks. Recently proposed defenses have their own caveats:\nRandomization-based defenses are vulnerable to the evolving attack algorithms\nbesides relying on weak cryptographic primitives, because they do not\nfundamentally address the root cause for cache side-channel attacks. Cache\npartitioning defenses, on the other hand, provide the strict resource\npartitioning and effectively block all side-channel threats. However, they\nusually rely on way-based partitioning which is not fine-grained and cannot\nscale to support a larger number of protection domains, e.g., in trusted\nexecution environment (TEE) security architectures, besides degrading\nperformance and often resulting in cache underutilization.\n  To overcome the shortcomings of both approaches, we present a novel and\nflexible set-associative cache partitioning design for TEE architectures,\ncalled Chunked-Cache. Chunked-Cache enables an execution context to ""carve"" out\nan exclusive configurable chunk of the cache if the execution requires\nside-channel resilience. If side-channel resilience is not required, mainstream\ncache resources are freely utilized. Hence, our solution addresses the\nsecurity-performance trade-off practically by enabling selective and on-demand\nutilization of side-channel-resilient caches, while providing well-grounded\nfuture-proof security guarantees. We show that Chunked-Cache provides\nside-channel-resilient cache utilization for sensitive code execution, with\nsmall hardware overhead, while incurring no performance overhead on the OS. We\nalso show that it outperforms conventional way-based cache partitioning by 43%,\nwhile scaling significantly better to support a larger number of protection\ndomains.', 'Hardware (HW) security issues have been emerging at an alarming rate in\nrecent years. Transient execution attacks, in particular, pose a genuine threat\nto the security of modern computing systems. Despite recent advances,\nunderstanding the intricate implications of microarchitectural design decisions\non processor security remains a great challenge and has caused a number of\nupdate cycles in the past. number of update cycles in the past. This papers\naddresses the need for a new approach to HW sign-off verification which\nguarantees the security of processors at the Register Transfer Level (RTL). To\nthis end, we introduce a formal definition of security with respect to\ntransient execution attacks, formulated as a HW property. We present a formal\nproof methodology based on Unique Program Execution Checking (UPEC) which can\nbe used to systematically detect all vulnerabilities to transient execution\nattacks in RTL designs. UPEC does not exploit any a priori knowledge on known\nattacks and can therefore detect also vulnerabilities based on new, so far\nunknown, types of channels. This is demonstrated by two new attack scenarios\ndiscovered in our experiments with UPEC. UPEC scales to a wide range of HW\ndesigns, including in-order processors (RocketChip), pipelines with\nout-of-order writeback (Ariane), and processors with deep out-of-order\nspeculative execution (BOOM). To the best of our knowledge, UPEC is the first\nRTL verification technique that exhaustively covers transient execution side\nchannels in processors of realistic complexity.', 'On modern x86 processors, data prefetching instructions can be used by\nprogrammers to boost performance. Although good for performance, we found that\nPREFETCHW, which is a data prefetching instruction to accelerate future write\noperations, has two significant security flaws on Intel processors: first, this\ninstruction can execute on data with read-only permission; second, the\nexecution time of this instruction leaks the current coherence state of the\ntarget data.\n  Based on these two design flaws, we build the first two cross-core cache\ntiming attacks that can work on private caches. Specifically, we first propose\ntwo covert channel attacks that can achieve a 864KB/s transmission rate which\nis higher than all existing cache covert channel attacks. Then we further\npropose two side channel attacks that can be used to monitor the access pattern\nof the victim running on the same processor. We demonstrate the efficacy of our\nattacks by using them to leak private information from daily applications.\nFinally, we show that our prefetch based attacks can be used in transient\nexecution attacks to leak more secrets within one speculative window.']"
51,37,51_covid_pandemic_19_disease,"['covid', 'pandemic', '19', 'disease', 'epidemic', 'infection', 'virus', 'mortality', 'infected', 'epidemiological']","['Determinants of COVID-19 clinical severity are commonly assessed by\ntransverse or longitudinal studies of the fatality counts. However, the\nfatality counts depend both on disease clinical severity and transmissibility,\nas more infected also lead to more deaths. Moreover, fatality counts (and\nrelated measures such as Case Fatality Rate) are dynamic quantities, as they\nappear with a delay to infections, while different geographic regions generally\nbelong to different points on the epidemics curve. Instead, we use\nepidemiological modeling to propose a disease severity measure, which accounts\nfor the underlying disease dynamics. The measure corresponds to the ratio of\npopulation averaged mortality and recovery rates (m/r). It is independent of\nthe disease transmission dynamics (i.e., the basic reproduction number) and has\na direct mechanistic interpretation. We use this measure to assess demographic,\nmedical, meteorological and environmental factors associated with the disease\nseverity. For this, we employ an ecological regression study design and analyze\ndifferent US states during the first disease outbreak. Principal Component\nAnalysis, followed by univariate and multivariate analyses based on machine\nlearning techniques, is used for selecting important predictors. Without using\nprior knowledge from clinical studies, we recover significant predictors known\nto influence disease severity, in particular age, chronic diseases, and racial\nfactors. Additionally, we identify long-term pollution exposure and population\ndensity as not widely recognized (though for the pollution previously\nhypothesized) predictors of the disease severity. Overall, the proposed measure\nis useful for inferring severity determinants of COVID-19 and other infectious\ndiseases, and the obtained results may aid a better understanding of COVID-19\nrisks.', 'Background: The novel coronavirus, COVID-19, was first detected in the United\nStates in January 2020. To curb the spread of the disease in mid-March,\ndifferent states issued mandatory stay-at-home (SAH) orders. These\nnonpharmaceutical interventions were mandated based on prior experiences, such\nas the 1918 influenza epidemic. Hence, we decided to study the impact of\nrestrictions on mobility on reducing COVID-19 transmission. Methods: We\ndesigned an ecological time series study with our exposure variable as Mobility\npatterns in the state of Maryland for March- December 2020 and our outcome\nvariable as the COVID-19 hospitalizations for the same period. We built an\nExtreme Gradient Boosting (XGBoost) ensemble machine learning model and\nregressed the lagged COVID-19 hospitalizations with Mobility volume for\ndifferent regions of Maryland. Results: We found an 18% increase in COVID-19\nhospitalizations when mobility was increased by a factor of five, similarly a\n43% increase when mobility was further increased by a factor of ten.\nConclusion: The findings of our study demonstrated a positive linear\nrelationship between mobility and the incidence of COVID-19 cases. These\nfindings are partially consistent with other studies suggesting the benefits of\nmobility restrictions. Although more detailed approach is needed to precisely\nunderstand the benefits and limitations of mobility restrictions as part of a\nresponse to the COVID-19 pandemic.', 'Since the outbreak of COVID-19, an astronomical number of publications on the\npandemic dynamics appeared in the literature, of which many use the susceptible\ninfected removed (SIR) and susceptible exposed infected removed (SEIR) models,\nor their variants, to simulate and study the spread of the coronavirus. SIR and\nSEIR are continuous-time models which are a class of initial value problems\n(IVPs) of ordinary differential equations (ODEs). Discrete-time models such as\nregression and machine learning have also been applied to analyze COVID-19\npandemic data (e.g. predicting infection cases), but most of these methods use\nsimplified models involving a small number of input variables pre-selected\nbased on a priori knowledge, or use very complicated models (e.g. deep\nlearning), purely focusing on certain prediction purposes and paying little\nattention to the model interpretability. There have been relatively fewer\nstudies focusing on the investigations of the inherent time-lagged or\ntime-delayed relationships e.g. between the reproduction number (R number),\ninfection cases, and deaths, analyzing the pandemic spread from a systems\nthinking and dynamic perspective. The present study, for the first time,\nproposes using systems engineering and system identification approach to build\ntransparent, interpretable, parsimonious and simulatable (TIPS) dynamic machine\nlearning models, establishing links between the R number, the infection cases\nand deaths caused by COVID-19. The TIPS models are developed based on the\nwell-known NARMAX (Nonlinear AutoRegressive Moving Average with eXogenous\ninputs) model, which can help better understand the COVID-19 pandemic dynamics.\nA case study on the UK COVID-19 data is carried out, and new findings are\ndetailed. The proposed method and the associated new findings are useful for\nbetter understanding the spread dynamics of the COVID-19 pandemic.']"
52,35,52_telescope_ray_cta_telescopes,"['telescope', 'ray', 'cta', 'telescopes', 'camera', 'gamma', 'cherenkov', 'will', 'array', 'observatory']","['A prototype Schwarzschild-Couder Telescope (pSCT) has been constructed at the\nFred Lawrence Whipple Observatory as a candidate for the medium-sized\ntelescopes of the Cherenkov Telescope Array Observatory (CTAO). CTAO is\ncurrently entering early construction phase of the project and once completed\nit will vastly improve very high energy gamma-ray detection component in\nmulti-wavelength and multi-messenger observations due to significantly improved\nsensitivity, angular resolution and field of view comparing to the current\ngeneration of the ground-based gamma-ray observatories H.E.S.S., MAGIC and\nVERITAS. The pSCT uses a dual aspheric mirror design with a $9.7$ m primary\nmirror and $5.4$ m secondary mirror, both of which are segmented. The\nSchwarzschild-Couder (SC) optical system (OS) selected for the prototype\ntelescope achieves wide field of view of $8$ degrees and simultaneously reduces\nthe focal plane plate scale allowing an unprecedented compact ($0.78$m\ndiameter) implementation of the high-resolution camera ($6$mm/ $0.067$deg per\nimaging pixel with $11,328$ pixels) based on the silicon photo-multipliers\n(SiPMs). The OS of the telescope is designed to eliminate spherical and comatic\naberrations and minimize astigmatism to radically improve off-axis imaging and\nconsequently angular resolution across all the field of view with respect to\nthe conventional single-mirror telescopes. Fast and high imaging resolution OS\nof the pSCT comes with the challenging submillimeter-precision custom alignment\nsystem, which was successfully demonstrated with an on-axis point spread\nfunction (PSF) of $2.9$ arcmin prior to the first-light detection of the Crab\nNebula in 2020. Ongoing and future commissioning activities are reported.', 'The Cherenkov Telescope Array (CTA) will use three telescope sizes to\nefficiently detect cosmic gamma rays in the energy range from several tens of\nGeV to hundreds of TeV. The Small-Sized Telescopes (SSTs) will form the largest\nsection of the array, covering an area of many square kilometres on the CTA\nsouthern site in Paranal, Chile. Up to 70 SSTs will be implemented by an\ninternational consortium of institutes and teams as an in-kind contribution to\nthe CTA Observatory. The SSTs will provide unprecedented sensitivity to gamma\nrays above 1 TeV and the highest angular resolution of any instrument above the\nhard X-ray band. CTA has recently finalised the technology that will be used\nfor the SSTs: the telescopes will be a dual-reflector design with a primary\nreflector of ~4 m diameter, equipped with an SiPM-based camera with full\nwaveform readout from $\\sim$2000 channels covering a $\\sim$9$^\\circ$ field of\nview. The Schwarzschild-Couder optical configuration leads to a small\nplate-scale, and consequently a compact, cost-efficient camera. In this\ncontribution, we describe the experience gained operating telescope and camera\nprototypes during the CTA preparatory phase, and the development of the final\nSST design.', ""The Cherenkov Telescope Array (CTA) is the next-generation ground-based\nobservatory for very-high-energy gamma-ray astronomy. An innovative 9.7 m\naperture, dual-mirror Schwarzschild-Couder Telescope (SCT) design is a\ncandidate design for CTA Medium-Sized Telescopes. A prototype SCT (pSCT) has\nbeen constructed at the Fred Lawrence Whipple Observatory in Arizona, USA. Its\ncamera is currently partially instrumented with 1600 pixels covering a field of\nview of 2.7 degrees square. The small plate scale of the optical system allows\ndensely packed silicon photomultipliers to be used, which combined with\nhigh-density trigger and waveform readout electronics enable the\nhigh-resolution camera. The camera's electronics are capable of imaging air\nshower development at a rate of one billion samples per second. We describe the\ncommissioning and performance of the pSCT camera, including trigger and\nwaveform readout performance, calibration, and absolute GPS time stamping. We\nalso present the upgrade to the camera, which is currently underway. The\nupgrade will fully populate the focal plane, increasing the field of view to 8\ndegree diameter, and lower the front-end electronics noise, enabling a lower\ntrigger threshold and improved reconstruction and background rejection.""]"
53,34,53_metasurfaces_optical_plasmonic_metasurface,"['metasurfaces', 'optical', 'plasmonic', 'metasurface', 'dielectric', 'light', 'nanostructures', 'nm', 'resonances', 'color']","[""Optical metasurfaces consist of a 2D arrangement of scatterers, and they\ncontrol the amplitude, phase, and polarization of an incidence field on demand.\nOptical metasurfaces are the cornerstone for a future generation of flat\noptical devices in a wide range of applications. The rapidly growing advances\nin nanofabrication have made the versatile design and analysis of these\nultra-thin surfaces an ever-growing necessity. However, despite their\nimportance, a comprehensive theory to describe the optical response of periodic\nmetasurfaces in closed-form and analytical expressions has not been formulated,\nand prior attempts were frequently approximate. Here, we develop a theory that\nanalytically links the properties of the scatterer, from which a periodic\nmetasurface is made, to its optical response via the lattice coupling matrix.\nThe scatterers are represented by their polarizability or T matrix, and our\ntheory works for normal and oblique incidence. We provide explicit expressions\nfor the optical response up to octupolar order in both spherical and Cartesian\ncoordinates. Several examples demonstrate that our analytical tool constitutes\na paradigm shift in designing and understanding optical metasurfaces. Novel\nfully-diffracting metagratings and particle-independent polarization filters\nare proposed, and novel insights into the response of Huygens' metasurfaces\nunder oblique incidence are provided. Our analytical expressions are a powerful\ntool for exploring the physics of metasurfaces and designing novel flat optics\ndevices."", 'Plasmonic metasurfaces based on the extraordinary optical transmission effect\n(EOT) can be designed to efficiently transmit specific spectral bands from the\nvisible to the far-infrared regimes, offering numerous applications in\nim-portant technological fields such as compact multispectral imaging,\nbiological and chemical sensing, or color displays. However, due to their\nsubwavelength nature, EOT metasurfaces are nowadays fabricated with nano- and\nmicro-lithographic techniques, requiring many processing steps and carried out\nin expensive cleanroom environments. In this work, we propose and\nexperimentally demonstrate a novel, single-step process for the rapid\nfabrication of high performance mid- and long-wave infrared EOT metasurfaces\nemploying ultrafast direct laser writing (DLW). Micro-hole arrays composing\nextraordinary transmission metasurfaces were fabricated over areas of 4 mm2 in\ntimescales of units of minutes, employing single pulse ablation of 40 nm thick\nAu films on dielectric substrates mounted on a high-precision motorized stage.\nWe show how by carefully characterizing the influence of only three key\nexperimental pa-rameters on the processed micro-morphologies (namely laser\npulse energy, scan velocity and beam shaping slit), we can have on-demand\ncontrol of the optical characteristics of the extraordinary transmission effect\nin terms of transmission wavelength, quality factor and polarization\nsensitivity of the resonances. To illustrate this concept, a set of EOT\nmetasurfaces having different performances and operating in different spectral\nregimes has been successfully designed, fabricated and tested. Comparison\nbetween transmittance measurements and numerical simulations have revealed that\nall the fabricated devices behave as expected, thus demonstrating the high\nperformance, flexibility and reliability of the proposed fabrication method.', 'Multiresonant metasurfaces could enable many applications in filtering,\nsensing and nonlinear optics. However, developing a metasurface with more than\none high-quality-factor or high-Q resonance at designated resonant wavelengths\nis challenging. Here, we experimentally demonstrate a plasmonic metasurface\nexhibiting different, narrow surface lattice resonances by exploiting the\npolarization degree of freedom where different lattice modes propagate along\ndifferent dimensions of the lattice. The surface consists of aluminum\nnanostructures in a rectangular periodic lattice. The resulting surface lattice\nresonances were measured around 630 nm and 1160 nm with Q-factors of ~50 and\n~800, respectively. The latter is a record-high plasmonic Q-factor within the\nnear-infrared type-II window. Such metasurfaces could benefit applications such\nas frequency conversion and all-optical switching.']"
54,34,54_spiking_synaptic_snn_neuromorphic,"['spiking', 'synaptic', 'snn', 'neuromorphic', 'memory', 'snns', 'neural', 'memories', 'synapses', 'memristor']","['Neuromorphic Computing (NC), which emulates neural activities of the human\nbrain, is considered for low-power implementation of artificial intelligence.\nTowards realizing NC, fabrication, and investigations of hardware elements such\nas synaptic devices and neurons are essential. Electrolyte gating has been\nwidely used for conductance modulation by massive carrier injections and has\nproven to be an effective way of emulating biological synapses. Synaptic\ndevices, in the form of synaptic transistors, have been studied using a wide\nvariety of materials. However, studies on metallic channel based synaptic\ntransistors remain vastly unexplored. Here, we have demonstrated a\nthree-terminal cobalt-based synaptic transistor to emulate biological synapse.\nWe realized gating controlled multilevel, nonvolatile conducting states in the\nproposed device. The device could successfully emulate essential synaptic\nfunctions demonstrating short-term and long-term plasticity. A transition from\nshort-term memory to long-term memory has been realized by tuning gate pulse\namplitude and duration. The crucial cognitive behavior viz., learning,\nforgetting, and relearning, has been emulated, showing resemblance to the human\nbrain. Along with learning and memory, the device showed dynamic filtering\nbehavior. These results provide an insight into the design of metallic channel\nbased synaptic transistors for neuromorphic computing.', 'Spiking neural networks (SNNs) have become an interesting alternative to\nconventional artificial neural networks (ANN) thanks to their temporal\nprocessing capabilities and their low-SWaP (Size, Weight, and Power) and energy\nefficient implementations in neuromorphic hardware. However the challenges\ninvolved in training SNNs have limited their performance in terms of accuracy\nand thus their applications. Improving learning algorithms and neural\narchitectures for a more accurate feature extraction is therefore one of the\ncurrent priorities in SNN research. In this paper we present a study on the key\ncomponents of modern spiking architectures. We empirically compare different\ntechniques in image classification datasets taken from the best performing\nnetworks. We design a spiking version of the successful residual network\n(ResNet) architecture and test different components and training strategies on\nit. Our results provide a state of the art guide to SNN design, which allows to\nmake informed choices when trying to build the optimal visual feature\nextractor. Finally, our network outperforms previous SNN architectures in\nCIFAR-10 (94.1%) and CIFAR-100 (74.5%) datasets and matches the state of the\nart in DVS-CIFAR10 (71.3%), with less parameters than the previous state of the\nart and without the need for ANN-SNN conversion. Code available at\nhttps://github.com/VicenteAlex/Spiking_ResNet.', 'Spiking Neural Networks (SNNs) are gaining widespread momentum in the field\nof neuromorphic computing. These network systems integrated with neurons and\nsynapses provide computational efficiency by mimicking the human brain. It is\ndesired to incorporate the biological neuronal dynamics, including complex\nspiking patterns which represent diverse brain activities within the neural\nnetworks. Earlier hardware realization of neurons was (1) area intensive\nbecause of large capacitors in the circuit design, (2) neuronal spiking\npatterns were demonstrated with clocked neurons at the device level. To achieve\nmore realistic biological neuron spiking behavior, emerging memristive devices\nare considered promising alternatives. In this paper, we propose, PrMnO3(PMO)\n-RRAM device-based neuron. The voltage-controlled electrothermal timescales of\nthe compact PMO RRAM device replace the electrical timescales of charging a\nlarge capacitor. The electrothermal timescale is used to implement an\nintegration block with multiple voltage-controlled timescales coupled with a\nrefractory block to generate biological neuronal dynamics. Here, first, a\nVerilog-A implementation of the thermal device model is demonstrated, which\ncaptures the current-temperature dynamics of the PMO device. Second, a driving\ncircuitry is designed to mimic different spiking patterns of cortical neurons,\nincluding Intrinsic bursting (IB) and Chattering (CH). Third, a neuron circuit\nmodel is simulated, which includes the PMO RRAM device model and the driving\ncircuitry to demonstrate the asynchronous neuron behavior. Finally, a\nhardware-software hybrid analysis is done in which the PMO RRAM device is\nexperimentally characterized to mimic neuron spiking dynamics. The work\npresents a realizable and more biologically comparable hardware-efficient\nsolution for large-scale SNNs.']"
55,34,55_quantum_qkd_protocol_entanglement,"['quantum', 'qkd', 'protocol', 'entanglement', 'protocols', 'key', 'secure', 'channel', 'veto', 'cv']","['We propose a W state-based protocol for anonymously transmitting quantum\nmessages in a quantum network. Different from the existing protocols [A.\nUnnikrishnan, et al., Phys. Rev. Lett. 122, 240501 (2019)], the proposed\nprotocol can be effectively implemented in the network only equipped with\nquantum channels and regular broadcast channels. Throughout the design\nprocedure, we develop three sub-protocols using the W state, including the\nquantum collision detection protocol and the quantum notification protocol.\nMoreover, together with the conventional anonymous entanglement protocol, the\nwhole anonymous communication protocol has been constructed. Finally, we\nexamine the correctness and security of the proposed quantum anonymous\ncommunication protocol.', 'This paper considers the problem of secure packet routing at the maximum\nachievable rate in a Quantum key distribution (QKD) network. Assume that a QKD\nprotocol generates symmetric private keys for secure communication over each\nlink in a multi-hop network. The quantum key generation process, which is\naffected by noise, is assumed to be modeled by a stochastic counting process.\nPackets are first encrypted with the available quantum keys for each hop and\nthen transmitted on a point-to-point basis over the communication links. A\nfundamental problem that arises in this setting is to design a secure and\ncapacity-achieving routing policy that accounts for the time-varying\navailability of the quantum keys for encryption and finite link capacities for\ntransmission. In this paper, by combining the QKD protocol with the Universal\nMax Weight (UMW) routing policy, we design a new secure throughput-optimal\nrouting policy, called Tandem Queue Decomposition (TQD). TQD solves the problem\nof secure routing efficiently for a wide class of traffic, including unicast,\nbroadcast, and multicast. One of our main contributions in this paper is to\nshow that the problem can be reduced to the usual generalized network flow\nproblem on a transformed network without the key availability constraints.\nSimulation results show that the proposed policy incurs a substantially smaller\ndelay as compared to the state-of-the-art routing and key management policies.\nThe proof of throughput-optimality of the proposed policy makes use of the\nLyapunov stability theory along with a careful treatment of the key-storage\ndynamics.', ""Quantum key distribution (QKD) establishes secure links between remote\ncommunication parties. As a key problem for various QKD protocols, security\nanalysis gives the amount of secure keys regardless of the eavesdropper's\ncomputational power, which can be done both analytically and numerically.\nCompared to analytical methods which tend to require techniques specific to\neach QKD protocol, numerical ones are more general since they can be directly\napplied to many QKD protocols without additional techniques. However, current\nnumerical methods are carried out based on some assumptions such as working in\nasymptotic limit and collective attacks from eavesdroppers. In this work, we\nremove these assumptions and develop a numerical finite-size security analysis\nagainst general attacks for general QKD protocols. We also give an example of\napplying the method to the recent Phase-Matching QKD protocol with a simple\nprotocol design. Our result shows that the finite-size key rate can surpass the\nlinear key-rate bound in a realistic communication time.""]"
56,33,56_segmentation_semantic_rgb_module,"['segmentation', 'semantic', 'rgb', 'module', 'depth', 'boundary', 'object', 'information', 'image', 'features']","[""In this paper, we focus on the challenging multicategory instance\nsegmentation problem in remote sensing images (RSIs), which aims at predicting\nthe categories of all instances and localizing them with pixel-level masks.\nAlthough many landmark frameworks have demonstrated promising performance in\ninstance segmentation, the complexity in the background and scale variability\ninstances still remain challenging for instance segmentation of RSIs. To\naddress the above problems, we propose an end-to-end multi-category instance\nsegmentation model, namely Semantic Attention and Scale Complementary Network,\nwhich mainly consists of a Semantic Attention (SEA) module and a Scale\nComplementary Mask Branch (SCMB). The SEA module contains a simple fully\nconvolutional semantic segmentation branch with extra supervision to strengthen\nthe activation of interest instances on the feature map and reduce the\nbackground noise's interference. To handle the under-segmentation of geospatial\ninstances with large varying scales, we design the SCMB that extends the\noriginal single mask branch to trident mask branches and introduces\ncomplementary mask supervision at different scales to sufficiently leverage the\nmulti-scale information. We conduct comprehensive experiments to evaluate the\neffectiveness of our proposed method on the iSAID dataset and the NWPU Instance\nSegmentation dataset and achieve promising performance."", 'The popularity and promotion of depth maps have brought new vigor and\nvitality into salient object detection (SOD), and a mass of RGB-D SOD\nalgorithms have been proposed, mainly concentrating on how to better integrate\ncross-modality features from RGB image and depth map. For the cross-modality\ninteraction in feature encoder, existing methods either indiscriminately treat\nRGB and depth modalities, or only habitually utilize depth cues as auxiliary\ninformation of the RGB branch. Different from them, we reconsider the status of\ntwo modalities and propose a novel Cross-modality Discrepant Interaction\nNetwork (CDINet) for RGB-D SOD, which differentially models the dependence of\ntwo modalities according to the feature representations of different layers. To\nthis end, two components are designed to implement the effective cross-modality\ninteraction: 1) the RGB-induced Detail Enhancement (RDE) module leverages RGB\nmodality to enhance the details of the depth features in low-level encoder\nstage. 2) the Depth-induced Semantic Enhancement (DSE) module transfers the\nobject positioning and internal consistency of depth features to the RGB branch\nin high-level encoder stage. Furthermore, we also design a Dense Decoding\nReconstruction (DDR) structure, which constructs a semantic block by combining\nmulti-level encoder features to upgrade the skip connection in the feature\ndecoding. Extensive experiments on five benchmark datasets demonstrate that our\nnetwork outperforms $15$ state-of-the-art methods both quantitatively and\nqualitatively. Our code is publicly available at:\nhttps://rmcong.github.io/proj_CDINet.html.', 'Modelling long-range contextual relationships is critical for pixel-wise\nprediction tasks such as semantic segmentation. However, convolutional neural\nnetworks (CNNs) are inherently limited to model such dependencies due to the\nnaive structure in its building modules (\\eg, local convolution kernel). While\nrecent global aggregation methods are beneficial for long-range structure\ninformation modelling, they would oversmooth and bring noise to the regions\ncontaining fine details (\\eg,~boundaries and small objects), which are very\nmuch cared for the semantic segmentation task. To alleviate this problem, we\npropose to explore the local context for making the aggregated long-range\nrelationship being distributed more accurately in local regions. In particular,\nwe design a novel local distribution module which models the affinity map\nbetween global and local relationship for each pixel adaptively. Integrating\nexisting global aggregation modules, we show that our approach can be\nmodularized as an end-to-end trainable block and easily plugged into existing\nsemantic segmentation networks, giving rise to the \\emph{GALD} networks.\nDespite its simplicity and versatility, our approach allows us to build new\nstate of the art on major semantic segmentation benchmarks including\nCityscapes, ADE20K, Pascal Context, Camvid and COCO-stuff. Code and trained\nmodels are released at \\url{https://github.com/lxtGH/GALD-DGCNet} to foster\nfurther research.']"
57,32,57_topology_optimization_material_element,"['topology', 'optimization', 'material', 'element', 'structures', 'crack', 'formulation', 'method', 'elastic', 'elements']","['This paper extends the nonsmooth Relaxed Variational Approach (RVA) to\ntopology optimization, proposed by the authors in a preceding work, to the\nsolution of thermal optimization problems. First, the RVA topology optimization\nmethod is briefly discussed and, then, it is applied to a set of representative\nproblems in which the thermal compliance, the deviation of the heat flux from a\ngiven field and the average temperature are minimized. For each optimization\nproblem, the relaxed topological derivative and the corresponding adjoint\nequations are presented. This set of expressions are then discretized in the\ncontext of the finite element method and used in the optimization algorithm to\nupdate the characteristic function. Finally, some representative (3D) thermal\ntopology optimization examples are presented to asses the performance of the\nproposed method and the Relaxed Variational Approach solutions are compared\nwith the ones obtained with the level set method in terms of the cost function,\nthe topology design and the computational cost.', 'This paper deals with the applications of stochastic spectral methods for\nstructural topology optimization in the presence of uncertainties. A\nnon-intrusive polynomial chaos expansion is integrated into a topology\noptimization algorithm to calculate low-order statistical moments of the\nmechanical-mathematical model response. This procedure, known as robust\ntopology optimization, can optimize the mean of the compliance while\nsimultaneously minimizing its standard deviation. In order to address possible\nvariabilities in the loads applied to the mechanical system of interest,\nmagnitude and direction of the external forces are assumed to be uncertain. In\nthis probabilistic framework, forces are described as a random field or a set\nof random variables. Representation of the random objects and propagation of\nload uncertainties through the model are efficiently done through\nKarhunen-Lo\\`{e}ve and polynomial chaos expansions. We take advantage of using\npolygonal elements, which have been shown to be effective in suppressing\ncheckerboard patterns and reducing mesh dependency in the solution of topology\noptimization problems. Accuracy and applicability of the proposed methodology\nare demonstrated by means of several topology optimization examples. The\nobtained results, which are in excellent agreement with reference solutions\ncomputed via Monte Carlo method, show that load uncertainties play an important\nrole in optimal design of structural systems, so that they must be taken into\naccount to ensure a reliable optimization process.', 'This paper considers the design of structures made of engineered materials,\naccounting for uncertainty in material properties. We present a topology\noptimization approach that optimizes the structural shape and topology at the\nmacroscale assuming design-independent uncertain microstructures. The\nstructural geometry at the macroscale is described by an explicit level set\napproach, and the macroscopic structural response is predicted by the eXtended\nFinite Element Method (XFEM). We describe the microscopic layout by either an\nanalytic geometric model with uncertain parameters or a level cut from a\nGaussian random field. The macroscale properties of the microstructured\nmaterial are predicted by homogenization. Considering the large number of\npossible microscale configurations, one of the main challenges of solving such\ntopology optimization problems is the computational cost of estimating the\nstatistical moments of the cost and constraint functions and their gradients\nwith respect to the design variables. Methods for predicting these moments,\nsuch as Monte Carlo sampling, and Taylor series and polynomial chaos expansions\noften require many random samples resulting in an impractical computation. To\nreduce this cost, we propose an approach wherein, at every design iteration, we\nonly use a small number of microstructure configurations to generate an\nindependent, stochastic approximation of the gradients. These gradients are\nthen used either with a gradient descent algorithm, namely Adam, or the\nglobally convergent method of moving asymptotes (GCMMA). Three numerical\nexamples from structural mechanics are used to show that the proposed approach\nprovides a computationally efficient way for macroscale topology optimization\nin the presence of microstructural uncertainty and enables the designers to\nconsider a new class of problems that are out of reach today with conventional\ntools.']"
58,32,58_convergence_convex_gradient_optimization,"['convergence', 'convex', 'gradient', 'optimization', 'point', 'blo', 'bf', 'function', 'bilevel', 'nesterov']","['We estimate convergence rates for fixed-point iterations of a class of\nnonlinear operators which are partially motivated from solving convex\noptimization problems. We introduce the notion of the generalized averaged\nnonexpansive (GAN) operator with a positive exponent, and provide a convergence\nrate analysis of the fixed-point iteration of the GAN operator. The proposed\ngeneralized averaged nonexpansiveness is weaker than the averaged\nnonexpansiveness while stronger than nonexpansiveness. We show that the\nfixed-point iteration of a GAN operator with a positive exponent converges to\nits fixed-point and estimate the local convergence rate (the convergence rate\nin terms of the distance between consecutive iterates) according to the range\nof the exponent. We prove that the fixed-point iteration of a GAN operator with\na positive exponent strictly smaller than 1 can achieve an exponential global\nconvergence rate (the convergence rate in terms of the distance between an\niterate and the solution). Furthermore, we establish the global convergence\nrate of the fixed-point iteration of a GAN operator, depending on both the\nexponent of generalized averaged nonexpansiveness and the exponent of the\nH$\\ddot{\\text{o}}$lder regularity, if the GAN operator is also\nH$\\ddot{\\text{o}}$lder regular. We then apply the established theory to three\ntypes of convex optimization problems that appear often in data science to\ndesign fixed-point iterative algorithms for solving these optimization problems\nand to analyze their convergence properties.', 'We present a unified convergence analysis for first order convex optimization\nmethods using the concept of strong Lyapunov conditions. Combining this with\nsuitable time scaling factors, we are able to handle both convex and strong\nconvex cases, and establish contraction properties of Lyapunov functions for\nmany existing ordinary differential equation models. Then we derive prevailing\nfirst order optimization algorithms, such as proximal gradient methods, heavy\nball methods (also known as momentum methods), Nesterov accelerated gradient\nmethods, and accelerated proximal gradient methods from numerical\ndiscretizations of corresponding dynamical systems. We also apply strong\nLyapunov conditions to the discrete level and provide a more systematical\nanalysis framework. Another contribution is a novel second order dynamical\nsystem called Hessian-driven Nesterov accelerated gradient flow which can be\nused to design and analyze accelerated first order methods for smooth and\nnon-smooth convex optimizations.', ""We study the trade-off between convergence rate and sensitivity to stochastic\nadditive gradient noise for first-order optimization methods. Ordinary Gradient\nDescent (GD) can be made fast-and-sensitive or slow-and-robust by increasing or\ndecreasing the stepsize, respectively. However, it is not clear how such a\ntrade-off can be navigated when working with accelerated methods such as\nPolyak's Heavy Ball (HB) or Nesterov's Fast Gradient (FG) methods, or whether\nany of these methods can achieve an optimal trade-off. We consider three\nclasses of functions: (1) strongly convex quadratics, (2) smooth strongly\nconvex functions, and (3) nonconvex functions that satisfy a weak notion of\nstrong convexity. For each function class, we present a tractable way to\ncompute convergence rate and sensitivity to additive gradient noise for a broad\nfamily of first-order methods, and we present near-Pareto-optimal algorithm\ndesigns. Each design consists of a simple analytic update rule with two states\nof memory, similar to HB and FG. Moreover, each design has a scalar tuning\nparameter that explicitly trades off convergence rate and sensitivity to\nadditive gradient noise. When tuned as aggressively as possible, our proposed\nalgorithms recover the algorithms with fastest-known convergence rates for each\nfunction class. When tuned to be more robust, our algorithms are novel and\nprovide a practical way to control noise sensitivity while maintaining the\nfastest possible convergence rate. We validate the performance and\nnear-optimality of our designs through numerous numerical simulations.""]"
59,31,59_topological_photonic_weyl_chiral,"['topological', 'photonic', 'weyl', 'chiral', 'light', 'coupling', 'modes', 'optical', 'mode', 'states']","['Weyl points are the degenerate points in three-dimensional momentum space\nwith nontrivial topological phase, which are usually realized in classical\nsystem with structure and symmetry designs. Here we proposed a one-dimensional\nlayer-stacked photonic crystal using anisotropic materials to realize ideal\ntype-II Weyl points without structure designs. The topological transition from\ntwo Dirac points to four Weyl points can be clearly observed by tuning the\ntwist angle between layers. Besides, on the interface between the photonic\ntype-II Weyl material and air, gappless surface states have also been\ndemonstrated in an incomplete bulk bandgap. By breaking parameter symmetry,\nthese ideal type-II Weyl points at the same frequency would transform into the\nnon-ideal ones, and exhibit topological surface states with single group\nvelocity. Our work may provide a new idea for the realization of photonic Weyl\npoints or other semimetal phases by utilizing naturally anisotropic materials.', 'The past decade has witnessed a booming development of topological photonics,\nwhich revolutionizes the methodology for controlling the behavior of light. A\ngigantic achievement is to engineer robust confined modes localized at\ninterfaces between topologically distinct regions, where the optical context\ncan trigger exotic topological phenomena exclusive to photons. Here, we provide\nan experimentally flexible approach to engineering topologically induced\ninterface states in the visible regime via a unique design of complex\nsuperlattice formed by connecting two component superlattices of distinguished\ntopological phases. Assisted by the intrinsic pseudospin degree due to the\nsplitting between TM and TE polarized modes, we attain a precise manipulation\nof the spin-dependent topological interface states that can manifest themselves\nstraightforwardly through transmission spectra. More specifically, since these\ntopological localized modes stem from the hybridization of artificial photonic\norbitals that are of topological origin as well, they are deemed as a novel\ntopological effect and thus named as the secondary topological interface\nstates. Our work develops an innovative and productive strategy to tune\ntopologically protected localized modes, based on which various applications\nsuch as selective local enhancement can be exploited.', 'On-chip chiral quantum light-matter interfaces, which support directional\ninteractions, provide a promising platform for efficient spin-photon coupling,\nnon-reciprocal photonic elements, and quantum logic architectures. We present\nfull-wave three-dimensional calculations to quantify the performance of\nconventional and topological photonic crystal waveguides as chiral\nemitter-photon interfaces. Specifically, the ability of these structures to\nsupport and enhance directional interactions while suppressing subsequent\nbackscattering losses is quantified. Broken symmetry waveguides, such as the\nnon-topological glide-plane waveguide and topological bearded interface\nwaveguide are found to act as efficient chiral interfaces, with the topological\nwaveguide modes allowing for operation at significantly higher Purcell\nenhancement factors. Finally, although all structures suffer from\nbackscattering losses due to fabrication imperfections, these are found to be\nsmaller at high enhancement factors for the topological waveguide. These\nreduced losses occur because the optical mode is pushed away from the\nair-dielectric interfaces where scattering occurs, and not because of any\ntopological protection. These results are important to the understanding of\nlight-matter interactions in topological photonic crystals and to the design of\nefficient, on-chip chiral quantum devices.']"
60,31,60_plasma_beam_bunch_fusion,"['plasma', 'beam', 'bunch', 'fusion', 'neutron', 'magnetic', 'helium', 'particle', 'electron', 'discharge']","[""The self-consistent nonlinear dynamics of a relativistic charged particle\nbeam interacting with its complete self-fields is a fundamental problem\nunderpinning many of the accelerator design issues in high brightness beam\napplications, as well as the development of advanced accelerators.\nParticularly, synchrotron radiation induced effects in a magnetic dispersive\nbeamline element can lead to collective beam instabilities and emittance\ngrowth. A novel beam dynamic code is developed based on a Lagrangian method for\nthe calculation of the particles' radiation near-fields using wavefront/wavelet\nmeshes via the Green's function of the Maxwell equations. These fields are then\ninterpolated onto a moving mesh for dynamic update of the beam. This method\nallows radiation co-propagation and self-consistent interaction with the beam\nin the simulation at greatly reduced numerical errors. Multiple levels of\nparallelisms are inherent in this method and implemented in our code CoSyR to\nenable at-scale simulations of nonlinear beam dynamics on modern computing\nplatforms using MPI, multi-threading, and GPUs. CoSyR has been used to evaluate\nthe transverse and longitudinal coherent radiation effects on the beam and to\ninvestigate beam optics designs proposed for mitigation of beam brightness\ndegradation in a magnetic bunch compressor. In this paper, the design of CoSyR,\nas well as the benchmark with other coherent synchrotron radiation models, are\ndescribed and discussed."", 'Short bunch proton beams are of great significance for the applications of\nwhite neutron beams and muon beams. The accelerator complex of China Spallation\nNeutron Source (CSNS) was designed to support the applications mainly based on\nneutron scattering techniques where the proton pulse length is not very\nsensitive. Some theoretical and experimental studies have been performed to see\nif one can extract a short-bunch proton beam by bunch rotation from the rapid\ncycling synchrotron (RCS) at CSNS. The experimental results at RCS have\nevidently displayed the bunch lengthening and rotation process, which\ndemonstrates the effectiveness of this method even with a very short available\ntime for the RF gymnastic processes and a high-intensity beam. With a beam\npower of 50 kW and normal longitudinal emittance at the injection, the proton\nbeam with a bunch length of about 53% with respect to the one in the normal\noperation mode was obtained and transported to the spallation target. With a\nreduced longitudinal emittance at injection and the beam power of 30 kW, the\nshortest extraction bunch length obtained is about 26% of the one in the normal\noperation mode. Different machine settings have also been tested to show the\nimpact of the desynchronization between the RF and magnetic fields, the\ninfluence of the non-adiabatic risetime and the adiabatic decay time of the RF\nvoltage on the extraction bunch length. The experimental results are well\nconsistent with the theoretical and simulated ones. It is interesting to\nobserve that space charge has a beneficial effect on the bunch lengthening\nwhich will result in a shorter bunch at the extraction with the later bunch\nrotation. The controlled desynchronization method between the RF and magnetic\nfields in an RCS was also proven successful.', 'We present a theoretical investigation of the chromatic dynamics of the\nwitness beam within a plasma based accelerator. We derive the single particle\nmotion of an electron in an ion column within a nonlinear, blowout wake\nincluding adiabatic dampening and adiabatic variations in plasma density. Using\nthis, we calculate the evolution of the beam moments and emittance for an\nelectron beam. Our model can handle near arbitrary longitudinal phase space\ndistributions. We include the effects of energy change in the beam, imperfect\nwake loading, initial transverse offsets of the beam, and mismatch between the\nbeam and plasma. We use our model to derive analytic saturation lengths for the\nprojected, longitudinal slice, and energy slice emittance under different beam\nloading conditions. Further, we show that the centroid oscillations and spot\nsizes vary between the slices and the variation depends strongly on the beam\nloading. Next, we show how a beam evolves in a full plasma source with density\nramps and show that the integral of the plasma density along the ramp\ndetermines the impact on the beam. Finally, we derive several simple scaling\nlaws that show how to design a plasma based injector to produce a target beam\nenergy and energy spread.']"
61,29,61_noma_access_wireless_owc,"['noma', 'access', 'wireless', 'owc', 'latency', 'interference', 'multiple', 'scheduling', 'mmwave', 'spectrum']","['The millimeter-wave (mmW) communications is a key enabling technology in 5G\nto provide ultra-high throughput. Current mmW technologies rely on analog\nphased arrays to realize beamforming gain and overcome high path loss. However,\ndue to a limited number of simultaneous beams that can be created with\nanalog/hybrid phased antenna arrays, the overheads of beam training and beam\nscheduling become a bottleneck for emerging networks that need to support a\nlarge number of users and low latency applications. This paper introduces\nrainbow-link, a novel multiple access protocol, that can achieve low latency\nand massive connectivity by exploiting wide bandwidth at mmW frequencies and\nnovel analog true-time-delay array architecture with frequency dependent\nbeamforming capability. In the proposed design, the network infrastructure is\nequipped with the true-time-delay array to simultaneously steer different\nfrequency resource blocks towards distinct directions covering the entire cell\nsector. Users or devices, equipped with a narrowband receiver and either a\nsingle antenna or small phased antenna array, connect to the network based on\ntheir angular positions by selecting frequency resources within their rainbow\nbeam allocation. Rainbow-link is combined with a contention-based grant-free\naccess to eliminate the explicit beam training and user scheduling. The\nproposed design and analysis show that rainbow-link grant-free access is a\npotential candidate for latency-critical use cases within massive connectivity.\nOur results show that, given less than 1e-5 probability of packet loss, a\nrainbow-link cell, over 1 GHz bandwidth using 64 element antenna array, attains\nsub-millisecond user-plane latency and Mbps user rates with an approximate 400m\nline-of-sight coverage and a density of up to 5 active single antenna users per\nsecond per meter square.', 'Massive Ultra-Reliable and Low-Latency Communications (mURLLC), which\nintegrates URLLC with massive access, is emerging as a new and important\nservice class in the next generation (6G) for time-sensitive traffics and has\nrecently received tremendous research attention. However, realizing efficient,\ndelay-bounded, and reliable communications for a massive number of user\nequipments (UEs) in mURLLC, is extremely challenging as it needs to\nsimultaneously take into account the latency, reliability, and massive access\nrequirements. To support these requirements, the third generation partnership\nproject (3GPP) has introduced enhanced grant-free (GF) transmission in the\nuplink (UL), with multiple active configured-grants (CGs) for URLLC UEs. With\nmultiple CGs (MCG) for UL, UE can choose any of these grants as soon as the\ndata arrives. In addition, non-orthogonal multiple access (NOMA) has been\nproposed to synergize with GF transmission to mitigate the serious transmission\ndelay and network congestion problems. In this paper, we develop a novel\nlearning framework for MCG-GF-NOMA systems with bursty traffic. We first design\nthe MCG-GF-NOMA model by characterizing each CG using the parameters: the\nnumber of contention-transmission units (CTUs), the starting slot of each CG\nwithin a subframe, and the number of repetitions of each CG. Based on the\nmodel, the latency and reliability performances are characterized. We then\nformulate the MCG-GF-NOMA resources configuration problem taking into account\nthree constraints. Finally, we propose a Cooperative Multi-Agent based Double\nDeep Q-Network (CMA-DDQN) algorithm to allocate the channel resources among\nMCGs so as to maximize the number of successful transmissions under the latency\nconstraint. Our results show that the MCG-GF-NOMA framework can simultaneously\nimprove the low latency and high reliability performances in massive URLLC.', 'Due to the explosive growth in the number of wireless devices and diverse\nwireless services, such as virtual/augmented reality and\nInternet-of-Everything, next generation wireless networks face unprecedented\nchallenges caused by heterogeneous data traffic, massive connectivity, and\nultra-high bandwidth efficiency and ultra-low latency requirements. To address\nthese challenges, advanced multiple access schemes are expected to be\ndeveloped, namely next generation multiple access (NGMA), which are capable of\nsupporting massive numbers of users in a more resource- and\ncomplexity-efficient manner than existing multiple access schemes. As the\nresearch on NGMA is in a very early stage, in this paper, we explore the\nevolution of NGMA with a particular focus on non-orthogonal multiple access\n(NOMA), i.e., the transition from NOMA to NGMA. In particular, we first review\nthe fundamental capacity limits of NOMA, elaborate the new requirements for\nNGMA, and discuss several possible candidate techniques. Moreover, given the\nhigh compatibility and flexibility of NOMA, we provide an overview of current\nresearch efforts on multi-antenna techniques for NOMA, promising future\napplication scenarios of NOMA, and the interplay between NOMA and other\nemerging physical layer techniques. Furthermore, we discuss advanced\nmathematical tools for facilitating the design of NOMA communication systems,\nincluding conventional optimization approaches and new machine learning\ntechniques. Next, we propose a unified framework for NGMA based on multiple\nantennas and NOMA, where both downlink and uplink transmission are considered,\nthus setting the foundation for this emerging research area. Finally, several\npractical implementation challenges for NGMA are highlighted as motivation for\nfuture work.']"
62,28,62_vr_virtual_reality_user,"['vr', 'virtual', 'reality', 'user', 'brush', 'users', 'immersive', 'ar', 'hmd', 'gestures']","['Emotions are multifaceted phenomena that affect our behaviour, perception,\nand cognition. Increasing evidence indicates that induction mechanisms play a\ncrucial role in triggering emotions by simulating the sensations required for\nan experimental design. Over the years, many reviews have evaluated a passive\nelicitation mechanism where the user is an observer, ignoring the importance of\nself-relevance in emotional experience. So, in response to the gap in the\nliterature, this study intends to explore the possibility of using Virtual\nReality (VR) as an active mechanism for emotion induction. Furthermore, for the\nsuccess and quality of research settings, VR must select the appropriate\nmaterial to effectively evoke emotions. Therefore, in the present review, we\nevaluated to what extent VR visual and audio-visual stimuli, games, and tasks,\nand 360-degree panoramas and videos can elicit emotions based on the current\nliterature. Further, we present public datasets generated by VR and\nemotion-sensing interfaces that can be used in VR based research. The\nconclusions of this survey reveal that VR has a great potential to evoke\nemotions effectively and naturally by generating motivational and empathy\nmechanisms which makes it an ecologically valid paradigm to study emotions.', ""Mode-switching supports multilevel operations using a limited number of input\nmethods. In Virtual Reality (VR) head-mounted displays (HMD), common approaches\nfor mode-switching use buttons, controllers, and users' hands. However, they\nare inefficient and challenging to do with tasks that require both hands (e.g.,\nwhen users need to use two hands during drawing operations). Using head\ngestures for mode-switching can be an efficient and cost-effective way,\nallowing for a more continuous and smooth transition between modes. In this\npaper, we explore the use of head gestures for mode-switching especially in\nscenarios when both users' hands are performing tasks. We present a first user\nstudy that evaluated eight head gestures that could be suitable for VR HMD with\na dual-hand line-drawing task. Results show that move forward, move backward,\nroll left, and roll right led to better performance and are preferred by\nparticipants. A second study integrating these four gestures in Tilt Brush, an\nopen-source painting VR application, is conducted to further explore the\napplicability of these gestures and derive insights. Results show that Tilt\nBrush with head gestures allowed users to change modes with ease and led to\nimproved interaction and user experience. The paper ends with a discussion on\nsome design recommendations for using head-based mode-switching in VR HMD."", ""Virtual Reality (VR) applications often require users to perform actions with\ntwo hands when performing tasks and interacting with objects in virtual\nenvironments. Although bimanual interactions in VR can resemble real-world\ninteractions -- thus increasing realism and improving immersion -- they can\nalso pose significant accessibility challenges to people with limited mobility,\nsuch as for people who have full use of only one hand. An opportunity exists to\ncreate accessible techniques that take advantage of users' abilities, but\ndesigners currently lack structured tools to consider alternative approaches.\nTo begin filling this gap, we propose Two-in-One, a design space that\nfacilitates the creation of accessible methods for bimanual interactions in VR\nfrom unimanual input. Our design space comprises two dimensions, bimanual\ninteractions and computer assistance, and we provide a detailed examination of\nissues to consider when creating new unimanual input techniques that map to\nbimanual interactions in VR. We used our design space to create three\ninteraction techniques that we subsequently implemented for a subset of\nbimanual interactions and received user feedback through a video elicitation\nstudy with 17 people with limited mobility. Our findings explore complex\ntradeoffs associated with autonomy and agency and highlight the need for\nadditional settings and methods to make VR accessible to people with limited\nmobility.""]"
63,28,63_refactoring_developers_code_bug,"['refactoring', 'developers', 'code', 'bug', 'software', 'smells', 'static', 'projects', 'test', 'bugs']","['Despite the availability of refactoring as a feature in popular IDEs, recent\nstudies revealed that developers are reluctant to use them, and still prefer\nthe manual refactoring of their code. At JetBrains, our goal is to fully\nsupport refactoring features in IntelliJ-based IDEs and improve their adoption\nin practice. Therefore, we start by raising the following main questions. How\nexactly do people refactor code? What refactorings are the most popular? Why do\nsome developers tend not to use convenient IDE refactoring tools?\n  In this paper, we investigate the raised questions through the design and\nimplementation of a survey targeting 1,183 users of IntelliJ-based IDEs. Our\nquantitative and qualitative analysis of the survey results shows that almost\ntwo-thirds of developers spend more than one hour in a single session\nrefactoring their code; that refactoring types vary greatly in popularity; and\nthat a lot of developers would like to know more about IDE refactoring features\nbut lack the means to do so. These results serve us internally to support the\nnext generation of refactoring features, as well as can help our research\ncommunity to establish new directions in the refactoring usability research.', ""Refactoring is widely recognized as one of the efficient techniques to manage\ntechnical debt and maintain a healthy software project through enforcing best\ndesign practices or coping with design defects. Previous refactoring surveys\nhave shown that code refactoring activities are mainly executed by developers\nwho have sufficient knowledge of the system's design and disposing of\nleadership roles in their development teams. However, these surveys were mainly\nlimited to specific projects and companies. In this paper, we explore the\ngeneralizability of the previous results by analyzing 800 open-source projects.\nWe mine their refactoring activities, and we identify their corresponding\ncontributors. Then, we associate an experience score to each contributor in\norder to test various hypotheses related to whether developers with higher\nscores tend to 1) perform a higher number of refactoring operations 2) exhibit\ndifferent motivations behind their refactoring, and 3) better document their\nrefactoring activity. We found that (1) although refactoring is not restricted\nto a subset of developers, those with higher contribution scores tend to\nperform more refactorings than others; (2) while there is no correlation\nbetween experience and motivation behind refactoring, top contributed\ndevelopers are found to perform a wider variety of refactoring operations,\nregardless of their complexity; and (3) top contributed developer tend to\ndocument less their refactoring activity. Our qualitative analysis of three\nrandomly sampled projects shows that the developers who are responsible for the\nmajority of refactoring activities are typically in advanced positions in their\ndevelopment teams, demonstrating their extensive knowledge of the design of the\nsystems they contribute to."", 'An essential part of software maintenance and evolution, refactoring is\nperformed by developers, regardless of technology or domain, to improve the\ninternal quality of the system, and reduce its technical debt. However,\nchoosing the appropriate refactoring strategy is not always straightforward,\nresulting in developers seeking assistance. Although research in refactoring is\nwell-established, with several studies altering between the detection of\nrefactoring opportunities and the recommendation of appropriate code changes,\nlittle is known about their adoption in practice. Analyzing the perception of\ndevelopers is critical to understand better what developers consider to be\nproblematic in their code and how they handle it. Additionally, there is a need\nfor bridging the gap between refactoring, as research, and its adoption in\npractice, by extracting common refactoring intents that are more suitable for\nwhat developers face in reality. In this study, we analyze refactoring\ndiscussions on Stack Overflow through a series of quantitative and qualitative\nexperiments. Our results show that Stack Overflow is utilized by a diverse set\nof developers for refactoring assistance for a variety of technologies. Our\nobservations show five areas that developers typically require help with\nrefactoring -- Code Optimization, Tools and IDEs, Architecture and Design\nPatterns, Unit Testing, and Database. We envision our findings better bridge\nthe support between traditional (or academic) aspects of refactoring and their\nreal-world applicability, including better tool support.']"
64,27,64_acoustic_bf_metamaterials_cloaks,"['acoustic', 'bf', 'metamaterials', 'cloaks', 'waves', 'metamaterial', 'elastic', 'wave', 'band', 'scattering']","['Parametric amplification -- injecting energy into waves via periodic\nmodulation of system parameters -- is typically restricted to specific\nmultiples of the modulation frequency. However, broadband parametric\namplification can be achieved in active metamaterials which allow local\nparameters to be modulated both in space and in time. Inspired by the concept\nof luminal metamaterials in optics, we describe a mechanism for one-way\namplification of sound waves across an entire frequency band using\nspacetime-periodic modulation of local stiffnesses in the form of a traveling\nwave. When the speed of the modulation wave approaches that of the speed of\nsound in the metamaterial -- a regime called the sonic limit -- nearly all\nmodes in the forward-propagating acoustic band are amplified, whereas no\namplification occurs in the reverse-propagating band. To eliminate divergences\nthat are inherent to the sonic limit in continuum materials, we use an exact\nFloquet-Bloch approach to compute the dynamic excitation bands of discrete\nperiodic systems. We find wide ranges of parameters for which the amplification\nis nearly uniform across the lowest-frequency band, enabling amplification of\nwavepackets while preserving their speed, shape, and spectral content. Our\nmechanism provides a route to designing acoustic metamaterials which can\npropagate wave pulses without losses or distortion across a wide range of\nfrequencies.', 'Local resonance band gaps in acoustic metamaterials are widely known for\ntheir strong attenuation yet narrow frequency span. The latter limits the\npractical ability to implement subwavelength band gaps for broadband\nattenuation and has motivated novel metamaterial designs in recent years. In\nthis paper, we investigate the behavior of acoustic metamaterials where unit\ncells house multiple resonating elements stacked in different configurations,\naimed at instigating a wide array of wave propagation profiles that are\notherwise unattainable. The dispersion mechanics of the multi-resonator\nmetamaterials are developed using purely analytical expressions which depict\nand explain the underlying dynamics of such systems both at the unit cell level\nas well as the frequency response of their finite realizations. The framework\nreveals the mechanism behind the transition of the lower and upper band gap\nbounds in metamaterials with parallel resonators resulting in a significant\nband gap widening. The analysis also illustrates the ability of metamaterials\nwith dual-periodic super cells to exhibit a range of dispersion transitions\nculminating in collapsing solutions of acoustic and optic bands, enabling a\ncoalescence of local resonance band gaps, vanishing resonances, and a number of\nintriguing scenarios in between.', ""Acoustic meta-atoms serve as the building blocks of metamaterials, with\nlinear properties designed to achieve functions such as beam steering, cloaking\nand focusing. They have also been used to shape the characteristics of incident\nacoustic fields, which led to the manipulation of acoustic radiation force and\ntorque for development of acoustic tweezers with improved spatial resolution.\nHowever, acoustic radiation force and torque also depend on the shape of the\nobject, which strongly affects its scattering properties. We show that by\ndesigning linear properties of an object using metamaterial concepts, the\nnonlinear acoustic effects of radiation force and torque can be controlled.\nTrapped objects are typically small compared to the wavelength, and are\ndescribed as particles, inducing monopole and dipole scattering. We extend such\nmodels to a polarizability tensor including Willis coupling terms, as a measure\nof asymmetry, capturing the significance of geometrical features. We apply our\nmodel to a three-dimensional, sub-wavelength meta-atom with maximal Willis\ncoupling, demonstrating that the force and the torque can be reversed relative\nto an equivalent symmetrical particle in acoustophoretic applications. By\nconsidering shape asymmetry in the acoustic radiation force and torque,\nGorkov's fundamental theory of acoustophoresis is thereby extended.\nAsymmetrical shapes influence the acoustic fields by shifting the stable\ntrapping location, highlighting a potential for tunable, shape-dependent\nparticle sorting.""]"
65,27,65_manipulation_robot_object_planning,"['manipulation', 'robot', 'object', 'planning', 'robotic', 'objects', 'tasks', 'motion', 'servoing', 'task']","['In this paper, we present a semi-autonomous teleoperation framework for a\npick-and-place task using an RGB-D sensor. In particular, we assume that the\ntarget object is located in a cluttered environment where both prehensile\ngrasping and non-prehensile manipulation are combined for efficient\nteleoperation. A trajectory-based reinforcement learning is utilized for\nlearning the non-prehensile manipulation to rearrange the objects for enabling\ndirect grasping. From the depth image of the cluttered environment and the\nlocation of the goal object, the learned policy can provide multiple options of\nnon-prehensile manipulation to the human operator. We carefully design a reward\nfunction for the rearranging task where the policy is trained in a simulational\nenvironment. Then, the trained policy is transferred to a real-world and\nevaluated in a number of real-world experiments with the varying number of\nobjects where we show that the proposed method outperforms manual keyboard\ncontrol in terms of the time duration for the grasping.', 'A seamless integration of robots into human environments requires robots to\nlearn how to use existing human tools. Current approaches for learning tool\nmanipulation skills mostly rely on expert demonstrations provided in the target\nrobot environment, for example, by manually guiding the robot manipulator or by\nteleoperation. In this work, we introduce an automated approach that replaces\nan expert demonstration with a Youtube video for learning a tool manipulation\nstrategy. The main contributions are twofold. First, we design an alignment\nprocedure that aligns the simulated environment with the real-world scene\nobserved in the video. This is formulated as an optimization problem that finds\na spatial alignment of the tool trajectory to maximize the sparse goal reward\ngiven by the environment. Second, we describe an imitation learning approach\nthat focuses on the trajectory of the tool rather than the motion of the human.\nFor this we combine reinforcement learning with an optimization procedure to\nfind a control policy and the placement of the robot based on the tool motion\nin the aligned environment. We demonstrate the proposed approach on spade,\nscythe and hammer tools in simulation, and show the effectiveness of the\ntrained policy for the spade on a real Franka Emika Panda robot demonstration.', 'We present a strategy for designing and building very general robot\nmanipulation systems involving the integration of a general-purpose\ntask-and-motion planner with engineered and learned perception modules that\nestimate properties and affordances of unknown objects. Such systems are\nclosed-loop policies that map from RGB images, depth images, and robot joint\nencoder measurements to robot joint position commands. We show that following\nthis strategy a task-and-motion planner can be used to plan intelligent\nbehaviors even in the absence of a priori knowledge regarding the set of\nmanipulable objects, their geometries, and their affordances. We explore\nseveral different ways of implementing such perceptual modules for\nsegmentation, property detection, shape estimation, and grasp generation. We\nshow how these modules are integrated within the PDDLStream task and motion\nplanning framework. Finally, we demonstrate that this strategy can enable a\nsingle system to perform a wide variety of real-world multi-step manipulation\ntasks, generalizing over a broad class of objects, object arrangements, and\ngoals, without any prior knowledge of the environment and without re-training.']"
66,27,66_anomaly_detection_outlier_anomalies,"['anomaly', 'detection', 'outlier', 'anomalies', 'outliers', 'novelty', 'abnormal', 'events', 'data', 'normal']","['Anomaly detection in video streams is a challenging problem because of the\nscarcity of abnormal events and the difficulty of accurately annotating them.\nTo alleviate these issues, unsupervised learning-based prediction methods have\nbeen previously applied. These approaches train the model with only normal\nevents and predict a future frame from a sequence of preceding frames by use of\nencoder-decoder architectures so that they result in small prediction errors on\nnormal events but large errors on abnormal events. The architecture, however,\ncomes with the computational burden as some anomaly detection tasks require low\ncomputational cost without sacrificing performance. In this paper,\nCross-Parallel Network (CPNet) for efficient anomaly detection is proposed here\nto minimize computations without performance drops. It consists of N smaller\nparallel U-Net, each of which is designed to handle a single input frame, to\nmake the calculations significantly more efficient. Additionally, an\ninter-network shift module is incorporated to capture temporal relationships\namong sequential frames to enable more accurate future predictions.The\nquantitative results show that our model requires less computational cost than\nthe baseline U-Net while delivering equivalent performance in anomaly\ndetection.', 'Anomaly detection plays a crucial role in various real-world applications,\nincluding healthcare and finance systems. Owing to the limited number of\nanomaly labels in these complex systems, unsupervised anomaly detection methods\nhave attracted great attention in recent years. Two major challenges faced by\nthe existing unsupervised methods are: (i) distinguishing between normal and\nabnormal data in the transition field, where normal and abnormal data are\nhighly mixed together; (ii) defining an effective metric to maximize the gap\nbetween normal and abnormal data in a hypothesis space, which is built by a\nrepresentation learner. To that end, this work proposes a novel scoring network\nwith a score-guided regularization to learn and enlarge the anomaly score\ndisparities between normal and abnormal data. With such score-guided strategy,\nthe representation learner can gradually learn more informative representation\nduring the model training stage, especially for the samples in the transition\nfield. We next propose a score-guided autoencoder (SG-AE), incorporating the\nscoring network into an autoencoder framework for anomaly detection, as well as\nother three state-of-the-art models, to further demonstrate the effectiveness\nand transferability of the design. Extensive experiments on both synthetic and\nreal-world datasets demonstrate the state-of-the-art performance of these\nscore-guided models (SGMs).', 'Edge computing enabled smart greenhouse is a representative application of\nInternet of Things technology, which can monitor the environmental information\nin real time and employ the information to contribute to intelligent\ndecision-making. In the process, anomaly detection for wireless sensor data\nplays an important role. However, traditional anomaly detection algorithms\noriginally designed for anomaly detection in static data have not properly\nconsidered the inherent characteristics of data stream produced by wireless\nsensor such as infiniteness, correlations and concept drift, which may pose a\nconsiderable challenge on anomaly detection based on data stream, and lead to\nlow detection accuracy and efficiency. First, data stream usually generates\nquickly which means that it is infinite and enormous, so any traditional\noff-line anomaly detection algorithm that attempts to store the whole dataset\nor to scan the dataset multiple times for anomaly detection will run out of\nmemory space. Second, there exist correlations among different data streams,\nwhich traditional algorithms hardly consider. Third, the underlying data\ngeneration process or data distribution may change over time. Thus, traditional\nanomaly detection algorithms with no model update will lose their effects.\nConsidering these issues, a novel method (called DLSHiForest) on basis of\nLocality-Sensitive Hashing and time window technique in this paper is proposed\nto solve these problems while achieving accurate and efficient detection.\nComprehensive experiments are executed using real-world agricultural greenhouse\ndataset to demonstrate the feasibility of our approach. Experimental results\nshow that our proposal is practicable in addressing challenges of traditional\nanomaly detection while ensuring accuracy and efficiency.']"
67,27,67_pose_estimation_3d_hand,"['pose', 'estimation', '3d', 'hand', 'joints', 'keypoint', 'poses', '6d', 'mvp', 'keypoints']","['Estimating human pose is an important yet challenging task in multimedia\napplications. Existing pose estimation libraries target reproducing standard\npose estimation algorithms. When it comes to customising these algorithms for\nreal-world applications, none of the existing libraries can offer both the\nflexibility of developing custom pose estimation algorithms and the\nhigh-performance of executing these algorithms on commodity devices. In this\npaper, we introduce Hyperpose, a novel flexible and high-performance pose\nestimation library. Hyperpose provides expressive Python APIs that enable\ndevelopers to easily customise pose estimation algorithms for their\napplications. It further provides a model inference engine highly optimised for\nreal-time pose estimation. This engine can dynamically dispatch carefully\ndesigned pose estimation tasks to CPUs and GPUs, thus automatically achieving\nhigh utilisation of hardware resources irrespective of deployment environments.\nExtensive evaluation results show that Hyperpose can achieve up to 3.1x~7.3x\nhigher pose estimation throughput compared to state-of-the-art pose estimation\nlibraries without compromising estimation accuracy. By 2021, Hyperpose has\nreceived over 1000 stars on GitHub and attracted users from both industry and\nacademy.', 'Estimating the 3D hand pose from a 2D image is a well-studied problem and a\nrequirement for several real-life applications such as virtual reality,\naugmented reality, and hand-gesture recognition. Currently, good estimations\ncan be computed starting from single RGB images, especially when forcing the\nsystem to also consider, through a multi-task learning approach, the hand shape\nwhen the pose is determined. However, when addressing the aforementioned\nreal-life tasks, performances can drop considerably depending on the hand\nrepresentation, thus suggesting that stable descriptions are required to\nachieve satisfactory results. As a consequence, in this paper we present a\nkeypoint-based end-to-end framework for the 3D hand and pose estimation, and\nsuccessfully apply it to the hand-gesture recognition task as a study case.\nSpecifically, after a pre-processing step where the images are normalized, the\nproposed pipeline comprises a multi-task semantic feature extractor generating\n2D heatmaps and hand silhouettes from RGB images; a viewpoint encoder\npredicting hand and camera view parameters; a stable hand estimator producing\nthe 3D hand pose and shape; and a loss function designed to jointly guide all\nof the components during the learning phase. To assess the proposed framework,\ntests were performed on a 3D pose and shape estimation benchmark dataset,\nobtaining state-of-the-art performances. What is more, the devised system was\nalso evaluated on 2 hand-gesture recognition benchmark datasets, where the\nframework significantly outperforms other keypoint-based approaches; indicating\nthat the presented method is an effective solution able to generate stable 3D\nestimates for the hand pose and shape.', ""Estimating the 6D pose of objects is beneficial for robotics tasks such as\ntransportation, autonomous navigation, manipulation as well as in scenarios\nbeyond robotics like virtual and augmented reality. With respect to single\nimage pose estimation, pose tracking takes into account the temporal\ninformation across multiple frames to overcome possible detection\ninconsistencies and to improve the pose estimation efficiency. In this work, we\nintroduce a novel Deep Neural Network (DNN) called VIPose, that combines\ninertial and camera data to address the object pose tracking problem in\nreal-time. The key contribution is the design of a novel DNN architecture which\nfuses visual and inertial features to predict the objects' relative 6D pose\nbetween consecutive image frames. The overall 6D pose is then estimated by\nconsecutively combining relative poses. Our approach shows remarkable pose\nestimation results for heavily occluded objects that are well known to be very\nchallenging to handle by existing state-of-the-art solutions. The effectiveness\nof the proposed approach is validated on a new dataset called VIYCB with RGB\nimage, IMU data, and accurate 6D pose annotations created by employing an\nautomated labeling technique. The approach presents accuracy performances\ncomparable to state-of-the-art techniques, but with the additional benefit of\nbeing real-time.""]"
68,27,68_channel_dnn_pilot_mimo,"['channel', 'dnn', 'pilot', 'mimo', 'codes', 'channels', 'deep', 'turboae', 'neural', 'massive']","['Accurate downlink channel information is crucial to the beamforming design,\nbut it is difficult to obtain in practice. This paper investigates a deep\nlearning-based optimization approach of the downlink beamforming to maximize\nthe system sum rate, when only the uplink channel information is available. Our\nmain contribution is to propose a model-driven learning technique that exploits\nthe structure of the optimal downlink beamforming to design an effective hybrid\nlearning strategy with the aim to maximize the sum rate performance. This is\nachieved by jointly considering the learning performance of the downlink\nchannel, the power and the sum rate in the training stage. The proposed\napproach applies to generic cases in which the uplink channel information is\navailable, but its relation to the downlink channel is unknown and does not\nrequire an explicit downlink channel estimation. We further extend the\ndeveloped technique to massive multiple-input multiple-output scenarios and\nachieve a distributed learning strategy for multicell systems without an\ninter-cell signalling overhead. Simulation results verify that our proposed\nmethod provides the performance close to the state of the art numerical\nalgorithms with perfect downlink channel information and significantly\noutperforms existing data-driven methods in terms of the sum rate.', 'Generalized optical multiple-input multiple-output (GOMIMO) techniques have\nbeen recently shown to be promising for high-speed optical wireless\ncommunication (OWC) systems. In this paper, we propose a novel deep\nlearning-aided GOMIMO (DeepGOMIMO) framework for GOMIMO systems, where channel\nstate information (CSI)-free blind detection can be enabled by employing a\nspecially designed deep neural network (DNN)-based MIMO detector. The CSI-free\nblind DNN detector mainly consists of two modules: one is the pre-processing\nmodule which is designed to address both the path loss and channel crosstalk\nissues caused by MIMO transmission, and the other is the feed-forward DNN\nmodule which is used for joint detection of spatial and constellation\ninformation by learning the statistics of both the input signal and the\nadditive noise. Our simulation results clearly verify that, in a typical indoor\n4 $\\times$ 4 MIMO-OWC system using both generalized optical spatial modulation\n(GOSM) and generalized optical spatial multiplexing (GOSMP) with unipolar\nnon-zero 4-ary pulse amplitude modulation (4-PAM) modulation, the proposed\nCSI-free blind DNN detector achieves near the same bit error rate (BER)\nperformance as the optimal joint maximum-likelihood (ML) detector, but with\nmuch reduced computational complexity. Moreover, since the CSI-free blind DNN\ndetector does not require instantaneous channel estimation to obtain accurate\nCSI, it enjoys the unique advantages of improved achievable data rate and\nreduced communication time delay in comparison to the CSI-based zero-forcing\nDNN (ZF-DNN) detector.', 'In this paper, we propose an end-to-end deep learning-based joint transceiver\ndesign algorithm for millimeter wave (mmWave) massive multiple-input\nmultiple-output (MIMO) systems, which consists of deep neural network\n(DNN)-aided pilot training, channel feedback, and hybrid analog-digital (HAD)\nprecoding. Specifically, we develop a DNN architecture that maps the received\npilots into feedback bits at the receiver, and then further maps the feedback\nbits into the hybrid precoder at the transmitter. To reduce the signaling\noverhead and channel state information (CSI) mismatch caused by the\ntransmission delay, a two-timescale DNN composed of a long-term DNN and a\nshort-term DNN is developed. The analog precoders are designed by the long-term\nDNN based on the CSI statistics and updated once in a frame consisting of a\nnumber of time slots. In contrast, the digital precoders are optimized by the\nshort-term DNN at each time slot based on the estimated low-dimensional\nequivalent CSI matrices. A two-timescale training method is also developed for\nthe proposed DNN with a binary layer. We then analyze the generalization\nability and signaling overhead for the proposed DNN based algorithm. Simulation\nresults show that our proposed technique significantly outperforms conventional\nschemes in terms of bit-error rate performance with reduced signaling overhead\nand shorter pilot sequences.']"
69,26,69_trust_human_robots_robot,"['trust', 'human', 'robots', 'robot', 'automation', 'interaction', 'social', 'avs', 'participants', 'autonomous']","[""Objective: We examine how human operators adjust their trust in automation as\na result of their moment-to-moment interaction with automation. Background:\nMost existing studies measured trust by administering questionnaires at the end\nof an experiment. Only a limited number of studies viewed trust as a dynamic\nvariable that can strengthen or decay over time. Method: Seventy-five\nparticipants took part in an aided memory recognition task. In the task,\nparticipants viewed a series of images and later on performed 40 trials of the\nrecognition task to identify a target image when it was presented with a\ndistractor. In each trial, participants performed the initial recognition by\nthemselves, received a recommendation from an automated decision aid, and\nperformed the final recognition. After each trial, participants reported their\ntrust on a visual analog scale. Results: Outcome bias and contrast effect\nsignificantly influence human operators' trust adjustments. An automation\nfailure leads to a larger trust decrement if the final outcome is undesirable,\nand a marginally larger trust decrement if the human operator succeeds the task\nby him-/her-self. An automation success engenders a greater trust increment if\nthe human operator fails the task. Additionally, automation failures have a\nlarger effect on trust adjustment than automation successes. Conclusion: Human\noperators adjust their trust in automation as a result of their\nmoment-to-moment interaction with automation. Their trust adjustments are\nsignificantly influenced by decision-making heuristics/biases. Application:\nUnderstanding the trust adjustment process enables accurate prediction of the\noperators' moment-to-moment trust in automation and informs the design of\ntrust-aware adaptive automation."", 'In recent years a modern conceptualization of trust in human-robot\ninteraction (HRI) was introduced by Ullman et al.\\cite{ullman2018does}. This\nnew conceptualization of trust suggested that trust between humans and robots\nis multidimensional, incorporating both performance aspects (i.e., similar to\nthe trust in human-automation interaction) and moral aspects (i.e., similar to\nthe trust in human-human interaction). But how does a robot violating each of\nthese different aspects of trust affect human trust in a robot? How does trust\nin robots change when a robot commits a moral-trust violation compared to a\nperformance-trust violation? And whether physiological signals have the\npotential to be used for assessing gain/loss of each of these two trust aspects\nin a human. We aim to design an experiment to study the effects of\nperformance-trust violation and moral-trust violation separately in a search\nand rescue task. We want to see whether two failures of a robot with equal\nmagnitudes would affect human trust differently if one failure is due to a\nperformance-trust violation and the other is a moral-trust violation.', 'In 2018 the European Commission highlighted the demand of a human-centered\napproach to AI. Such a claim is gaining even more relevance considering\ntechnologies specifically designed to directly interact and physically\ncollaborate with human users in the real world. This is notably the case of\nsocial robots. The domain of Human-Robot Interaction (HRI) emerged to\ninvestigate these issues. ""Human-robot trust"" has been highlighted as one of\nthe most challenging and intriguing factors influencing HRI. On the one hand,\nuser studies and technical experts underline how trust is a key element to\nfacilitate users\' acceptance, consequently increasing the chances to pursue the\ngiven task. On the other hand, such a phenomenon raises also ethical and\nphilosophical concerns leading scholars in these domains to argue that humans\nshould not trust robots. However, trust in HRI is not an index of fragility, it\nis rooted in anthropomorphism, and it is a natural characteristic of every\nhuman being. Thus, instead of focusing solely on how to inspire user trust in\nsocial robots, this paper argues that what should be investigated is to what\nextent and for which purpose it is suitable to trust robots. Such an endeavour\nrequires an interdisciplinary approach taking into account (i) technical needs\nand (ii) psychological implications.']"
70,25,70_metasurface_metasurfaces_band_dielectric,"['metasurface', 'metasurfaces', 'band', 'dielectric', 'wave', 'antenna', 'absorption', 'metamaterial', 'medium', 'filter']","['A fast metasurface optimization strategy for finite-size metasurfaces modeled\nusing integral equations is presented. The metasurfaces considered are\nconstructed from finite patterned metallic claddings supported by grounded\ndielectric spacers. Integral equations are used to model the response of the\nmetasurface to a known excitation and solved by Method of Moments. An\naccelerated gradient descent optimization algorithm is presented that enables\nthe direct optimization of such metasurfaces. The gradient is normally\ncalculated by solving the method of moments problem N+1 times where N is the\nnumber of homogenized elements in the metasurface. Since the calculation of\neach component of the N-dimensional gradient involves perturbing the moment\nmethod impedance matrix along one element of its diagonal and inverting the\nresult, this numerical gradient calculation can be accelerated using the\nWoodbury Matrix Identity. The Woodbury Matrix Identity allows the inverse of\nthe perturbed impedance matrix to be computed at a low cost by forming a rank-r\ncorrection to the inverse of the unperturbed impedance matrix. Timing diagrams\nshow up to a 26.5 times improvement in algorithm times when the acceleration\ntechnique is applied. An example of a passive and lossless wide-angle\nreflecting metasurface designed using the accelerated optimization technique is\nreported.', 'In recent years, new functionality and unprecedented wavefront control has\nbeen enabled by the introduction of bianisotropic metasurfaces. A bianisotropic\nmetasurface is characterized by an electric response, a magnetic response, and\nan electromagnetic/magnetoelectric response. In general, these metasur-faces\nconsists of an array of metallic or dielectric particles located within a\nsubwavelength thick host medium, and are approximated and modelled as\ninfinitely-thin, idealized sheet boundaries defined along a surface. An\nappropriate sheet boundary condition which effectively models the tangential\nfield discontinuity due to the array of magnetoelectric inclusions is the\nGeneralized Sheet Transition Condition or GSTC. Several forms of the GSTC\nappear in literature. Here, we present each interpretation and show how they\nare related. Synthesis approaches unique to each form are overviewed. By\nutilizing the GSTC in metasurface design, new possibilities emerge which are\nnot possible with conventional design techniques incorporating only electric or\nonly magnetic responses. Since the metasurfaces are designed using\nbianisotropic boundary conditions, they must be realized using particles which\ncontain magnetoelectric responses. This review article discusses the design of\nmetasurfaces using the GSTC, and the bianisotropic particles used to realize\nGSTCs. Further, it discusses new and recent applications that have emerged due\nto bianisotropy, and future prospects in metasurface design using bianisotropic\nboundary conditions. The intent is to provide a comprehensive overview of\nmetasurface design involving bianisotropy and for this review article to serve\nas a starting point for engineers and scientist that wish to introduce\nbianisotropy into metasurface design.', ""We present a systematic design method for a cylindrical conformal array of\nrectangular waveguide-fed metasurfaces. The conformal metasurface consists of\nmultiple curved rectangular waveguides loaded with metamaterial\nelements\\textemdash electrically small irises\\textemdash inserted into the\nupper conducting walls of the waveguides. Each element radiates energy into\nfree space to contribute to an overall radiation pattern. Thus, the geometry or\nelectrical configuration of each of the individual metamaterial elements needs\nto be tailored to generate a desired pattern. In general, due to difficulties\nin modeling the effect of curvature, the design of conformal metasurface arrays\nhas relied on full-wave simulations or experiments. In this study, we propose a\ndesign method utilizing the analytic model of a planar metasurface accounting\nfor metamaterial elements' locations and orientations over a surface with\ncurvature. Although approximate, we demonstrate that such an alteration along\nwith the framework of dipolar modeling of planar elements can be used for the\nanalysis of conformal arrays with small curvature. We then design a conformal\narray metasurface using the method combined with CMA-ES optimizer. Through\nnumerical simulations, we confirm the validity of the proposed design method.\nApplications include the design of metasurfaces for radar, communications, and\nimaging systems for automobiles and airplanes.""]"
71,25,71_domain_adaptation_source_target,"['domain', 'adaptation', 'source', 'target', 'uda', 'domains', 'alignment', 'feature', 'unsupervised', 'labeled']","['Deep learning (DL) has been the primary approach used in various computer\nvision tasks due to its relevant results achieved on many tasks. However, on\nreal-world scenarios with partially or no labeled data, DL methods are also\nprone to the well-known domain shift problem. Multi-source unsupervised domain\nadaptation (MSDA) aims at learning a predictor for an unlabeled domain by\nassigning weak knowledge from a bag of source models. However, most works\nconduct domain adaptation leveraging only the extracted features and reducing\ntheir domain shift from the perspective of loss function designs. In this\npaper, we argue that it is not sufficient to handle domain shift only based on\ndomain-level features, but it is also essential to align such information on\nthe feature space. Unlike previous works, we focus on the network design and\npropose to embed Multi-Source version of DomaIn Alignment Layers (MS-DIAL) at\ndifferent levels of the predictor. These layers are designed to match the\nfeature distributions between different domains and can be easily applied to\nvarious MSDA methods. To show the robustness of our approach, we conducted an\nextensive experimental evaluation considering two challenging scenarios: digit\nrecognition and object classification. The experimental results indicated that\nour approach can improve state-of-the-art MSDA methods, yielding relative gains\nof up to +30.64% on their classification accuracies.', 'Deep learning models usually require a large amount of labeled data to\nachieve satisfactory performance. In multimedia analysis, domain adaptation\nstudies the problem of cross-domain knowledge transfer from a label rich source\ndomain to a label scarce target domain, thus potentially alleviates the\nannotation requirement for deep learning models. However, we find that\ncontemporary domain adaptation methods for cross-domain image understanding\nperform poorly when source domain is noisy. Weakly Supervised Domain Adaptation\n(WSDA) studies the domain adaptation problem under the scenario where source\ndata can be noisy. Prior methods on WSDA remove noisy source data and align the\nmarginal distribution across domains without considering the fine-grained\nsemantic structure in the embedding space, which have the problem of class\nmisalignment, e.g., features of cats in the target domain might be mapped near\nfeatures of dogs in the source domain. In this paper, we propose a novel\nmethod, termed Noise Tolerant Domain Adaptation, for WSDA. Specifically, we\nadopt the cluster assumption and learn cluster discriminatively with class\nprototypes in the embedding space. We propose to leverage the location\ninformation of the data points in the embedding space and model the location\ninformation with a Gaussian mixture model to identify noisy source data. We\nthen design a network which incorporates the Gaussian mixture noise model as a\nsub-module for unsupervised noise removal and propose a novel cluster-level\nadversarial adaptation method which aligns unlabeled target data with the less\nnoisy class prototypes for mapping the semantic structure across domains. We\nconduct extensive experiments to evaluate the effectiveness of our method on\nboth general images and medical images from COVID-19 and e-commerce datasets.\nThe results show that our method significantly outperforms state-of-the-art\nWSDA methods.', 'Unsupervised domain adaptation (UDA) aims to transfer knowledge learned from\na labeled source domain to a different unlabeled target domain. Most existing\nUDA methods focus on learning domain-invariant feature representation, either\nfrom the domain level or category level, using convolution neural networks\n(CNNs)-based frameworks. One fundamental problem for the category level based\nUDA is the production of pseudo labels for samples in target domain, which are\nusually too noisy for accurate domain alignment, inevitably compromising the\nUDA performance. With the success of Transformer in various tasks, we find that\nthe cross-attention in Transformer is robust to the noisy input pairs for\nbetter feature alignment, thus in this paper Transformer is adopted for the\nchallenging UDA task. Specifically, to generate accurate input pairs, we design\na two-way center-aware labeling algorithm to produce pseudo labels for target\nsamples. Along with the pseudo labels, a weight-sharing triple-branch\ntransformer framework is proposed to apply self-attention and cross-attention\nfor source/target feature learning and source-target domain alignment,\nrespectively. Such design explicitly enforces the framework to learn\ndiscriminative domain-specific and domain-invariant representations\nsimultaneously. The proposed method is dubbed CDTrans (cross-domain\ntransformer), and it provides one of the first attempts to solve UDA tasks with\na pure transformer solution. Extensive experiments show that our proposed\nmethod achieves the best performance on Office-Home, VisDA-2017, and DomainNet\ndatasets.']"
72,25,72_software_quality_development_agile,"['software', 'quality', 'development', 'agile', 'requirements', 'devops', 'studies', 'engineering', 'developers', 'research']","['Human values (e.g., pleasure, privacy, and social justice) are what a person\nor a society considers important. Inability to address them in\nsoftware-intensive systems can result in numerous undesired consequences (e.g.,\nfinancial losses) for individuals and communities. Various solutions (e.g.,\nmethodologies, techniques) are developed to help ""operationalize values in\nsoftware"". The ultimate goal is to ensure building software (better) reflects\nand respects human values. In this survey, ""operationalizing values"" is\nreferred to as the process of identifying human values and translating them to\naccessible and concrete concepts so that they can be implemented, validated,\nverified, and measured in software. This paper provides a deep understanding of\nthe research landscape on operationalizing values in software engineering,\ncovering 51 primary studies. It also presents an analysis and taxonomy of 51\nsolutions for operationalizing values in software engineering. Our survey\nreveals that most solutions attempt to help operationalize values in the early\nphases (requirements and design) of the software development life cycle.\nHowever, the later phases (implementation and testing) and other aspects of\nsoftware development (e.g., ""team organization"") still need adequate\nconsideration. We outline implications for research and practice and identify\nopen issues and future research directions to advance this area.', ""In this article we focus on the structural aspects of the development of\nethical software, and argue that ethical considerations need to be embedded\ninto the (agile) software development process. In fact, we claim that agile\nprocesses of software development lend themselves specifically well for this\nendeavour. First, we contend that ethical evaluations need to go beyond the use\nof software products and include an evaluation of the software itself. This\nimplies that software engineers influence peoples' lives through the features\nof their designed products. Embedded values are thus approached best by\nsoftware engineers themselves. Therefore, we put emphasis on the possibility to\nimplement ethical deliberations in already existing and well established agile\nsoftware development processes. Our approach relies on software engineers\nmaking their own judgments throughout the entire development process to ensure\nthat technical features and ethical evaluation can be addressed adequately to\ntransport and foster desirable values and norms. We argue that agile software\ndevelopment processes may help the implementation of ethical deliberation for\nfive reasons: 1) agile methods are widely spread, 2) their emphasis on flat\nhierarchies promotes independent thinking, 3) their reliance on existing team\nstructures serve as an incubator for deliberation, 4) agile development\nenhances object-focused techno-ethical realism, and, finally, 5) agile\nstructures provide a salient endpoint to deliberation."", ""Requirements engineering process intends to obtain software services and\nconstraints. This process is essential to meet the customer's needs and\nexpectations. This process includes three main activities in general. These are\ndetecting requirements by interacting with software stakeholders, transferring\nthese requirements into a standard document, and examining that the\nrequirements really define the software that the client needs. Functional\nrequirements are services that the software should deliver to the end-user. In\naddition, functional requirements describe how the software should respond to\nspecific inputs, and how the software should behave in certain circumstances.\nThis paper aims to develop a software requirements specification document of\nthe electronic IT news magazine system. The electronic magazine provides users\nto post and view up-to-date IT news. Still, there is a lack in the literature\nof comprehensive studies about the construction of the electronic magazine\nsoftware specification and design in conformance with the contemporary software\ndevelopment processes. Moreover, there is a need for a suitable research\nframework to support the requirements engineering process. The novelty of this\npaper is the construction of software specification and design of the\nelectronic magazine by following the Al-Msie'deen research framework. All the\ndocuments of software requirements specification and design have been\nconstructed to conform to the agile usage-centered design technique and the\nproposed research framework. A requirements specification and design are\nsuggested and followed for the construction of the electronic magazine\nsoftware. This study proved that involving users extensively in the process of\nsoftware requirements specification and design will lead to the creation of\ndependable and acceptable software systems.""]"
73,24,73_stock_portfolio_financial_prices,"['stock', 'portfolio', 'financial', 'prices', 'market', 'return', 'series', 'historical', 'future', 'trading']","['Designing an optimum portfolio that allocates weights to its constituent\nstocks in a way that achieves the best trade-off between the return and the\nrisk is a challenging research problem. The classical mean-variance theory of\nportfolio proposed by Markowitz is found to perform sub-optimally on the\nreal-world stock market data since the error in estimation for the expected\nreturns adversely affects the performance of the portfolio. This paper presents\nthree approaches to portfolio design, viz, the minimum risk portfolio, the\noptimum risk portfolio, and the Eigen portfolio, for seven important sectors of\nthe Indian stock market. The daily historical prices of the stocks are scraped\nfrom Yahoo Finance website from January 1, 2016, to December 31, 2020. Three\nportfolios are built for each of the seven sectors chosen for this study, and\nthe portfolios are analyzed on the training data based on several metrics such\nas annualized return and risk, weights assigned to the constituent stocks, the\ncorrelation heatmaps, and the principal components of the Eigen portfolios.\nFinally, the optimum risk portfolios and the Eigen portfolios for all sectors\nare tested on their return over a period of a six-month period. The\nperformances of the portfolios are compared and the portfolio yielding the\nhigher return for each sector is identified.', 'The financial time series analysis is important access to touch the complex\nlaws of financial markets. Among many goals of the financial time series\nanalysis, one is to construct a model that can extract the information of the\nfuture return out of the known historical stock data, such as stock price,\nfinancial news, and e.t.c. To design such a model, prior knowledge on how the\nfuture return is correlated with the historical stock prices is needed. In this\nwork, we focus on the issue: in what mode the future return is correlated with\nthe historical stock prices. We manually design several financial time series\nwhere the future return is correlated with the historical stock prices in\npre-designed modes, namely the curve-shape-feature (CSF) and the\nnon-curve-shape-feature (NCSF) modes. In the CSF mode, the future return can be\nextracted from the curve shapes of the historical stock prices. By applying\nvarious kinds of existing algorithms on those pre-designed time series and real\nfinancial time series, we show that: (1) the major information of the future\nreturn is not contained in the curve-shape features of historical stock prices.\nThat is, the future return is not mainly correlated with the historical stock\nprices in the CSF mode. (2) Various kinds of existing machine learning\nalgorithms are good at extracting the curveshape features in the historical\nstock prices and thus are inappropriate for financial time series analysis\nalthough they are successful in the image recognition and natural language\nprocessing. That is, new models handling the NCSF series are needed in the\nfinancial time series analysis.', 'Predicting future stock prices and their movement patterns is a complex\nproblem. Hence, building a portfolio of capital assets using the predicted\nprices to achieve the optimization between its return and risk is an even more\ndifficult task. This work has carried out an analysis of the time series of the\nhistorical prices of the top five stocks from the nine different sectors of the\nIndian stock market from January 1, 2016, to December 31, 2020. Optimum\nportfolios are built for each of these sectors. For predicting future stock\nprices, a long-and-short-term memory (LSTM) model is also designed and\nfine-tuned. After five months of the portfolio construction, the actual and the\npredicted returns and risks of each portfolio are computed. The predicted and\nthe actual returns of each portfolio are found to be high, indicating the high\nprecision of the LSTM model.']"
74,24,74_iot_security_cyber_smart,"['iot', 'security', 'cyber', 'smart', 'cps', 'mud', 'things', 'internet', 'systems', 'vulnerabilities']","['Smart local energy system (SLES) is considered as a promising pathway\nfacilitating a more effective and localised operation, benefited from the\ncomplex information and communication technology (ICT) infrastructures and\nInternet of things (IoT) technologies. As a part of the critical\ninfrastructure, it is important to not only put effective detection and\nmanagement to tackle potential cybersecurity issues, but also require\nconsiderable numbers of standards to ensure the security of the internet of\nthings system to minimise the risks. This study aims to review the existing\nstandards, investigate how the compatibility with SLES development, and\nidentify the area to focus on in the future. Although existing standards and\nprotocols are highly fragmented, our findings suggest that many of them can\nmeet the requirements of the applications and infrastructures of SLES.\nAdditionally, many standards have been introduced to protect information\nsecurity and personal privacy due to their increasing importance. The research\nalso suggests that the industry needs to produce more affordable and\ncyber-secured devices and services. For the government and regulators, relevant\nguidelines on the minimum function and security requirements for applications\nshould be provided. Additionally, compliance testing and certifications should\nbe in place and carried out by an independent third party to ensure the\ncomponents of SLES ecosystem with a satisfied security level by design.', 'Threat modeling and risk assessments are common ways to identify, estimate,\nand prioritize risk to national, organizational, and individual operations and\nassets. Several threat modeling and risk assessment approaches have been\nproposed prior to the advent of the Internet of Things (IoT) that focus on\nthreats and risks in information technology (IT). Due to shortcomings in these\napproaches and the fact that there are significant differences between the IoT\nand IT, we synthesize and adapt these approaches to provide a threat modeling\nframework that focuses on threats and risks in the IoT. In doing so, we develop\nan IoT attack taxonomy that describes the adversarial assets, adversarial\nactions, exploitable vulnerabilities, and compromised properties that are\ncomponents of any IoT attack. We use this IoT attack taxonomy as the foundation\nfor designing a joint risk assessment and maturity assessment framework that is\nimplemented as an interactive online tool. The assessment framework this tool\nencodes provides organizations with specific recommendations about where\nresources should be devoted to mitigate risk. The usefulness of this IoT\nframework is highlighted by case study implementations in the context of\nmultiple industrial manufacturing companies, and the interactive implementation\nof this framework is available at http://iotrisk.andrew.cmu.edu.', 'The Internet of Things (IoT), the Industrial Internet of Things (IIoT), and\nCyber-Physical Systems (CPS) have become essential for our daily lives in\ncontexts such as our homes, buildings, cities, health, transportation,\nmanufacturing, infrastructure, and agriculture. However, they have become\npopular targets of attacks, due to their inherent limitations which create\nvulnerabilities. Honeypots and honeynets can prove essential to understand and\ndefend against attacks on IoT, IIoT, and CPS environments by attracting\nattackers and deceiving them into thinking that they have gained access to the\nreal systems. Honeypots and honeynets can complement other security solutions\n(i.e., firewalls, Intrusion Detection Systems - IDS) to form a strong defense\nagainst malicious entities. This paper provides a comprehensive survey of the\nresearch that has been carried out on honeypots and honeynets for IoT, IIoT,\nand CPS. It provides a taxonomy and extensive analysis of the existing\nhoneypots and honeynets, states key design factors for the state-of-the-art\nhoneypot/honeynet research and outlines open issues for future honeypots and\nhoneynets for IoT, IIoT, and CPS environments.']"
75,23,75_formal_systems_development_twin,"['formal', 'systems', 'development', 'twin', 'system', 'verification', 'industrial', 'supply', 'requirements', 'ias']","['Rigorous development processes aim to be effective in developing critical\nsystems, especially if failures can have catastrophic consequences for humans\nand the environment. Such processes generally rely on formal methods, which can\nguarantee, thanks to their mathematical foundation, model preciseness, and\nproperties assurance. However, they are rarely adopted in practice. In this\npaper, we report our experience in using the Abstract State Machine formal\nmethod and the ASMETA framework in developing a prototype of the control\nsoftware of the MVM (Mechanical Ventilator Milano), a mechanical lung\nventilator that has been designed, successfully certified, and deployed during\nthe COVID-19 pandemic. Due to time constraints and lack of skills, no formal\nmethod was applied for the MVM project. However, we here want to assess the\nfeasibility of developing (part of) the ventilator by using a formal\nmethod-based approach. Our development process starts from a high-level formal\nspecification of the system to describe the MVM main operation modes. Then,\nthrough a sequence of refined models, all the other requirements are captured,\nup to a level in which a C++ implementation of a prototype of the MVM\ncontroller is automatically generated from the model, and tested. Along the\nprocess, at each refinement level, different model validation and verification\nactivities are performed, and each refined model is proved to be a correct\nrefinement of the previous level. By means of the MVM case study, we evaluate\nthe effectiveness and usability of our formal approach.', 'The use of domain-specific modeling for development of complex\n(cyber-physical) systems is gaining increasing acceptance in the industrial\nenvironment. Domain-specific modeling allows complex systems and data to be\nabstracted for a more efficient system design, development, validation, and\nconfiguration. However, no existing (meta-)modeling framework can be used with\nreasonable effort in certified software so far, neither for the development of\nsystems nor for the execution of system functions. For the use of (development)\nartifacts from domain-specific modeling in safety-critical processes or systems\nit is required to ensure their correctness by either subsequent (manual)\nverification or the usage of (pre-)qualified software. Existing meta-languages\noften contain modeling elements that are difficult or impossible to implement\nin a qualifiable manner leading to a high manual, subsequent certification\neffort. Therefore, the aim is to develop a (meta-)modeling framework, that can\nbe used in certified software. This can significantly reduce the development\neffort for safety-critical systems and enables the full advantages of\ndomain-specific modeling. The framework components considered in this\nPhD-Thesis include: (1) an essential meta-language, (2) a qualifiable runtime\nenvironment, and (3) a suitable persistence. The essential\n\\mbox{(meta-)}modeling language is mainly based on the UML standard, but is\nenhanced with multi-level modeling concepts such as deep instantiation.\nSupporting a possible qualification, the meta-language is implemented using the\nhighly restrictive, but formally provable programming language Ada SPARK.', 'Industrial cyber-physical systems require complex distributed software to\norchestrate many heterogeneous mechatronic components and control multiple\nphysical processes. Industrial automation software is typically developed in a\nmodel-driven fashion where abstractions of physical processes called plant\nmodels are co-developed and iteratively refined along with the control code.\nTesting such multi-dimensional systems is extremely difficult because often\nmodels might not be accurate, do not correspond accurately with subsequent\nrefinements, and the software must eventually be tested on the real plant,\nespecially in safety-critical systems like nuclear plants. This paper proposes\na framework wherein high-level functional requirements are used to\nautomatically generate test cases for designs at all abstraction levels in the\nmodel-driven engineering process. Requirements are initially specified in\nnatural language and then analyzed and specified using a formalized ontology.\nThe requirements ontology is then refined along with controller and plant\nmodels during design and development stages such that test cases can be\ngenerated automatically at any stage. A representative industrial water process\nsystem case study illustrates the strengths of the proposed formalism. The\nrequirements meta-model proposed by the CESAR European project is used for\nrequirements engineering while IEC 61131-3 and model-driven concepts are used\nin the design and development phases. A tool resulting from the proposed\nframework called REBATE (Requirements Based Automatic Testing Engine) is used\nto generate and execute test cases for increasingly concrete controller and\nplant models.']"
76,23,76_navigation_planning_robot_path,"['navigation', 'planning', 'robot', 'path', 'robots', 'cef', 'learning', 'environments', 'environment', 'planner']","[""This study presents a new methodology for learning-based motion planning for\nautonomous exploration using aerial robots. Through the reinforcement learning\nmethod of learning through trial and error, the action policy is derived that\ncan guide autonomous exploration of underground and tunnel environments. A new\nMarkov decision process state is designed to learn the robot's action policy by\nusing simulation only, and the results are applied to the real-world\nenvironment without further learning. Reduce the need for the precision map in\ngrid-based path planner and achieve map-less navigation. The proposed method\ncan have a path with less computing cost than the grid-based planner but has\nsimilar performance. The trained action policy is broadly evaluated in both\nsimulation and field trials related to autonomous exploration of underground\nmines or indoor spaces."", 'In this paper, we present a novel path planning algorithm to achieve fast\npath planning in complex environments. Most existing path planning algorithms\nare difficult to quickly find a feasible path in complex environments or even\nfail. However, our proposed framework can overcome this difficulty by using a\nlearning-based prediction module and a sampling-based path planning module. The\nprediction module utilizes an auto-encoder-decoder-like convolutional neural\nnetwork (CNN) to output a promising region where the feasible path probably\nlies in. In this process, the environment is treated as an RGB image to feed in\nour designed CNN module, and the output is also an RGB image. No extra\ncomputation is required so that we can maintain a high processing speed of 60\nframes-per-second (FPS). Incorporated with a sampling-based path planner, we\ncan extract a feasible path from the output image so that the robot can track\nit from start to goal. To demonstrate the advantage of the proposed algorithm,\nwe compare it with conventional path planning algorithms in a series of\nsimulation experiments. The results reveal that the proposed algorithm can\nachieve much better performance in terms of planning time, success rate, and\npath length.', 'Robots moving safely and in a socially compliant manner in dynamic human\nenvironments is an essential benchmark for long-term robot autonomy. However,\nit is not feasible to learn and benchmark social navigation behaviors entirely\nin the real world, as learning is data-intensive, and it is challenging to make\nsafety guarantees during training. Therefore, simulation-based benchmarks that\nprovide abstractions for social navigation are required. A framework for these\nbenchmarks would need to support a wide variety of learning approaches, be\nextensible to the broad range of social navigation scenarios, and abstract away\nthe perception problem to focus on social navigation explicitly. While there\nhave been many proposed solutions, including high fidelity 3D simulators and\ngrid world approximations, no existing solution satisfies all of the\naforementioned properties for learning and evaluating social navigation\nbehaviors. In this work, we propose SOCIALGYM, a lightweight 2D simulation\nenvironment for robot social navigation designed with extensibility in mind,\nand a benchmark scenario built on SOCIALGYM. Further, we present benchmark\nresults that compare and contrast human-engineered and model-based learning\napproaches to a suite of off-the-shelf Learning from Demonstration (LfD) and\nReinforcement Learning (RL) approaches applied to social robot navigation.\nThese results demonstrate the data efficiency, task performance, social\ncompliance, and environment transfer capabilities for each of the policies\nevaluated to provide a solid grounding for future social navigation research.']"
77,23,77_defect_fault_deep_cnn,"['defect', 'fault', 'deep', 'cnn', 'damage', 'vibration', 'weld', 'feature', 'features', 'frictional']","['Vibration-based techniques are among the most common condition monitoring\napproaches. With the advancement of computers, these approaches have also been\nimproved such that recently, these approaches in conjunction with deep learning\nmethods attract attention among researchers. This is mostly due to the nature\nof the deep learning method that could facilitate the monitoring procedure by\nintegrating the feature extraction, feature selection, and classification steps\ninto one automated step. However, this can be achieved at the expense of\nchallenges in designing the architecture of a deep learner, tuning its\nhyper-parameters. Moreover, it sometimes gives low generalization capability.\nAs a remedy to these problems, this study proposes a framework based on\nensemble deep learning methodology. The framework was initiated by creating a\npool of Convolutional neural networks (CNN). To create diversity to the CNNs,\nthey are fed by frequency responses which are passed through different\nfunctions. As the next step, proper CNNs are selected based on an information\ncriterion to be used for fusion. The fusion is then carried out by improved\nDempster-Shafer theory. The proposed framework is applied to real test data\ncollected from Equiax Polycrystalline Nickel alloy first-stage turbine blades\nwith complex geometry.', 'Surface defect detection plays an increasingly important role in\nmanufacturing industry to guarantee the product quality. Many deep learning\nmethods have been widely used in surface defect detection tasks, and have been\nproven to perform well in defects classification and location. However, deep\nlearning-based detection methods often require plenty of data for training,\nwhich fail to apply to the real industrial scenarios since the distribution of\ndefect categories is often imbalanced. In other words, common defect classes\nhave many samples but rare defect classes have extremely few samples, and it is\ndifficult for these methods to well detect rare defect classes. To solve the\nimbalanced distribution problem, in this paper we propose TL-SDD: a novel\nTransfer Learning-based method for Surface Defect Detection. First, we adopt a\ntwo-phase training scheme to transfer the knowledge from common defect classes\nto rare defect classes. Second, we propose a novel Metric-based Surface Defect\nDetection (M-SDD) model. We design three modules for this model: (1) feature\nextraction module: containing feature fusion which combines high-level semantic\ninformation with low-level structural information. (2) feature reweighting\nmodule: transforming examples to a reweighting vector that indicates the\nimportance of features. (3) distance metric module: learning a metric space in\nwhich defects are classified by computing distances to representations of each\ncategory. Finally, we validate the performance of our proposed method on a real\ndataset including surface defects of aluminum profiles. Compared to the\nbaseline methods, the performance of our proposed method has improved by up to\n11.98% for rare defect classes.', 'Nowadays, using vibration data in conjunction with pattern recognition\nmethods is one of the most common fault detection strategies for structures.\nHowever, their performances depend on the features extracted from vibration\ndata, the features selected to train the classifier, and the classifier used\nfor pattern recognition. Deep learning facilitates the fault detection\nprocedure by automating the feature extraction and selection, and\nclassification procedure. Though, deep learning approaches have challenges in\ndesigning its structure and tuning its hyperparameters, which may result in a\nlow generalization capability. Therefore, this study proposes an ensemble deep\nlearning framework based on a convolutional neural network (CNN) and\nDempster-Shafer theory (DST), called CNN-DST. In this framework, several CNNs\nwith the proposed structure are first trained, and then, the outputs of the\nCNNs selected by the proposed technique are combined by using an improved\nDST-based method. To validate the proposed CNN-DST framework, it is applied to\nan experimental dataset created by the broadband vibrational responses of\npolycrystalline Nickel alloy first-stage turbine blades with different types\nand severities of damage. Through statistical analysis, it is shown that the\nproposed CNN-DST framework classifies the turbine blades with an average\nprediction accuracy of 97.19%. The proposed CNN-DST framework is benchmarked\nwith other state-of-the-art classification methods, demonstrating its high\nperformance. The robustness of the proposed CNN-DST framework with respect to\nmeasurement noise is investigated, showing its high noise-resistance. Further,\nbandwidth analysis reveals that most of the required information for detecting\nfaulty samples is available in a small frequency range.']"
78,22,78_tensor_rank_mathbf_matrix,"['tensor', 'rank', 'mathbf', 'matrix', 'low', 'recovery', 'subspace', 'signal', 'sparse', 'dictionary']","['Recently, the low-rank property of different components extracted from the\nimage has been considered in man hyperspectral image denoising methods.\nHowever, these methods usually unfold the 3D tensor to 2D matrix or 1D vector\nto exploit the prior information, such as nonlocal spatial self-similarity\n(NSS) and global spectral correlation (GSC), which break the intrinsic\nstructure correlation of hyperspectral image (HSI) and thus lead to poor\nrestoration quality. In addition, most of them suffer from heavy computational\nburden issues due to the involvement of singular value decomposition operation\non matrix and tensor in the original high-dimensionality space of HSI. We\nemploy subspace representation and the weighted low-rank tensor regularization\n(SWLRTR) into the model to remove the mixed noise in the hyperspectral image.\nSpecifically, to employ the GSC among spectral bands, the noisy HSI is\nprojected into a low-dimensional subspace which simplified calculation. After\nthat, a weighted low-rank tensor regularization term is introduced to\ncharacterize the priors in the reduced image subspace. Moreover, we design an\nalgorithm based on alternating minimization to solve the nonconvex problem.\nExperiments on simulated and real datasets demonstrate that the SWLRTR method\nperforms better than other hyperspectral denoising methods quantitatively and\nvisually.', ""Low-rank matrix models have been universally useful for numerous applications\nstarting from classical system identification to more modern matrix completion\nin signal processing and statistics. The nuclear norm has been employed as a\nconvex surrogate of the low-rankness since it induces a low-rank solution to\ninverse problems. While the nuclear norm for low-rankness has a nice analogy\nwith the $\\ell_1$ norm for sparsity through the singular value decomposition,\nother matrix norms also induce low-rankness. Particularly as one interprets a\nmatrix as a linear operator between Banach spaces, various tensor product norms\ngeneralize the role of the nuclear norm. We provide a tensor-norm-constrained\nestimator for recovery of approximately low-rank matrices from local\nmeasurements corrupted with noise. A tensor-norm regulizer is designed adapting\nto the local structure. We derive statistical analysis of the estimator over\nmatrix completion and decentralized sketching through applying Maurey's\nempirical method to tensor products of Banach spaces. The estimator provides a\nnear optimal error bound in a minimax sense and admits a polynomial-time\nalgorithm for these applications."", 'Low-rank tensor decomposition generalizes low-rank matrix approximation and\nis a powerful technique for discovering low-dimensional structure in\nhigh-dimensional data. In this paper, we study Tucker decompositions and use\ntools from randomized numerical linear algebra called ridge leverage scores to\naccelerate the core tensor update step in the widely-used alternating least\nsquares (ALS) algorithm. Updating the core tensor, a severe bottleneck in ALS,\nis a highly-structured ridge regression problem where the design matrix is a\nKronecker product of the factor matrices. We show how to use approximate ridge\nleverage scores to construct a sketched instance for any ridge regression\nproblem such that the solution vector for the sketched problem is a\n$(1+\\varepsilon)$-approximation to the original instance. Moreover, we show\nthat classical leverage scores suffice as an approximation, which then allows\nus to exploit the Kronecker structure and update the core tensor in time that\ndepends predominantly on the rank and the sketching parameters (i.e., sublinear\nin the size of the input tensor). We also give upper bounds for ridge leverage\nscores as rows are removed from the design matrix (e.g., if the tensor has\nmissing entries), and we demonstrate the effectiveness of our approximate ridge\nregressioni algorithm for large, low-rank Tucker decompositions on both\nsynthetic and real-world data.']"
79,22,79_shot_few_fsl_meta,"['shot', 'few', 'fsl', 'meta', 'learning', 'class', 'classes', 'feature', 'classification', 'novel']","['Standard few-shot relation classification (RC) is designed to learn a robust\nclassifier with only few labeled data for each class. However, previous works\nrarely investigate the effects of a different number of classes (i.e., $N$-way)\nand number of labeled data per class (i.e., $K$-shot) during training vs.\ntesting. In this work, we define a new task, \\textit{inconsistent few-shot RC},\nwhere the model needs to handle the inconsistency of $N$ and $K$ between\ntraining and testing. To address this new task, we propose Prototype\nNetwork-based cross-attention contrastive learning (ProtoCACL) to capture the\nrich mutual interactions between the support set and query set. Experimental\nresults demonstrate that our ProtoCACL can outperform the state-of-the-art\nbaseline model under both inconsistent $K$ and inconsistent $N$ settings, owing\nto its more robust and discriminate representations. Moreover, we identify that\nin the inconsistent few-shot learning setting, models can achieve better\nperformance with \\textit{less data} than the standard few-shot setting with\ncarefully-selected $N$ and $K$. In the end of the paper, we provide further\nanalyses and suggestions to systematically guide the selection of $N$ and $K$\nunder different scenarios.', 'The existing few-shot video classification methods often employ a\nmeta-learning paradigm by designing customized temporal alignment module for\nsimilarity calculation. While significant progress has been made, these methods\nfail to focus on learning effective representations, and heavily rely on the\nImageNet pre-training, which might be unreasonable for the few-shot recognition\nsetting due to semantics overlap. In this paper, we aim to present an in-depth\nstudy on few-shot video classification by making three contributions. First, we\nperform a consistent comparative study on the existing metric-based methods to\nfigure out their limitations in representation learning. Accordingly, we\npropose a simple classifier-based baseline without any temporal alignment that\nsurprisingly outperforms the state-of-the-art meta-learning based methods.\nSecond, we discover that there is a high correlation between the novel action\nclass and the ImageNet object class, which is problematic in the few-shot\nrecognition setting. Our results show that the performance of training from\nscratch drops significantly, which implies that the existing benchmarks cannot\nprovide enough base data. Finally, we present a new benchmark with more base\ndata to facilitate future few-shot video classification without pre-training.\nThe code will be made available at https://github.com/MCG-NJU/FSL-Video.', 'Few-shot learning aims to adapt knowledge learned from previous tasks to\nnovel tasks with only a limited amount of labeled data. Research literature on\nfew-shot learning exhibits great diversity, while different algorithms often\nexcel at different few-shot learning scenarios. It is therefore tricky to\ndecide which learning strategies to use under different task conditions.\nInspired by the recent success in Automated Machine Learning literature\n(AutoML), in this paper, we present Meta Navigator, a framework that attempts\nto solve the aforementioned limitation in few-shot learning by seeking a\nhigher-level strategy and proffer to automate the selection from various\nfew-shot learning designs. The goal of our work is to search for good parameter\nadaptation policies that are applied to different stages in the network for\nfew-shot classification. We present a search space that covers many popular\nfew-shot learning algorithms in the literature and develop a differentiable\nsearching and decoding algorithm based on meta-learning that supports\ngradient-based optimization. We demonstrate the effectiveness of our\nsearching-based method on multiple benchmark datasets. Extensive experiments\nshow that our approach significantly outperforms baselines and demonstrates\nperformance advantages over many state-of-the-art methods. Code and models will\nbe made publicly available.']"
80,22,80_network_resource_drl_reinforcement,"['network', 'resource', 'drl', 'reinforcement', 'allocation', 'learning', 'slices', 'onslicing', 'marl', 'slice']","[""Future Internet involves several emerging technologies such as 5G and beyond\n5G networks, vehicular networks, unmanned aerial vehicle (UAV) networks, and\nInternet of Things (IoTs). Moreover, future Internet becomes heterogeneous and\ndecentralized with a large number of involved network entities. Each entity may\nneed to make its local decision to improve the network performance under\ndynamic and uncertain network environments. Standard learning algorithms such\nas single-agent Reinforcement Learning (RL) or Deep Reinforcement Learning\n(DRL) have been recently used to enable each network entity as an agent to\nlearn an optimal decision-making policy adaptively through interacting with the\nunknown environments. However, such an algorithm fails to model the\ncooperations or competitions among network entities, and simply treats other\nentities as a part of the environment that may result in the non-stationarity\nissue. Multi-agent Reinforcement Learning (MARL) allows each network entity to\nlearn its optimal policy by observing not only the environments, but also other\nentities' policies. As a result, MARL can significantly improve the learning\nefficiency of the network entities, and it has been recently used to solve\nvarious issues in the emerging networks. In this paper, we thus review the\napplications of MARL in the emerging networks. In particular, we provide a\ntutorial of MARL and a comprehensive survey of applications of MARL in next\ngeneration Internet. In particular, we first introduce single-agent RL and\nMARL. Then, we review a number of applications of MARL to solve emerging issues\nin future Internet. The issues consist of network access, transmit power\ncontrol, computation offloading, content caching, packet routing, trajectory\ndesign for UAV-aided networks, and network security issues."", 'Network slicing is a promising technology that allows mobile network\noperators to efficiently serve various emerging use cases in 5G. It is\nchallenging to optimize the utilization of network infrastructures while\nguaranteeing the performance of network slices according to service level\nagreements (SLAs). To solve this problem, we propose SafeSlicing that\nintroduces a new constraint-aware deep reinforcement learning (CaDRL) algorithm\nto learn the optimal resource orchestration policy within two steps, i.e.,\noffline training in a simulated environment and online learning with the real\nnetwork system. On optimizing the resource orchestration, we incorporate the\nconstraints on the statistical performance of slices in the reward function\nusing Lagrangian multipliers, and solve the Lagrangian relaxed problem via a\npolicy network. To satisfy the constraints on the system capacity, we design a\nconstraint network to map the latent actions generated from the policy network\nto the orchestration actions such that the total resources allocated to network\nslices do not exceed the system capacity. We prototype SafeSlicing on an\nend-to-end testbed developed by using OpenAirInterface LTE, OpenDayLight-based\nSDN, and CUDA GPU computing platform. The experimental results show that\nSafeSlicing reduces more than 20% resource usage while meeting SLAs of network\nslices as compared with other solutions.', ""Network slicing allows mobile network operators to virtualize infrastructures\nand provide customized slices for supporting various use cases with\nheterogeneous requirements. Online deep reinforcement learning (DRL) has shown\npromising potential in solving network problems and eliminating the\nsimulation-to-reality discrepancy. Optimizing cross-domain resources with\nonline DRL is, however, challenging, as the random exploration of DRL violates\nthe service level agreement (SLA) of slices and resource constraints of\ninfrastructures. In this paper, we propose OnSlicing, an online end-to-end\nnetwork slicing system, to achieve minimal resource usage while satisfying\nslices' SLA. OnSlicing allows individualized learning for each slice and\nmaintains its SLA by using a novel constraint-aware policy update method and\nproactive baseline switching mechanism. OnSlicing complies with resource\nconstraints of infrastructures by using a unique design of action modification\nin slices and parameter coordination in infrastructures. OnSlicing further\nmitigates the poor performance of online learning during the early learning\nstage by offline imitating a rule-based solution. Besides, we design four new\ndomain managers to enable dynamic resource configuration in radio access,\ntransport, core, and edge networks, respectively, at a timescale of subseconds.\nWe implement OnSlicing on an end-to-end slicing testbed designed based on\nOpenAirInterface with both 4G LTE and 5G NR, OpenDayLight SDN platform, and\nOpenAir-CN core network. The experimental results show that OnSlicing achieves\n61.3% usage reduction as compared to the rule-based solution and maintains\nnearly zero violation (0.06%) throughout the online learning phase. As online\nlearning is converged, OnSlicing reduces 12.5% usage without any violations as\ncompared to the state-of-the-art online DRL solution.""]"
81,22,81_uav_uavs_aerial_unmanned,"['uav', 'uavs', 'aerial', 'unmanned', 'trajectory', 'communication', 'ground', 'algorithm', 'system', 'drl']","['The usage of unmanned aerial vehicles (UAVs) in civil and military\napplications continues to increase due to the numerous advantages that they\nprovide over conventional approaches. Despite the abundance of such advantages,\nit is imperative to investigate the performance of UAV utilization while\nconsidering their design limitations. This paper investigates the deployment of\nUAV swarms when each UAV carries a machine learning classification task. To\navoid data exchange with ground-based processing nodes, a federated learning\napproach is adopted between a UAV leader and the swarm members to improve the\nlocal learning model while avoiding excessive air-to-ground and ground-to-air\ncommunications. Moreover, the proposed deployment framework considers the\nstringent energy constraints of UAVs and the problem of class imbalance, where\nwe show that considering these design parameters significantly improves the\nperformances of the UAV swarm in terms of classification accuracy, energy\nconsumption and availability of UAVs when compared with several baseline\nalgorithms.', ""In this paper, we investigate a multi-user downlink multiple-input\nsingle-output (MISO) unmanned aerial vehicle (UAV) communication system, where\na multi-antenna UAV is employed to serve multiple ground terminals. Unlike\nexisting approaches focus only on a simplified two-dimensional scenario, this\npaper considers a three-dimensional (3D) urban environment, where the UAV's 3D\ntrajectory is designed to minimize data transmission completion time subject to\npractical throughput and flight movement constraints. Specifically, we propose\na deep reinforcement learning (DRL)-based trajectory design for completion time\nminimization (DRL-TDCTM), which is developed from a deep deterministic policy\ngradient algorithm. In particular, to represent the state information of UAV\nand environment, we set an additional information, i.e., the merged pheromone,\nas a reference of reward which facilitates the algorithm design. By interacting\nwith the external environment in the corresponding Markov decision process, the\nproposed algorithm can continuously and adaptively learn how to adjust the\nUAV's movement strategy. Finally, simulation results show the superiority of\nthe proposed DRL-TDCTM algorithm over the conventional baseline methods."", 'This paper studies the UAV-enabled integrated sensing and communication\n(ISAC), in which UAVs are dispatched as aerial dual-functional access points\n(APs) for efficient ISAC. In particular, we consider a scenario with one UAV-AP\nequipped with a vertically placed uniform linear array (ULA), which sends\ncombined information and sensing signals to communicate with multiple users and\nsense potential targets at interested areas on the ground simultaneously. Our\nobjective is to jointly design the UAV maneuver with the transmit beamforming\nfor optimizing the communication performance while ensuring the sensing\nrequirements. First, we consider the quasi-stationary UAV scenario, in which\nthe UAV is deployed at an optimizable location over the whole ISAC mission\nperiod. In this case, we jointly optimize the UAV deployment location, as well\nas the transmit information and sensing beamforming to maximize the weighted\nsum-rate throughput, subject to the sensing beampattern gain requirements and\ntransmit power constraint. Although the above problem is non-convex, we find a\nhigh-quality solution by using the techniques of SCA and SDR, together with a\n2D location search. Next, we consider the fully mobile UAV scenario, in which\nthe UAV can fly over different locations during the ISAC mission period. In\nthis case, we optimize the UAV flight trajectory, jointly with the transmit\nbeamforming over time, to maximize the average weighted sum-rate throughput,\nsubject to the sensing beampattern gain requirements and transmit power\nconstraints as well as practical flight constraints. While the joint UAV\ntrajectory and beamforming problem is more challenging to solve, we propose an\nefficient algorithm by adopting the alternating optimization together with SCA.\nFinally, numerical results are provided to validate the superiority of our\nproposed designs as compared to various benchmark schemes.']"
82,21,82_radar_communication_sensing_mimo,"['radar', 'communication', 'sensing', 'mimo', 'waveform', 'isac', 'dfrc', 'frequency', 'transmit', 'power']","['In this work we consider a multiple-input multiple-output (MIMO)\ndual-function radar-communication (DFRC) system that employs an orthogonal\nfrequency division multiplexing (OFDM) and a differential phase shift keying\n(DPSK) modulation, and study the design of the radiated waveforms and of the\nreceive filters employed by the radar and the users. The approach is\ncommunication-centric, in the sense that a radar-oriented objective is\noptimized under constraints on the average transmit power, the power leakage\ntowards specific directions, and the error rate of each user, thus safeguarding\nthe communication quality of service (QoS). We adopt a unified design approach\nallowing a broad family of radar objectives, including both estimation- and\ndetection-oriented merit functions. We devise a suboptimal solution based on\nalternating optimization of the involved variables, a convex restriction of the\nfeasible search set, and minorization-maximization, offering a single algorithm\nfor all of the radar merit functions in the considered family. Finally, the\nperformance is inspected through numerical examples.', 'Large-scale deployment of connected vehicles with cooperative awareness\ntechnologies increases the demand for vehicle-to-everything (V2X) communication\nspectrum in 5.9 GHz that is mainly allocated for the exchange of safety\nmessages. To supplement V2X communication and support the high data rates\nneeded by broadband applications, the millimeter-wave (mmWave) automotive radar\nspectrum at 76-81 GHz can be utilized. For this purpose, joint\nradar-communication systems have been proposed in the literature to perform\nboth functions using the same waveform and hardware. While multiple-input and\nmultiple-output (MIMO) communication with multiple users enables independent\ndata streaming for high throughput, MIMO radar processing provides\nhigh-resolution imaging that is crucial for safety-critical systems. However,\nemploying conventional precoding methods designed for communication generates\ndirectional beams that impair MIMO radar imaging and target tracking\ncapabilities during data streaming. In this paper, we propose a MIMO joint\nautomotive radar-communication (JARC) framework based on orthogonal frequency\ndivision multiplexing (OFDM) waveform. First, we show that the MIMO-OFDM\npreamble can be exploited for both MIMO radar processing and estimation of the\ncommunication channel. Then, we propose an optimal precoder design method that\nenables high accuracy target tracking while transmitting independent data\nstreams to multiple receivers. The proposed methods provide high-resolution\nradar imaging and high throughput capabilities for MIMO JARC networks. Finally,\nwe evaluate the efficacy of the proposed methods through numerical simulations.', 'In this paper, we propose a novel integrated sensing and communication (ISAC)\ntransmission framework based on the spatially-spread orthogonal time frequency\nspace (SS-OTFS) modulation by considering the fact that communication channel\nstrengths cannot be directly obtained from radar sensing. We first propose the\nconcept of SS-OTFS modulation, where the key novelty is the angular domain\ndiscretization enabled by the spatial-spreading/de-spreading. This\ndiscretization gives rise to simple and insightful effective models for both\nradar sensing and communication, which result in simplified designs for the\nrelated estimation and detection problems. In particular, we design simple beam\ntracking, angle estimation, and power allocation schemes for radar sensing, by\nutilizing the special structure of the effective radar sensing matrix.\nMeanwhile, we provide a detailed analysis on the pair-wise error probability\n(PEP) for communication, which unveils the key conditions for both precoding\nand power allocation designs. Based on those conditions, we design a\nsymbol-wise precoding scheme for communication based only on the delay,\nDoppler, and angle estimates from radar sensing, without the a priori knowledge\nof the communication channel fading coefficients, and also introduce the power\nallocation for communication. Furthermore, we notice that radar sensing and\ncommunication requires different power allocations. Therefore, we discuss the\nperformances of both the radar sensing and communication with different power\nallocations and show that the power allocation should be designed leaning\ntowards radar sensing in practical scenarios. The effectiveness of the proposed\nISAC transmission framework is verified by our numerical results, which also\nagree with our analysis and discussions.']"
83,21,83_diffractive_optical_imaging_phase,"['diffractive', 'optical', 'imaging', 'phase', 'light', 'diffraction', 'auditory', 'tissue', 'optically', 'nand']","['Owing to its potential advantages such as scalability, low latency and power\nefficiency, optical computing has seen rapid advances over the last decades. A\ncore unit of a potential all-optical processor would be the NAND gate, which\ncan be cascaded to perform an arbitrary logical operation. Here, we present the\ndesign and analysis of cascadable all-optical NAND gates using diffractive\nneural networks. We encoded the logical values at the input and output planes\nof a diffractive NAND gate using the relative optical power of two\nspatially-separated apertures. Based on this architecture, we numerically\noptimized the design of a diffractive neural network composed of 4 passive\nlayers to all-optically perform NAND operation using the diffraction of light,\nand cascaded these diffractive NAND gates to perform complex logical functions\nby successively feeding the output of one diffractive NAND gate into another.\nWe demonstrated the cascadability of our diffractive NAND gates by using\nidentical diffractive designs to all-optically perform AND and OR operations,\nas well as a half-adder. Cascadable all-optical NAND gates composed of\nspatially-engineered passive diffractive layers can serve as a core component\nof various optical computing platforms.', 'Diffractive optical networks unify wave optics and deep learning to\nall-optically compute a given machine learning or computational imaging task as\nthe light propagates from the input to the output plane. Here, we report the\ndesign of diffractive optical networks for the classification and\nreconstruction of spatially overlapping, phase-encoded objects. When two\ndifferent phase-only objects spatially overlap, the individual object functions\nare perturbed since their phase patterns are summed up. The retrieval of the\nunderlying phase images from solely the overlapping phase distribution presents\na challenging problem, the solution of which is generally not unique. We show\nthat through a task-specific training process, passive diffractive networks\ncomposed of successive transmissive layers can all-optically and simultaneously\nclassify two different randomly-selected, spatially overlapping phase images at\nthe input. After trained with ~550 million unique combinations of phase-encoded\nhandwritten digits from the MNIST dataset, our blind testing results reveal\nthat the diffractive network achieves an accuracy of >85.8% for all-optical\nclassification of two overlapping phase images of new handwritten digits. In\naddition to all-optical classification of overlapping phase objects, we also\ndemonstrate the reconstruction of these phase images based on a shallow\nelectronic neural network that uses the highly compressed output of the\ndiffractive network as its input (with e.g., ~20-65 times less number of\npixels) to rapidly reconstruct both of the phase images, despite their spatial\noverlap and related phase ambiguity. The presented phase image classification\nand reconstruction framework might find applications in e.g., computational\nimaging, microscopy and quantitative phase imaging fields.', 'We report the design of diffractive surfaces to all-optically perform\narbitrary complex-valued linear transformations between an input (N_i) and\noutput (N_o), where N_i and N_o represent the number of pixels at the input and\noutput fields-of-view (FOVs), respectively. First, we consider a single\ndiffractive surface and use a matrix pseudoinverse-based method to determine\nthe complex-valued transmission coefficients of the diffractive\nfeatures/neurons to all-optically perform a desired/target linear\ntransformation. In addition to this data-free design approach, we also consider\na deep learning-based design method to optimize the transmission coefficients\nof diffractive surfaces by using examples of input/output fields corresponding\nto the target transformation. We compared the all-optical transformation errors\nand diffraction efficiencies achieved using data-free designs as well as\ndata-driven (deep learning-based) diffractive designs to all-optically perform\n(i) arbitrarily-chosen complex-valued transformations including unitary,\nnonunitary and noninvertible transforms, (ii) 2D discrete Fourier\ntransformation, (iii) arbitrary 2D permutation operations, and (iv) high-pass\nfiltered coherent imaging. Our analyses reveal that if the total number (N) of\nspatially-engineered diffractive features/neurons is N_i x N_o or larger, both\ndesign methods succeed in all-optical implementation of the target\ntransformation, achieving negligible error. However, compared to data-free\ndesigns, deep learning-based diffractive designs are found to achieve\nsignificantly larger diffraction efficiencies for a given N and their\nall-optical transformations are more accurate for N < N_i x N_o. These\nconclusions are generally applicable to various optical processors that employ\nspatially-engineered diffractive surfaces.']"
84,20,84_transportation_traffic_passengers_timetable,"['transportation', 'traffic', 'passengers', 'timetable', 'public', 'ride', 'demand', 'bus', 'mobility', 'riders']","[""Bus timetable optimization is a key issue to reduce operational cost of bus\ncompanies and improve the service quality. Existing methods use exact or\nheuristic algorithms to optimize the timetable in an offline manner. In\npractice, the passenger flow may change significantly over time. Timetables\ndetermined in offline cannot adjust the departure interval to satisfy the\nchanged passenger flow. Aiming at improving the online performance of bus\ntimetable, we propose a Deep Reinforcement Learning based bus Timetable dynamic\nOptimization method (DRL-TO). In this method, the timetable optimization is\nconsidered as a sequential decision problem. A Deep Q-Network (DQN) is employed\nas the decision model to determine whether to dispatch a bus service during\neach minute of the service period. Therefore, the departure intervals of bus\nservices are determined in real time in accordance with passenger demand. We\nidentify several new and useful state features for the DQN, including the load\nfactor, carrying capacity utilization rate, and the number of stranding\npassengers. Taking into account both the interests of the bus company and\npassengers, a reward function is designed, which includes the indicators of\nfull load rate, empty load rate, passengers' waiting time, and the number of\nstranding passengers. Building on an existing method for calculating the\ncarrying capacity, we develop a new technique to enhance the matching degree at\neach bus station. Experiments demonstrate that compared with the timetable\ngenerated by the state-of-the-art bus timetable optimization approach based on\na memetic algorithm (BTOA-MA), Genetic Algorithm (GA) and the manual method,\nDRL-TO can dynamically determine the departure intervals based on the real-time\npassenger flow, saving 8$\\%$ of vehicles and reducing 17$\\%$ of passengers'\nwaiting time on average."", 'Rideshare is one way to share mobility in transportation without increasing\ntraffic demand, where travel mobility and usage of vehicle capacity can be\nimproved. However, current literature on rideshare has allowed only one-modal\ntrips and may be limited in the matching efficiency, especially when there is a\nlarge gap between the supply and demand of mobility. Therefore, the objectives\nof this paper are first to develop a multi-modal matching framework of shared\nmobility with public transportation to maximize the performance of a rideshare\nsystem, and second to evaluate the effect of the public transportation and of\nthe schedule flexibility on the matching efficiency. To fulfill the first\nobjective, a multi-modal matching framework is developed to allow rideshare\nwith both private and public vehicles with detailed design of detour, using\nGenetic Algorithm. Also for the second objective, the effects of public\ntransportation and schedule flexibility are evaluated with a simplified network\nof Sioux Falls. The results show that public transportation helps the match\nrate slightly at a low supply of private vehicle, but this must be evaluated\nfor practical implementation as different cities may bring different results.\nAlso, a larger schedule flexibility helps greatly in increasing match rate even\nat a lower supply level. As well, the planning subject of time schedule is\nbenefited more with larger schedule flexibility, in this paper the drivers, on\nthe matching efficiency. Moreover, a rideshare system with private vehicles\noutperforms a public transportation system, possibly due to the rigid route of\npublic transportation that takes no detour burden. This confirms the need for a\nflexible design of sharing mobility, as can be fulfilled with the multi-modal\nmatching framework developed in this research.', 'Cities worldwide struggle with overloaded transportation systems and their\nexternalities, such as traffic congestion and emissions. The emerging\nautonomous transportation technology has a potential to alleviate these issues.\nYet, the decisions of profit-maximizing operators running large autonomous\nfleets could have a negative impact on other stakeholders, e.g., by\ndisproportionately cannibalizing public transport, and therefore could make the\ntransportation system even less efficient and sustainable. A careful analysis\nof these tradeoffs requires modeling the main modes of transportation,\nincluding public transport, within a unified framework. In this paper, we\npropose such a framework, which allows us to study the interplay among mobility\nservice providers, public transport authorities, and customers. Our framework\ncombines a graph-theoretic network model for the transportation system with a\ngame-theoretic market model in which mobility service providers are\nprofit-maximizers, while customers select individually-optimal transportation\noptions. We apply our framework to data for the city of Berlin, Germany, and\npresent sensitivity analyses to study parameters that mobility service\nproviders or municipalities can influence to steer the system. We show that\ndepending on market conditions and policy restrictions, autonomous ride-hailing\nsystems may complement or cannibalize a public transportation system, serving\nbetween 7% and 80% of all customers. We discuss the main factors behind\ndifferences in these outcomes as well as strategic design options available to\npolicymakers. Among others, we show that the monopolistic and the competitive\ncases yield similar modal shares, but differ in the profit outcome of each\nmobility service provider.']"
85,19,85_kv_stores_data_write,"['kv', 'stores', 'data', 'write', 'rme', 'rdma', 'transactional', 'store', 'read', 'recoverable']","['Deterministic database systems have received increasing attention from the\ndatabase research community in recent years. Despite their current limitations,\nrecent proposals of distributed deterministic transaction processing systems\ndemonstrated significant improvements over systems using traditional\ntransaction processing techniques (e.g., two-phase-locking or optimistic\nconcurrency control with two-phase-commit). However, the problem of ensuring\nhigh availability in deterministic distributed transaction processing systems\nhas received less attention from the research community, and this aspect has\nnot been analyzed and evaluated well. This paper proposes a generic framework\nto model the replication process in deterministic transaction processing\nsystems and use it to study three cases. We design and implement QR-Store, a\nqueue-oriented replicated transaction processing system, and extensively\nevaluate it with various workloads based on a transactional version of YCSB.\nOur prototype implementation QR-Store can achieve a throughput of 1.9 million\nreplicated transactions per second in under 200 milliseconds and a replication\noverhead of 8%-25% compared to non-replicated configurations.', 'The log-structured merge tree (LSM-tree) gains wide popularity in building\nkey-value (KV) stores. It employs logs to back up arriving KV pairs and\nmaintains a few on-disk levels with exponentially increasing capacity limits,\nresembling a tiered tree-like structure. A level comprises SST files, each of\nwhich holds a sequence of sorted KV pairs. From time to time, LSM-tree\nredeploys KV pairs from a full level to the lower level by compaction, which\nmerge-sorts and moves KV pairs among SST files, thereby incurring substantial\ndisk I/Os.\n  In this paper, we revisit the design of LSM-tree and find that organizing\nmultiple KV pairs in an SST file entails the heavyweight redeployment of actual\nKV pairs in a compaction. Accordingly we revolutionize the organization of KV\npairs by transforming an SST file of KV pairs to an SST directory, in which\neach KV pair makes into an independent KV file with the key and value as\nfilename and main file contents, respectively. Moving KV pairs in a compaction\nconverts to transferring directory entries (dentrys), which causes concretely\nfewer disk I/Os. This is the essence of our design named DeLSM. We build a\nprototype of DeLSM on LevelDB and evaluation results show that it significantly\noutperforms the state-of-the-art LSM-tree variants in different dimensions.', 'Log-Structured Merge tree (LSM tree) Key-Value (KV) stores have become a\nfoundational layer in the storage stacks of datacenter and cloud services.\nCurrent approaches for achieving reliability and availability avoid replication\nat the KV store level and instead perform these operations at higher layers,\ne.g., the DB layer that runs on top of the KV store. The main reason is that\npast designs for replicated KV stores favor reducing network traffic and\nincreasing I/O size. Therefore, they perform costly compactions to reorganize\ndata in both the primary and backup nodes, which hurts overall system\nperformance.\n  In this paper, we design and implement Talos, an efficient rack-scale\nLSM-based KV store that aims to significantly reduce the I/O amplification and\nCPU overhead in backup nodes and make replication in the KV store practical. We\nrely on two observations: (a) the increased use of RDMA in the datacenter,\nwhich reduces CPU overhead for communication, and (b) the use of KV separation\nthat is becoming prevalent in modern KV stores. We use a primary-backup\nreplication scheme that performs compactions only on the primary nodes and\nsends the pre-built index to the backup nodes of the region, avoiding all\ncompactions in backups. Our approach includes an efficient mechanism to deal\nwith pointer translation across nodes in the region index. Our results show\nthat Talos reduces in the backup nodes, I/O amplification by up to $3\\times$,\nCPU overhead by up to $1.6\\times$, and memory size needed for the write path by\nup to $2\\times$, without increasing network bandwidth excessively, and by up to\n$1.3\\times$. Overall, we show that our approach has benefits even when small KV\npairs dominate in a workload (80%-90%). Finally, it enables KV stores to\noperate with larger growth factors (from 10 to 16) to reduce space\namplification without sacrificing precious CPU cycles.']"
86,18,86_gans_gan_data_generative,"['gans', 'gan', 'data', 'generative', 'discriminator', 'cgans', 'series', 'generator', 'adversarial', 'imputation']","['Solving the convergence issues of Generative Adversarial Networks (GANs) is\none of the most outstanding problems in generative models. In this work, we\npropose a novel activation function to be used as output of the generator\nagent. This activation function is based on the Smirnov probabilistic\ntransformation and it is specifically designed to improve the quality of the\ngenerated data. In sharp contrast with previous works, our activation function\nprovides a more general approach that deals not only with the replication of\ncategorical variables but with any type of data distribution (continuous or\ndiscrete). Moreover, our activation function is derivable and therefore, it can\nbe seamlessly integrated in the backpropagation computations during the GAN\ntraining processes. To validate this approach, we evaluate our proposal against\ntwo different data sets: a) an artificially rendered data set containing a\nmixture of discrete and continuous variables, and b) a real data set of\nflow-based network traffic data containing both normal connections and\ncryptomining attacks. To evaluate the fidelity of the generated data, we\nanalyze both their results in terms of quality measures of statistical nature\nand also regarding the use of these synthetic data to feed a nested machine\nlearning-based classifier. The experimental results evince a clear\noutperformance of the GAN network tuned with this new activation function with\nrespect to both a na\\""ive mean-based generator and a standard GAN. The quality\nof the data is so high that the generated data can fully substitute real data\nfor training the nested classifier without a fall in the obtained accuracy.\nThis result encourages the use of GANs to produce high-quality synthetic data\nthat are applicable in scenarios in which data privacy must be guaranteed.', 'Generative adversarial networks (GANs) studies have grown exponentially in\nthe past few years. Their impact has been seen mainly in the computer vision\nfield with realistic image and video manipulation, especially generation,\nmaking significant advancements. While these computer vision advances have\ngarnered much attention, GAN applications have diversified across disciplines\nsuch as time series and sequence generation. As a relatively new niche for\nGANs, fieldwork is ongoing to develop high quality, diverse and private time\nseries data. In this paper, we review GAN variants designed for time series\nrelated applications. We propose a taxonomy of discrete-variant GANs and\ncontinuous-variant GANs, in which GANs deal with discrete time series and\ncontinuous time series data. Here we showcase the latest and most popular\nliterature in this field; their architectures, results, and applications. We\nalso provide a list of the most popular evaluation metrics and their\nsuitability across applications. Also presented is a discussion of privacy\nmeasures for these GANs and further protections and directions for dealing with\nsensitive data. We aim to frame clearly and concisely the latest and\nstate-of-the-art research in this area and their applications to real-world\ntechnologies.', ""We look into Generative Adversarial Network (GAN), its prevalent variants and\napplications in a number of sectors. GANs combine two neural networks that\ncompete against one another using zero-sum game theory, allowing them to create\nmuch crisper and discrete outputs. GANs can be used to perform image\nprocessing, video generation and prediction, among other computer vision\napplications. GANs can also be utilised for a variety of science-related\nactivities, including protein engineering, astronomical data processing, remote\nsensing image dehazing, and crystal structure synthesis. Other notable fields\nwhere GANs have made gains include finance, marketing, fashion design, sports,\nand music. Therefore in this article we provide a comprehensive overview of the\napplications of GANs in a wide variety of disciplines. We first cover the\ntheory supporting GAN, GAN variants, and the metrics to evaluate GANs. Then we\npresent how GAN and its variants can be applied in twelve domains, ranging from\nSTEM fields, such as astronomy and biology, to business fields, such as\nmarketing and finance, and to arts, such as music. As a result, researchers\nfrom other fields may grasp how GANs work and apply them to their own study. To\nthe best of our knowledge, this article provides the most comprehensive survey\nof GAN's applications in different fields.""]"
87,18,87_neutrino_detector_xenon_experiment,"['neutrino', 'detector', 'xenon', 'experiment', 'beta', '163', 'ho', 'liquid', 'ge', 'reactor']","['The EXO-200 experiment searched for neutrinoless double-beta decay of\n$^{136}$Xe with a single-phase liquid xenon detector. It used an active mass of\n110 kg of 80.6%-enriched liquid xenon in an ultra-low background time\nprojection chamber with ionization and scintillation detection and readout.\nThis paper describes the design and performance of the various support systems\nnecessary for detector operation, including cryogenics, xenon handling, and\ncontrols. Novel features of the system were driven by the need to protect the\nthin-walled detector chamber containing the liquid xenon, to achieve high\nchemical purity of the Xe, and to maintain thermal uniformity across the\ndetector.', 'The Deep Underground Neutrino Experiment (DUNE) will be the next generation\nlong-baseline neutrino experiment. The far detector is designed as a complex of\nfour LAr-TPC (Liquid Argon Time Projection Chamber) modules with 17 kt of\nliquid argon each. The development and validation of the first far detector\ntechnology is pursued through ProtoDUNE Single Phase (ProtoDUNE-SP), a 770 t\nLAr-TPC at CERN Neutrino Platform. Crucial in DUNE is the Photon Detection\nSystem that will ensure the trigger of non-beam events - proton decay,\nsupernova neutrino burst and BSM searches - and will improve the timing and\ncalorimetry for neutrino beam events. Doping liquid argon with xenon is a known\ntechnique to shift the light emitted by argon (128 nm) to a longer wavelength\n(178 nm) to ease its detection. The largest xenon doping test ever performed in\na LAr-TPC was carried out in ProtoDUNE-SP. From February to May 2020, a\ngradually increasing amount of xenon was injected to also compensate for the\nlight loss due to air contamination. The response of such a large TPC has been\nstudied using the ProtoDUNE-SP Photon Detection System (PDS) and a dedicated\nsetup installed before the run. With the first it was possible to study the\nlight collection efficiency with respect to the track position, while with the\nsecond it was possible to distinguish the xenon light from the LAr light. The\nlight shifting mechanism proved to be highly efficient even at small xenon\nconcentrations (< 20 ppm in mass) furthermore it allowed recovering the light\nquenched by pollutants. The light collection improved far from the detection\nplane, enhancing the photon detector response uniformity along the drift\ndirection and confirming a longer Rayleigh scattering length for 178 nm\nphotons, with respect to 128 nm ones. The charge collection by the TPC was\nmonitored proving that xenon up to 20 ppm does not impact its performance.', 'The ESS Neutrino Super-Beam (ESSnuSB) is a proposed long-baseline neutrino\noscillation experiment, performed with a high-intensity neutrino beam, to be\ndeveloped as an extension to the European Spallation Source proton linac\ncurrently under construction in Lund, Sweden. The neutrinos would be detected\nwith the near and far detectors of the experiment, the former within several\nhundred meters of the neutrino production point and the latter within several\nhundred kilometers. The far detector will consist of a megaton-scale\nwater-Cherenkov detector, and the near detector will consist of a kiloton-scale\nwater-Cherenkov detector in combination with a fine-grained tracking detector\nand an emulsion detector. The purpose of the near detector is to constrain the\nflux of the neutrino beam as well as to extract the electron-neutrino\ninteraction cross-section in water, which requires high-performance energy\nreconstruction and particle flavor identification techniques. These\nmeasurements are crucial for the neutrino oscillation measurements that will be\nconducted using the far detector.\n  Year 2021 sees the finalization of the conceptual design of the near detector\nafter a thorough evaluation of the performance of a number of different design\noptions, and a characterization of the neutrino reconstruction and flavor\nidentification performances. In this talk we report on thesestudies.']"
88,18,88_kernel_design_rbf_reliability,"['kernel', 'design', 'rbf', 'reliability', 'rbdo', 'method', 'retrospective', 'surrogate', 'function', 'pueblo']","['In many fields of science and engineering, models with different fidelities\nare available. Physical experiments or detailed simulations that accurately\ncapture the behavior of the system are regarded as high-fidelity models with\nlow model uncertainty, however, they are expensive to run. On the other hand,\nsimplified physical experiments or numerical models are seen as low-fidelity\nmodels that are cheaper to evaluate. Although low-fidelity models are often not\nsuitable for direct use in reliability analysis due to their low accuracy, they\ncan offer information about the trend of the high-fidelity model thus providing\nthe opportunity to explore the design space at a low cost. This study presents\na new approach called adaptive multi-fidelity Gaussian process for reliability\nanalysis (AMGPRA). Contrary to selecting training points and information\nsources in two separate stages as done in state-of-the-art mfEGRA method, the\nproposed approach finds the optimal training point and information source\nsimultaneously using the novel collective learning function (CLF). CLF is able\nto assess the global impact of a candidate training point from an information\nsource and it accommodates any learning function that satisfies a certain\nprofile. In this context, CLF provides a new direction for quantifying the\nimpact of new training points and can be easily extended with new learning\nfunctions to adapt to different reliability problems. The performance of the\nproposed method is demonstrated by three mathematical examples and one\nengineering problem concerning the wind reliability of transmission towers. It\nis shown that the proposed method achieves similar or higher accuracy with\nreduced computational costs compared to state-of-the-art single and\nmulti-fidelity methods. A key application of AMGPRA is high-fidelity fragility\nmodeling using complex and costly physics-based computational models.', 'Statistical flood frequency analysis coupled with hydrograph scaling is\ncommonly used to generate design floods to assess dam safety assessment. The\nsafety assessments can be highly sensitive to the choice of the statistical\nflood frequency model. Standard dam safety assessments are typically based on a\nsingle distribution model of flood frequency, often the Log Pearson Type III or\nGeneralized Extreme Value distributions. Floods, however, may result from\nmultiple physical processes such as rain on snow, snowmelt or rainstorms. This\ncan result in a mixed distribution of annual peak flows, according to the cause\nof each flood. Engineering design choices based on a single distribution\nstatistical model are vulnerable to the effects of this potential structural\nmodel error. To explore the practicality and potential value of implementing\nmixed distribution statistical models in engineering design, we compare the\ngoodness of fit of several single- and mixed-distribution peak flow models, as\nwell as the contingent dam safety assessment at Pueblo, Colorado as a didactic\nexample. Summer snowmelt and intense summer rainstorms are both key drivers of\nannual peak flow at Pueblo. We analyze the potential implications for the\nannual probability of overtopping-induced failure of the Pueblo Dam as a\ndidactic example. We address the temporal and physical cause separation\nproblems by building on previous work with mixed distributions. We find a Mixed\nGeneralized Extreme Value distribution model best fits peak flows observed in\nthe gaged record, historical floods, and paleo floods at Pueblo. Finally, we\nshow that accounting for mixed distributions in the safety assessment at Pueblo\nDam increases the assessed risk of overtopping.', 'Some response surface functions in complex engineering systems are usually\nhighly nonlinear, unformed, and expensive-to-evaluate. To tackle this\nchallenge, Bayesian optimization, which conducts sequential design via a\nposterior distribution over the objective function, is a critical method used\nto find the global optimum of black-box functions. Kernel functions play an\nimportant role in shaping the posterior distribution of the estimated function.\nThe widely used kernel function, e.g., radial basis function (RBF), is very\nvulnerable and susceptible to outliers; the existence of outliers is causing\nits Gaussian process surrogate model to be sporadic. In this paper, we propose\na robust kernel function, Asymmetric Elastic Net Radial Basis Function\n(AEN-RBF). Its validity as a kernel function and computational complexity are\nevaluated. When compared to the baseline RBF kernel, we prove theoretically\nthat AEN-RBF can realize smaller mean squared prediction error under mild\nconditions. The proposed AEN-RBF kernel function can also realize faster\nconvergence to the global optimum. We also show that the AEN-RBF kernel\nfunction is less sensitive to outliers, and hence improves the robustness of\nthe corresponding Bayesian optimization with Gaussian processes. Through\nextensive evaluations carried out on synthetic and real-world optimization\nproblems, we show that AEN-RBF outperforms existing benchmark kernel functions.']"
89,18,89_spin_qubits_magnetic_qubit,"['spin', 'qubits', 'magnetic', 'qubit', 'superconducting', 'quantum', 'device', 'junction', 'donor', 'soi']","['Quantum computation and simulation requires strong coherent coupling between\nqubits, which may be spatially separated. Achieving this coupling for\nsolid-state based spin qubits is a long-standing challenge. Here we\ntheoretically investigate a method for achieving such coupling, based on\nsuperconducting nano-structures designed to channel the magnetic flux created\nby the qubits. We detail semi-classical analytical calculations and simulations\nof the magnetic field created by a magnetic dipole, depicting the spin qubit,\npositioned directly below nanofabricated apertures in a superconducting layer.\nWe show that such structures could channel the magnetic flux, enhancing the\ndipole-dipole interaction between spin qubits and changing its scaling with\ndistance, thus potentially paving the way for controllably engineering an\ninteracting spin system.', 'Semiconductor spin qubits may show significant device-to-device variability\nin the presence of spin-orbit coupling mechanisms. Interface roughness, charge\ntraps, layout or process inhomogeneities indeed shape the real space wave\nfunctions, hence the spin properties. It is, therefore, important to understand\nhow reproducible the qubits can be, in order to assess strategies to cope with\nvariability, and to set constraints on the quality of materials and\nfabrication. Here we model the variability of single qubit properties (Larmor\nand Rabi frequencies) due to disorder at the Si/SiO$_2$ interface (roughness,\ncharge traps) in metal-oxide-semiconductor devices. We consider both electron\nqubits (with synthetic spin-orbit coupling fields created by micro-magnets) and\nhole qubits (with intrinsic spin-orbit coupling). We show that charge traps are\nmuch more limiting than interface roughness, and can scatter Rabi frequencies\nover one order of magnitude. We discuss the implications for the design of spin\nqubits and for the choice of materials.', 'Hole spin qubits in quasi one-dimensional structures are a promising platform\nfor quantum information processing because of the strong spin-orbit interaction\n(SOI). We present analytical results and discuss device designs that optimize\nthe SOI in Ge semiconductors. We show that at the magnetic field values at\nwhich qubits are operated, orbital effects of magnetic fields can strongly\naffect the response of the spin qubit. We study one-dimensional hole systems in\nGe under the influence of electric and magnetic fields applied perpendicularly\nto the device. In our theoretical description, we include these effects\nexactly. The orbital effects lead to a strong renormalization of the g-factor.\nWe find a sweet-spot of the nanowire (NW) g-factor where charge noise is\nstrongly suppressed and present an effective low-energy model that captures the\ndependence of the SOI on the electromagnetic fields. Moreover, we compare\nproperties of NWs with square and circular cross-sections with ones of\ngate-defined one-dimensional channels in two-dimensional Ge heterostructures.\nInterestingly, the effective model predicts a flat band ground state for\nfine-tuned electric and magnetic fields. By considering a quantum dot (QD)\nharmonically confined by gates, we demonstrate that the NW g-factor sweet spot\nis retained in the QD. Our calculations reveal that this sweet spot can be\ndesigned to coincide with the maximum of the SOI, yielding highly coherent\nqubits with large Rabi frequencies. We also study the effective g-factor of NWs\ngrown along different high symmetry axes and find that our model derived for\nisotropic semiconductors is valid for the most relevant growth directions of\nnon-isotropic Ge NWs. Moreover, a NW grown along one of the three main\ncrystallographic axes shows the largest SOI. Our results show that the\nisotropic approximation is not justified in Ge in all cases.']"
90,18,90_robot_walking_humanoid_position,"['robot', 'walking', 'humanoid', 'position', 'locomotion', 'control', 'legged', 'leg', 'tracking', 'foot']","[""The objective of this work is to design a mechatronic bipedal robot with\nmobility in 3D environments. The designed robot has a total of six actuated\ndegrees of freedom (DoF), each leg has two DoF located at the hip: one for\nabduction/adduction and another for thigh flexion/extension, and a third DoF at\nthe knee for the shin flexion/extension. This robot is designed with point-feet\nlegs to achieve a dynamic underactuated walking. Each actuator in the robot\nincludes a DC gear motor, an encoder for position measurement, a flexible joint\nto form a series flexible actuator, and a feedback controller to ensure\ntrajectory tracking. In order to reduce the total mass of the robot, the shin\nis designed using topology optimization. The resulting design is fabricated\nusing 3D printed parts, which allows to get a robot's prototype to validate the\nselection of actuators. The preliminary experiments confirm the robot's ability\nto maintain a stand-up position and let us drawn future works in dynamic\ncontrol and trajectory generation for periodic stable walking."", 'A safety-critical measure of legged locomotion performance is a robot\'s\nability to track its desired time-varying position trajectory in an\nenvironment, which is herein termed as ""global-position tracking"". This paper\nintroduces a nonlinear control approach that achieves asymptotic\nglobal-position tracking for three-dimensional (3-D) bipedal robot walking.\nDesigning a global-position tracking controller presents a challenging problem\ndue to the complex hybrid robot model and the time-varying desired\nglobal-position trajectory. Towards tackling this problem, the first main\ncontribution is the construction of impact invariance to ensure all desired\ntrajectories respect the foot-landing impact dynamics, which is a necessary\ncondition for realizing asymptotic tracking of hybrid walking systems. Thanks\nto their independence of the desired global position, these conditions can be\nexploited to decouple the higher-level planning of the global position and the\nlower-level planning of the remaining trajectories, thereby greatly alleviating\nthe computational burden of motion planning. The second main contribution is\nthe Lyapunov-based stability analysis of the hybrid closed-loop system, which\nproduces sufficient conditions to guide the controller design for achieving\nasymptotic global-position tracking during fully actuated walking. Simulations\nand experiments on a 3-D bipedal robot with twenty revolute joints confirm the\nvalidity of the proposed control approach in guaranteeing accurate tracking.', ""The task of self-balancing is one of the most important tasks when developing\nhumanoid robots. This paper proposes a novel external balance mechanism for\nhumanoid robot to maintain sideway balance. First, a dynamic model of the\nhumanoid robot with balance mechanism and its simplified model are introduced.\nSecondly, a backstepping-based control method is utilized to split the system\ninto two sub-systems. Then, a minimum observer-based controller is used to\ncontrol the first sub-system. Since the second sub-system has unknown\nparameters, a model reference adaptive controller (MRAC) is used to control it.\nThe proposed design divides the walking and balancing into two separated tasks,\nallowing the walking control can be executed independently of the balancing\ncontrol. Furthermore, the use of the balance mechanism ensures the humanoid\nrobot's hip movement does not exceed the threshold of a human when walking.\nThus, making the overall pose of the humanoid robot looks more natural. An\nexperiment is carried out on a commercial humanoid robot known as UXA-90 to\nevaluate the effectiveness of the proposed method.""]"
91,18,91_reconstruction_mri_images_image,"['reconstruction', 'mri', 'images', 'image', 'contrast', 'regularization', 'network', 'mr', 'cs', 'imaging']","['Goal: This work aims at developing a novel calibration-free fast parallel MRI\n(pMRI) reconstruction method incorporate with discrete-time optimal control\nframework. The reconstruction model is designed to learn a regularization that\ncombines channels and extracts features by leveraging the information sharing\namong channels of multi-coil images. We propose to recover both magnitude and\nphase information by taking advantage of structured multiplayer convolutional\nnetworks in image and Fourier spaces. Methods: We develop a novel variational\nmodel with a learnable objective function that integrates an adaptive\nmulti-coil image combination operator and effective image regularization in the\nimage and Fourier spaces. We cast the reconstruction network as a structured\ndiscrete-time optimal control system, resulting in an optimal control\nformulation of parameter training where the parameters of the objective\nfunction play the role of control variables. We demonstrate that the Lagrangian\nmethod for solving the control problem is equivalent to back-propagation,\nensuring the local convergence of the training algorithm. Results: We conduct a\nlarge number of numerical experiments of the proposed method with comparisons\nto several state-of-the-art pMRI reconstruction networks on real pMRI datasets.\nThe numerical results demonstrate the promising performance of the proposed\nmethod evidently. Conclusion: The proposed method provides a general deep\nnetwork design and training framework for efficient joint-channel pMRI\nreconstruction. Significance: By learning multi-coil image combination operator\nand performing regularizations in both image domain and k-space domain, the\nproposed method achieves a highly efficient image reconstruction network for\npMRI.', 'In clinical practice, magnetic resonance imaging (MRI) with multiple\ncontrasts is usually acquired in a single study to assess different properties\nof the same region of interest in human body. The whole acquisition process can\nbe accelerated by having one or more modalities under-sampled in the $k$-space.\nRecent researches demonstrate that, considering the redundancy between\ndifferent contrasts or modalities, a target MRI modality under-sampled in the\n$k$-space can be more efficiently reconstructed with a fully-sampled MRI\ncontrast as the reference modality. However, we find that the performance of\nthe above multi-modal reconstruction can be negatively affected by subtle\nspatial misalignment between different contrasts, which is actually common in\nclinical practice. In this paper, to compensate for such spatial misalignment,\nwe integrate the spatial alignment network with multi-modal reconstruction\ntowards better reconstruction quality of the target modality. First, the\nspatial alignment network estimates the spatial misalignment between the\nfully-sampled reference and the under-sampled target images, and warps the\nreference image accordingly. Then, the aligned fully-sampled reference image\njoins the multi-modal reconstruction of the under-sampled target image. Also,\nconsidering the contrast difference between the target and the reference\nimages, we particularly design the cross-modality-synthesis-based registration\nloss, in combination with the reconstruction loss, to jointly train the spatial\nalignment network and the reconstruction network. Experiments on both clinical\nMRI and multi-coil $k$-space raw data demonstrate the superiority and\nrobustness of multi-modal MRI reconstruction empowered with our spatial\nalignment network. Our code is publicly available at\n\\url{https://github.com/woxuankai/SpatialAlignmentNetwork}.', ""Image reconstruction from undersampled k-space data plays an important role\nin accelerating the acquisition of MR data, and a lot of deep learning-based\nmethods have been exploited recently. Despite the achieved inspiring results,\nthe optimization of these methods commonly relies on the fully-sampled\nreference data, which are time-consuming and difficult to collect. To address\nthis issue, we propose a novel self-supervised learning method. Specifically,\nduring model optimization, two subsets are constructed by randomly selecting\npart of k-space data from the undersampled data and then fed into two parallel\nreconstruction networks to perform information recovery. Two reconstruction\nlosses are defined on all the scanned data points to enhance the network's\ncapability of recovering the frequency information. Meanwhile, to constrain the\nlearned unscanned data points of the network, a difference loss is designed to\nenforce consistency between the two parallel networks. In this way, the\nreconstruction model can be properly trained with only the undersampled data.\nDuring the model evaluation, the undersampled data are treated as the inputs\nand either of the two trained networks is expected to reconstruct the\nhigh-quality results. The proposed method is flexible and can be employed in\nany existing deep learning-based method. The effectiveness of the method is\nevaluated on an open brain MRI dataset. Experimental results demonstrate that\nthe proposed self-supervised method can achieve competitive reconstruction\nperformance compared to the corresponding supervised learning method at high\nacceleration rates (4 and 8). The code is publicly available at\n\\url{https://github.com/chenhu96/Self-Supervised-MRI-Reconstruction}.""]"
92,18,92_alloys_mg_deformation_alloy,"['alloys', 'mg', 'deformation', 'alloy', 'corrosion', 'phase', 'tribological', 'mechanical', 'dislocation', 'strength']","['High-entropy alloys (HEAs), and even medium-entropy alloys (MEAs), are an\nintriguing class of materials in that structure and property relations can be\ncontrolled via alloying and chemical disorder over wide ranges in the\ncomposition space. Employing density-functional theory combined with the\ncoherent-potential approximation to average over all chemical configurations,\nwe tune free energies between face-centered-cubic (fcc) and\nhexagonal-close-packed (hcp) phases in Fe$_{x}$Mn$_{80-x}$Co$_{10}$Cr$_{10}$\nsystems.~Within Fe-Mn-based alloys, we show that the martensitic transformation\nand chemical short-range order directly correlate with the fcc-hcp energy\ndifference and stacking-fault energies, which are in quantitative agreement\nwith recent experiments on a $x$=40~at.\\% polycrystalline HEA/MEA. Our\npredictions are further confirmed by single-crystal measurements on\na$x$=40at.\\% using transmission-electron microscopy, selective-area\ndiffraction, and electron-backscattered-diffraction mapping. The results herein\noffer an understanding of transformation-induced/twinning-induced plasticity\n(TRIP/TWIP) in this class of HEAs and a design guide for controlling the\nphysics behind the TRIP effect at the electronic level.', 'The microstructure of Mg-Al-Ca alloys consists of a hard intra- and\nintergranular eutectic Laves phase network embedded in a soft $\\alpha$-Mg\nmatrix. For such heterogeneous microstructures, the mechanical response and\nco-deformation of both phases under external load are not yet fully understood.\nWe therefore used nano- and microindentation in combination with electron\nmicroscopy to study the deformation behaviour of an Mg-3.68Al-3.8Ca alloy.\n  We found that the hardness of the Mg$_2$Ca phase was significantly larger\nthan the $\\alpha$-Mg phase and stays constant within the measured temperature\nrange. The strain rate sensitivity of the softer $\\alpha$-Mg phase and of the\ninterfaces increased while activation volume decreased with temperature. The\ncreep deformation of the Mg$_2$Ca Laves phase was significantly lower than the\n$\\alpha$-Mg phase at 170 $^{\\circ}$C. Moreover, the deformation zone around and\nbelow microindents depends on the matrix orientation and is influenced by the\npresence of Laves phases. Most importantly, slip transfer from the $\\alpha$-Mg\nphase to the (Mg,Al)$_2$Ca Laves phase occurred, carried by the basal planes.\nBased on the observed orientation relationship and active slip systems, a slip\ntransfer mechanism from the soft $\\alpha$-Mg phase to the hard Laves phase is\nproposed. Further, we present implications for future alloy design strategies.', 'Phase diagrams supported by density functional theory methods can be crucial\nfor designing high-entropy alloys that are subset of multi-principal$-$element\nalloys. We present phase and property analysis of quinary\n(MoW)$_{x}$Zr$_{y}$(TaTi)$_{1-x-y}$ refractory high-entropy alloys from\ncombined Calculation of Phase Diagram (CALPHAD) and density-functional theory\nresults, supplemented by molecular dynamics simulations. Both CALPHAD and\ndensity-functional theory analysis of phase stability indicates a Mo-W-rich\nregion of this quinary has a stable single-phase body-centered-cubic structure.\nWe report first quinary composition from Mo$-$W$-$Ta$-$Ti$-$Zr family of alloy\nwith pseudo-elastic behavior, i.e., hysteresis in stress$-$strain. Our analysis\nshows that only Mo$-$W$-$rich compositions of Mo$-$W$-$Ta$-$Ti$-$Zr, i.e.,\nMo$+$W$\\ge$ 85 at.%, show reproducible hysteresis in stress-strain responsible\nfor pseudo-elastic behavior. The (MoW)$_{85}$Zr$_{7.5}$(TaTi)$_{7.5}$ was\ndown-selected based on temperature-dependent phase diagram analysis and\nmolecular dynamics simulations predicted elastic behavior that reveals twinning\nassisted pseudoelastic behavior. While mostly unexplored in body-centered-cubic\ncrystals, twinning is a fundamental deformation mechanism that competes against\ndislocation slip in crystalline solids. This alloy shows identical cyclic\ndeformation characteristics during uniaxial $\\lt$100$\\gt$ loading, i.e., the\npseudoelasticity is isotropic in loading direction. Additionally, a temperature\nincrease from 77 to 1500 K enhances the elastic strain recovery in load-unload\ncycles, offering possibly control to tune the pseudoelastic behavior.']"
93,17,93_acoustic_absorption_loudspeaker_sound,"['acoustic', 'absorption', 'loudspeaker', 'sound', 'ear', 'band', 'cylinder', 'frequency', 'waves', 'wave']","['Soft electroactive materials can undergo large deformation subjected to\neither mechanical or electrical stimulus, and hence they can be excellent\ncandidates for designing extremely flexible and adaptive structures and\ndevices. This paper proposes a simple one-dimensional soft phononic crystal\ncylinder made of dielectric elastomer to show how large deformation and\nelectric field can be used jointly to tune the longitudinal waves propagating\nin the PC. A series of soft electrodes are placed periodically along the\ndielectric elastomer cylinder, and hence the material can be regarded as\nuniform in the undeformed state. This is also the case for the uniformly\npre-stretched state induced by a static axial force only. The effective\nperiodicity of the structure is then achieved through two loading paths, i.e.\nby maintaining the longitudinal stretch and applying an electric voltage over\nany two neighbouring electrodes, or by holding the axial force and applying the\nvoltage. All physical field variables for both configurations can be determined\nexactly based on the nonlinear theory of electroelasticity. An infinitesimal\nwave motion is further superimposed on the pre-deformed configurations and the\ncorresponding dispersion equations are derived analytically by invoking the\nlinearized theory for incremental motions. Numerical examples are finally\nconsidered to show the tunability of wave propagation behavior in the soft PC\ncylinder. The outstanding performance regarding the band gap (BG) property of\nthe proposed soft dielectric PC is clearly demonstrated by comparing with the\nconventional design adopting the hard piezoelectric material. Note that soft\ndielectric PCs are susceptible to various kinds of failure (buckling,\nelectromechanical instability, electric breakdown, etc.), imposing\ncorresponding limits on the external stimuli.', 'Soft materials can be designed with a functionally graded (FG) property for\nspecific applications. In this paper, we analyze the axisymmetric guided wave\npropagation in a pressurized FG elastomeric hollow cylinder. The cylinder is\nsubjected to a combined action of axial pre-stretch and pressure difference\napplied to the inner and outer cylindrical surfaces. We consider both torsional\nwaves and longitudinal waves propagating in the FG cylinder made of\nincompressible isotropic elastomer, which is characterized by the Mooney-Rivlin\nstrain energy function but with the material parameters varying with the radial\ncoordinate in an affine way. The pressure difference generates an inhomogeneous\ndeformation field in the FG cylinder, which dramatically complicates the\nsuperimposed wave problem described by the small-on-large theory. A\nparticularly efficient approach is hence employed which combines the\nstate-space formalism for the incremental wave motion with the approximate\nlaminate or multi-layer technique. Dispersion relations for the two types of\naxisymmetric guided waves are then derived analytically. The accuracy and\nconvergence of the proposed approach is validated numerically. The effects of\nthe pressure difference, material gradient, and axial pre-stretch on both the\ntorsional and the longitudinal wave propagation characteristics are discussed\nin detail through numerical examples. It is found that the frequency of\naxisymmetric waves depends nonlinearly on the pressure difference and the\nmaterial gradient, and an increase in the material gradient enhances the\ncapability of the pressure difference to adjust the wave behavior in the FG\ncylinder.', 'Sound absorption at low frequencies still remains a challenge in both\nscientific research and engineering practice. Natural porous materials are\nineffective in this frequency range, as well as acoustic resonators which\npresent too narrow bandwidth of absorption, thus requiring alternative\nsolutions based on active absorption techniques. In the present work, we\npropose an active control framework applied on a closed-box loudspeaker to\nenable the adjustment of the acoustic impedance at the loudspeaker diaphragm.\nMore specifically, based on the proportionality between the pressure inside the\nenclosure and the axial displacement of the loudspeaker diaphragm at low\nfrequencies, we demonstrate both analytically and experimentally that a\nPID-like feedback control approach allows tuning independently the compliance,\nthe resistance and the moving mass of the closed-box loudspeaker to implement a\nprescribed impedance of a single-degree-of-freedom resonator. By considering\ndifferent control combinations to tailor the resonator characteristics, a\nperfect absorption (with absorption coefficient equal to 1) is achievable at\nthe target resonance frequency, while enlarging the effective absorption\nbandwidth. Moreover, the proposed feedback control strategy shows an excellent\ncontrol accuracy, especially compared to the feedforward-based control formerly\nreported in the literature. The mismatches between the performance of\nexperimental prototype and the model, likely to result from the control time\ndelay and the inaccuracy in estimating the loudspeaker parameters, can be\ncompensated directly by tuning the control parameters in the control platform.\nThe active resonators implemented through the reported control scheme can be\nused to build more complex acoustic devices/structures to enable\nhigh-efficiency broadband sound absorption or other types of acoustic phenomena\nsuch as wavefront shaping.']"
94,17,94_hat_left_right_cl,"['hat', 'left', 'right', 'cl', 'manifolds', 'w_1', 'lambda', 'orbits', 'curve', 'sets']","['We introduce the class of tame contact manifolds $(M,\\lambda)$, which\nincludes compact ones but not necessarily compact, and establish uniform a\npriori $C^0$-estimates for the nonlinear elliptic boundary value problem of\nHamiltonian-perturbed contact instantons with the Legendrian boundary condition\nintroduced in [Oh21b]. We study the problem of estimating the untangling energy\nof one Legendrian submanifold from the Reeb flow trace of another, and\nformulate a particularly designed parameterized moduli space for the study of\nthe problem. We establish the Gromov-Floer-Hofer type convergence result for\nHamiltonian-perturbed contact instantons of finite energy and construct its\ncompactification of the moduli space, first by defining the correct energy and\nthen by proving a uniform a priori energy bounds in terms of the oscillation of\nthe relevant contact Hamiltonian. Using this, we prove that the self\nReeb-untangling energy of a compact Legendrian submanifold $R$ in any tame\ncontact manifold $(M,\\lambda)$ is greater than that of the period gap\n$T_\\lambda(M,R)$ of the Reeb chords of $R$. Then applying the operation of the\n\\emph{Legendrianization} of contactomorphisms we prove a conjecture of Sandon\nand Shelukhin: there exists a translated point for any contactomorphism\nisotopic to the identity whose oscillation norm is smaller than the period gap\nof the contact manifold $(M,\\lambda)$, and at least $\\operatorname{dim}\nH_*(M,\\mathbb Z_2)$ translated points if the contactomorphism is nondegenerate\nin addition.', ""Focusing on the algebraical analysis of two various kinds of one-dimensional\nG-dynamics ${{\\hat{w}}^{\\left( cl \\right)}}$ and ${{\\hat{w}}^{\\left(\nri\\right)}}$ separately induced by different Hamiltonian operators $\\hat{H} $\nare the keypoints. In this work, it's evidently proved that an identity\n${{\\hat{w}}^{\\left( cl \\right)}}{{u}^{-1/2}}\\equiv0$ always holds for any $u>0$\nbased on the formula of one-dimensional G-dynamics ${{\\hat{w}}^{\\left( cl\n\\right)}}$. We prove that the G-dynamics ${{\\hat{w}}^{\\left( cl \\right)}}$ and\n${{\\hat{w}}^{\\left( ri\\right)}}$ obey Leibniz identity if and only if\n${{\\hat{w}}^{\\left( cl \\right)}}1=0$ and ${{\\hat{w}}^{\\left( ri\\right)}}1=0$,\nrespectively. \\par In accordance with the G-dynamics ${{\\hat{w}}^{\\left( cl\n\\right)}}$, we investigate the unique eigenvalues equation ${{\\hat{w}}^{\\left(\ncl \\right)}}L\\left( u,t,\\lambda \\right)=-\\sqrt{-1}\\lambda L\\left( u,t,\\lambda\n\\right)$ of the G-dynamics with a precise geometric eigenfunction $L\\left(\nu,t,\\lambda \\right)={{u}^{-1/2}}{{e}^{{{\\lambda }}t}},~u>0$ as time $t\\in\n\\left[ 0,T \\right]$ develops and the equation of energy spectrum is then\ninduced. The non-Hermitian Hamiltonian operators are studied as well, we obtain\na series of ODE with their special solutions, and we prove multiplicative\nproperty of the geometric eigenfunction. The coordinate derivative and time\nevolution of the G-dynamics are respectively considered. Seeking the invariance\nof G-dynamics ${{\\hat{w}}^{\\left( cl \\right)}}$ under coordinate transformation\nis considered, so that we think of one-dimensional G-dynamics\n${{\\hat{w}}^{\\left( cl \\right)}}$ on coordinate transformation, it gives the\nrule of conversion between two coordinate systems. As a application, some\nexamples are given for such rule of conversion. Meanwhile, we search the\nconditions that quantum geometric bracket vanishes and a specific case follows."", 'We revisit the geometrical meaning of statistical isotropy that is manifest\nin excursion sets of smooth random fields in two dimensions. Using the contour\nMinkowski tensor, $\\W_1$, as our basic tool we first examine geometrical\nproperties of single structures. For simple closed curves in two dimensions we\nshow that $\\W_1$ is proportional to the identity matrix if the curve has\n$m$-fold symmetry, with $m\\ge 3$. Then we elaborate on how $\\W_1$ maps any\narbitrary shaped simple closed curve to an ellipse that is unique up to\ntranslations of its centroid. We also carry out a comparison of the shape\nparameters, $\\alpha$ and $\\beta$, defined using $\\W_1$, with the filamentarity\nparameter defined using two scalar Minkowski functionals - area and contour\nlength. We show that they contain complementary shape information, with $\\W_1$\ncontaining additional information of orientation of structures. Next, we apply\nour method to boundaries of excursion sets of random fields and examine what\nstatistical isotropy means for the geometry of the excursion sets. Focusing on\nGaussian isotropic fields, and using a semi-numerical approach we quantify the\neffect of finite sampling of the field on the geometry of the excursion sets.\nIn doing so we obtain an analytic expression for $\\alpha$ which takes into\naccount the effect of finite sampling. Finally we derive an analytic expression\nfor the ensemble expectation of $\\W_1$ for Gaussian anisotropic random fields.\nOur results provide insights that are useful for designing tests of statistical\nisotropy using cosmological data.']"
95,17,95_dark_matter_axion_dm,"['dark', 'matter', 'axion', 'dm', 'cavity', 'sensitivity', 'haloscope', 'detector', 'uldm', 'axions']","['The axion is a hypothetical particle which is a candidate for cold dark\nmatter. Haloscope experiments directly search for these particles in strong\nmagnetic fields with RF cavities as detectors. The Relic Axion Detector\nExploratory Setup (RADES) at CERN in particular is searching for axion dark\nmatter in a mass range above 30 $\\mu$eV. The figure of merit of our detector\ndepends linearly on the quality factor of the cavity and therefore we are\nresearching the possibility of coating our cavities with different\nsuperconducting materials to increase the quality factor. Since the experiment\noperates in strong magnetic fields of 11 T and more, superconductors with high\ncritical magnetic fields are necessary. Suitable materials for this application\nare for example REBa$_2$Cu$_3$O$_{7-x}$, Nb$_3$Sn or NbN. We designed a\nmicrowave cavity which resonates at around 9~GHz, with a geometry optimized to\nfacilitate superconducting coating and designed to fit in the bore of available\nhigh-field accelerator magnets at CERN. Several prototypes of this cavity were\ncoated with different superconducting materials, employing different coating\ntechniques. These prototypes were characterized in strong magnetic fields at\n4.2 K.', 'Axion-like particles (ALPs) are undiscovered pseudo-scalar particles that are\ncandidates for ultralight dark matter. ALPs interact with photons slightly and\ncause the rotational oscillation of linearly polarized light. Dark matter Axion\nsearch with riNg Cavity Experiment (DANCE) searches for ALP dark matter by\namplifying the rotational oscillation with a bow-tie ring cavity. Simultaneous\nresonance of linear polarizations is necessary to amplify both the carrier\nfield and the ALP signal, and to achieve the design sensitivity. The\nsensitivity of the current prototype experiment DANCE Act-1 is less than\nexpectation by around three orders of magnitude due to the resonant frequency\ndifference between s- and p-polarization in the bow-tie ring cavity. In order\nto tune the resonant frequency difference, the method of introducing an\nauxiliary cavity was proposed. We designed an auxiliary cavity that can cancel\nout the resonant frequency difference and realize simultaneous resonance,\nconsidering optical loss. We also confirmed that the sensitivity of DANCE Act-1\nwith the auxiliary cavity can reach the original sensitivity.', 'Direct detection experiments are gaining in mass reach. Here we show that the\ninclusion of dark Compton scattering, which has typically been neglected in\nabsorption searches, has a substantial impact on the reach and discovery\npotential of direct detection experiments at high bosonic cold dark matter\nmasses. We demonstrate this for relic dark photons and axion-like particles: we\nimprove expected reach across materials, and further use results from\nSuperCDMS, EDELWEISS and GERDA to place enhanced limits on dark matter\nparameter space. We outline the implications for detector design and analysis.']"
96,16,96_fairness_bias_fair_sl,"['fairness', 'bias', 'fair', 'sl', 'counterfactual', 'causal', 'rule', 'counterfactuals', 'machine', 'vaca']","['Machine learning models are becoming pervasive in high-stakes applications.\nDespite their clear benefits in terms of performance, the models could show\nbias against minority groups and result in fairness issues in a decision-making\nprocess, leading to severe negative impacts on the individuals and the society.\nIn recent years, various techniques have been developed to mitigate the bias\nfor machine learning models. Among them, in-processing methods have drawn\nincreasing attention from the community, where fairness is directly taken into\nconsideration during model design to induce intrinsically fair models and\nfundamentally mitigate fairness issues in outputs and representations. In this\nsurvey, we review the current progress of in-processing bias mitigation\ntechniques. Based on where the fairness is achieved in the model, we categorize\nthem into explicit and implicit methods, where the former directly incorporates\nfairness metrics in training objectives, and the latter focuses on refining\nlatent representation learning. Finally, we conclude the survey with a\ndiscussion of the research challenges in this community to motivate future\nexploration.', 'Ensuring fairness in machine learning algorithms is a challenging and\nimportant task. We consider the problem of clustering a set of points while\nensuring fairness constraints. While there have been several attempts to\ncapture group fairness in the k-clustering problem, fairness at an individual\nlevel is not well-studied. We introduce a new notion of individual fairness in\nk-clustering based on features that are not necessarily used for clustering. We\nshow that this problem is NP-hard and does not admit a constant factor\napproximation. We then design a randomized algorithm that guarantees\napproximation both in terms of minimizing the clustering distance objective as\nwell as individual fairness under natural restrictions on the distance metric\nand fairness constraints. Finally, our experimental results validate that our\nalgorithm produces lower clustering costs compared to existing algorithms while\nbeing competitive in individual fairness.', 'With the growing awareness to fairness in machine learning and the\nrealization of the central role that data representation has in data processing\ntasks, there is an obvious interest in notions of fair data representations.\nThe goal of such representations is that a model trained on data under the\nrepresentation (e.g., a classifier) will be guaranteed to respect some fairness\nconstraints.\n  Such representations are useful when they can be fixed for training models on\nvarious different tasks and also when they serve as data filtering between the\nraw data (known to the representation designer) and potentially malicious\nagents that use the data under the representation to learn predictive models\nand make decisions.\n  A long list of recent research papers strive to provide tools for achieving\nthese goals.\n  However, we prove that this is basically a futile effort. Roughly stated, we\nprove that no representation can guarantee the fairness of classifiers for\ndifferent tasks trained using it; even the basic goal of achieving\nlabel-independent Demographic Parity fairness fails once the marginal data\ndistribution shifts. More refined notions of fairness, like Odds Equality,\ncannot be guaranteed by a representation that does not take into account the\ntask specific labeling rule with respect to which such fairness will be\nevaluated (even if the marginal data distribution is known a priory).\nFurthermore, except for trivial cases, no representation can guarantee Odds\nEquality fairness for any two different tasks, while allowing accurate label\npredictions for both.\n  While some of our conclusions are intuitive, we formulate (and prove) crisp\nstatements of such impossibilities, often contrasting impressions conveyed by\nmany recent works on fair representations.']"
97,15,97_video_grounding_temporal_modal,"['video', 'grounding', 'temporal', 'modal', 'moment', 'sentence', 'query', 'tsgv', 'videos', 'end']","['Video grounding aims to localize the temporal segment corresponding to a\nsentence query from an untrimmed video. Almost all existing video grounding\nmethods fall into two frameworks: 1) Top-down model: It predefines a set of\nsegment candidates and then conducts segment classification and regression. 2)\nBottom-up model: It directly predicts frame-wise probabilities of the\nreferential segment boundaries. However, all these methods are not end-to-end,\n\\ie, they always rely on some time-consuming post-processing steps to refine\npredictions. To this end, we reformulate video grounding as a set prediction\ntask and propose a novel end-to-end multi-modal Transformer model, dubbed as\n\\textbf{GTR}. Specifically, GTR has two encoders for video and language\nencoding, and a cross-modal decoder for grounding prediction. To facilitate the\nend-to-end training, we use a Cubic Embedding layer to transform the raw videos\ninto a set of visual tokens. To better fuse these two modalities in the\ndecoder, we design a new Multi-head Cross-Modal Attention. The whole GTR is\noptimized via a Many-to-One matching loss. Furthermore, we conduct\ncomprehensive studies to investigate different model design choices. Extensive\nresults on three benchmarks have validated the superiority of GTR. All three\ntypical GTR variants achieve record-breaking performance on all datasets and\nmetrics, with several times faster inference speed.', 'Video grounding aims to localize the corresponding video moment in an\nuntrimmed video given a language query. Existing methods often address this\ntask in an indirect way, by casting it as a proposal-and-match or\nfusion-and-detection problem. Solving these surrogate problems often requires\nsophisticated label assignment during training and hand-crafted removal of\nnear-duplicate results. Meanwhile, existing works typically focus on sparse\nvideo grounding with a single sentence as input, which could result in\nambiguous localization due to its unclear description. In this paper, we tackle\na new problem of dense video grounding, by simultaneously localizing multiple\nmoments with a paragraph as input. From a perspective on video grounding as\nlanguage conditioned regression, we present an end-to-end parallel decoding\nparadigm by re-purposing a Transformer-alike architecture (PRVG). The key\ndesign in our PRVG is to use languages as queries, and directly regress the\nmoment boundaries based on language-modulated visual representations. Thanks to\nits simplicity in design, our PRVG framework can be applied in different\ntesting schemes (sparse or dense grounding) and allows for efficient inference\nwithout any post-processing technique. In addition, we devise a robust\nproposal-level attention loss to guide the training of PRVG, which is invariant\nto moment duration and contributes to model convergence. We perform experiments\non two video grounding benchmarks of ActivityNet Captions and TACoS,\ndemonstrating that our PRVG can significantly outperform previous methods. We\nalso perform in-depth studies to investigate the effectiveness of parallel\nregression paradigm on video grounding.', 'This paper focuses on tackling the problem of temporal language localization\nin videos, which aims to identify the start and end points of a moment\ndescribed by a natural language sentence in an untrimmed video. However, it is\nnon-trivial since it requires not only the comprehensive understanding of the\nvideo and sentence query, but also the accurate semantic correspondence capture\nbetween them. Existing efforts are mainly centered on exploring the sequential\nrelation among video clips and query words to reason the video and sentence\nquery, neglecting the other intra-modal relations (e.g., semantic similarity\namong video clips and syntactic dependency among the query words). Towards this\nend, in this work, we propose a Multi-modal Interaction Graph Convolutional\nNetwork (MIGCN), which jointly explores the complex intra-modal relations and\ninter-modal interactions residing in the video and sentence query to facilitate\nthe understanding and semantic correspondence capture of the video and sentence\nquery. In addition, we devise an adaptive context-aware localization method,\nwhere the context information is taken into the candidate moments and the\nmulti-scale fully connected layers are designed to rank and adjust the boundary\nof the generated coarse candidate moments with different lengths. Extensive\nexperiments on Charades-STA and ActivityNet datasets demonstrate the promising\nperformance and superior efficiency of our model.']"
98,15,98_vehicle_video_driving_cctv,"['vehicle', 'video', 'driving', 'cctv', 'sensemag', 'traffic', 'gaze', 'cameras', 'privacy', 'vision']","['Movement specific vehicle classification and counting at traffic\nintersections is a crucial component for various traffic management activities.\nIn this context, with recent advancements in computer-vision based techniques,\ncameras have emerged as a reliable data source for extracting vehicular\ntrajectories from traffic scenes. However, classifying these trajectories by\nmovement type is quite challenging as characteristics of motion trajectories\nobtained this way vary depending on camera calibrations. Although some existing\nmethods have addressed such classification tasks with decent accuracies, the\nperformance of these methods significantly relied on manual specification of\nseveral regions of interest. In this study, we proposed an automated\nclassification method for movement specific classification (such as right-turn,\nleft-turn and through movements) of vision-based vehicle trajectories. Our\nclassification framework identifies different movement patterns observed in a\ntraffic scene using an unsupervised hierarchical clustering technique\nThereafter a similarity-based assignment strategy is adopted to assign incoming\nvehicle trajectories to identified movement groups. A new similarity measure\nwas designed to overcome the inherent shortcomings of vision-based\ntrajectories. Experimental results demonstrated the effectiveness of the\nproposed classification approach and its ability to adapt to different traffic\nscenarios without any manual intervention.', ""Autonomous driving gained huge traction in recent years, due to its potential\nto change the way we commute. Much effort has been put into trying to estimate\nthe state of a vehicle. Meanwhile, learning to forecast the state of a vehicle\nahead introduces new capabilities, such as predicting dangerous situations.\nMoreover, forecasting brings new supervision opportunities by learning to\npredict richer a context, expressed by multiple horizons. Intuitively, a video\nstream originated from a front-facing camera is necessary because it encodes\ninformation about the upcoming road. Besides, historical traces of the\nvehicle's states give more context. In this paper, we tackle multi-horizon\nforecasting of vehicle states by fusing the two modalities. We design and\nexperiment with 3 end-to-end architectures that exploit 3D convolutions for\nvisual features extraction and 1D convolutions for features extraction from\nspeed and steering angle traces. To demonstrate the effectiveness of our\nmethod, we perform extensive experiments on two publicly available real-world\ndatasets, Comma2k19 and the Udacity challenge. We show that we are able to\nforecast a vehicle's state to various horizons, while outperforming the current\nstate-of-the-art results on the related task of driving state estimation. We\nexamine the contribution of vision features, and find that a model fed with\nvision features achieves an error that is 56.6% and 66.9% of the error of a\nmodel that doesn't use those features, on the Udacity and Comma2k19 datasets\nrespectively."", ""The operation and management of intelligent transportation systems (ITS),\nsuch as traffic monitoring, relies on real-time data aggregation of vehicular\ntraffic information, including vehicular types (e.g., cars, trucks, and buses),\nin the critical roads and highways. While traditional approaches based on\nvehicular-embedded GPS sensors or camera networks would either invade drivers'\nprivacy or require high deployment cost, this paper introduces a low-cost\nmethod, namely SenseMag, to recognize the vehicular type using a pair of\nnon-invasive magnetic sensors deployed on the straight road section. SenseMag\nfilters out noises and segments received magnetic signals by the exact time\npoints that the vehicle arrives or departs from every sensor node. Further,\nSenseMag adopts a hierarchical recognition model to first estimate the\nspeed/velocity, then identify the length of vehicle using the predicted speed,\nsampling cycles, and the distance between the sensor nodes. With the vehicle\nlength identified and the temporal/spectral features extracted from the\nmagnetic signals, SenseMag classify the types of vehicles accordingly. Some\nsemi-automated learning techniques have been adopted for the design of filters,\nfeatures, and the choice of hyper-parameters. Extensive experiment based on\nreal-word field deployment (on the highways in Shenzhen, China) shows that\nSenseMag significantly outperforms the existing methods in both classification\naccuracy and the granularity of vehicle types (i.e., 7 types by SenseMag versus\n4 types by the existing work in comparisons). To be specific, our field\nexperiment results validate that SenseMag is with at least $90\\%$ vehicle type\nclassification accuracy and less than 5\\% vehicle length classification error.""]"
99,15,99_text_recognition_detection_ocr,"['text', 'recognition', 'detection', 'ocr', 'shrink', 'str', 'plate', 'shaped', 'highlight', 'masks']","['Arbitrary-shaped text detection has recently attracted increasing interests\nand witnessed rapid development with the popularity of deep learning\nalgorithms. Nevertheless, existing approaches often obtain inaccurate detection\nresults, mainly due to the relatively weak ability to utilize context\ninformation and the inappropriate choice of offset references. This paper\npresents a novel text instance expression which integrates both foreground and\nbackground information into the pipeline, and naturally uses the pixels near\ntext boundaries as the offset starts. Besides, a corresponding post-processing\nalgorithm is also designed to sequentially combine the four prediction results\nand reconstruct the text instance accurately. We evaluate our method on several\nchallenging scene text benchmarks, including both curved and multi-oriented\ntext datasets. Experimental results demonstrate that the proposed approach\nobtains superior or competitive performance compared to other state-of-the-art\nmethods, e.g., 83.4% F-score for Total-Text, 82.4% F-score for MSRA-TD500, etc.', ""The latest trend in the bottom-up perspective for arbitrary-shape scene text\ndetection is to reason the links between text segments using Graph\nConvolutional Network (GCN). Notwithstanding, the performance of the best\nperforming bottom-up method is still inferior to that of the best performing\ntop-down method even with the help of GCN. We argue that this is not mainly\ncaused by the limited feature capturing ability of the text proposal backbone\nor GCN, but by their failure to make a full use of visual-relational features\nfor suppressing false detection, as well as the sub-optimal route-finding\nmechanism used for grouping text segments. In this paper, we revitalize the\nclassic text detection frameworks by aggregating the visual-relational features\nof text with two effective false positive/negative suppression mechanisms.\nFirst, dense overlapping text segments depicting the `characterness' and\n`streamline' of text are generated for further relational reasoning and weakly\nsupervised segment classification. Here, relational graph features are used for\nsuppressing false positives/negatives. Then, to fuse the relational features\nwith visual features, a Location-Aware Transfer (LAT) module is designed to\ntransfer text's relational features into visual compatible features with a Fuse\nDecoding (FD) module to enhance the representation of text regions for the\nsecond step suppression. Finally, a novel multiple-text-map-aware\ncontour-approximation strategy is developed, instead of the widely-used\nroute-finding process. Experiments conducted on five benchmark datasets, i.e.,\nCTW1500, Total-Text, ICDAR2015, MSRA-TD500, and MLT2017 demonstrate that our\nmethod outperforms the state-of-the-art performance when being embedded in a\nclassic text detection framework, which revitalises the superb strength of the\nbottom-up methods."", 'Recent approaches for end-to-end text spotting have achieved promising\nresults. However, most of the current spotters were plagued by the\ninconsistency problem between text detection and recognition. In this work, we\nintroduce and prove the existence of the inconsistency problem and analyze it\nfrom two aspects: (1) inconsistency of text recognition features between\ntraining and testing, and (2) inconsistency of optimization targets between\ntext detection and recognition. To solve the aforementioned issues, we propose\na differentiable Auto-Rectification Module (ARM) together with a new training\nstrategy to enable propagating recognition loss back into detection branch, so\nthat our detection branch can be jointly optimized by detection and recognition\ntargets, which largely alleviates the inconsistency problem between text\ndetection and recognition. Based on these designs, we present a simple yet\nrobust end-to-end text spotting framework, termed Auto-Rectification Text\nSpotter (ARTS), to detect and recognize arbitrarily-shaped text in natural\nscenes. Extensive experiments demonstrate the superiority of our method. In\nparticular, our ARTS-S achieves 77.1% end-to-end text spotting F-measure on\nTotal-Text at a competitive speed of 10.5 FPS, which significantly outperforms\nprevious methods in both accuracy and inference speed.']"
100,15,100_brain_mdd_symptom_connectome,"['brain', 'mdd', 'symptom', 'connectome', 'fmri', 'connectivity', 'cog', 'cbt', 'adas', 'controllability']","['Learning how to estimate a connectional brain template(CBT) from a population\nof brain multigraphs, where each graph (e.g., functional) quantifies a\nparticular relationship between pairs of brain regions of interest (ROIs),\nallows to pin down the unique connectivity patterns shared across individuals.\nSpecifically, a CBT is viewed as an integral representation of a set of highly\nheterogeneous graphs and ideally meeting the centeredness (i.e., minimum\ndistance to all graphs in the population) and discriminativeness (i.e.,\ndistinguishes the healthy from the disordered population) criteria. So far,\nexisting works have been limited to only integrating and fusing a population of\nbrain multigraphs acquired at a single timepoint. In this paper, we\nunprecedentedly tackle the question: Given a baseline multigraph population,\ncan we learn how to integrate and forecast its CBT representations at follow-up\ntimepoints? Addressing such question is of paramount in predicting common\nalternations across healthy and disordered populations. To fill this gap, we\npropose Recurrent Multigraph Integrator Network (ReMI-Net), the first graph\nrecurrent neural network which infers the baseline CBT of an input population\nt1 and predicts its longitudinal evolution over time (ti > t1). Our ReMI-Net is\ncomposed of recurrent neural blocks with graph convolutional layers using a\ncross-node message passing to first learn hidden-states embeddings of each CBT\nnode (i.e., brain region of interest) and then predict its evolution at the\nconsecutive timepoint. Moreover, we design a novel time-dependent loss to\nregularize the CBT evolution trajectory over time and further introduce a\ncyclic recursion and learnable normalization layer to generate well-centered\nCBTs from time-dependent hidden-state embeddings. Finally, we derive the CBT\nadjacency matrix from the learned hidden state graph representation.', 'Charting the baby connectome evolution trajectory during the first year after\nbirth plays a vital role in understanding dynamic connectivity development of\nbaby brains. Such analysis requires acquisition of longitudinal connectomic\ndatasets. However, both neonatal and postnatal scans are rarely acquired due to\nvarious difficulties. A small body of works has focused on predicting baby\nbrain evolution trajectory from a neonatal brain connectome derived from a\nsingle modality. Although promising, large training datasets are essential to\nboost model learning and to generalize to a multi-trajectory prediction from\ndifferent modalities (i.e., functional and morphological connectomes). Here, we\nunprecedentedly explore the question: Can we design a few-shot learning-based\nframework for predicting brain graph trajectories across different modalities?\nTo this aim, we propose a Graph Multi-Trajectory Evolution Network (GmTE-Net),\nwhich adopts a teacher-student paradigm where the teacher network learns on\npure neonatal brain graphs and the student network learns on simulated brain\ngraphs given a set of different timepoints. To the best of our knowledge, this\nis the first teacher-student architecture tailored for brain graph\nmulti-trajectory growth prediction that is based on few-shot learning and\ngeneralized to graph neural networks (GNNs). To boost the performance of the\nstudent network, we introduce a local topology-aware distillation loss that\nforces the predicted graph topology of the student network to be consistent\nwith the teacher network. Experimental results demonstrate substantial\nperformance gains over benchmark methods. Hence, our GmTE-Net can be leveraged\nto predict atypical brain connectivity trajectory evolution across various\nmodalities. Our code is available at https: //github.com/basiralab/GmTE-Net.', 'Brain functional connectivity (FC) reveals biomarkers for identification of\nvarious neuropsychiatric disorders. Recent application of deep neural networks\n(DNNs) to connectome-based classification mostly relies on traditional\nconvolutional neural networks using input connectivity matrices on a regular\nEuclidean grid. We propose a graph deep learning framework to incorporate the\nnon-Euclidean information about graph structure for classifying functional\nmagnetic resonance imaging (fMRI)- derived brain networks in major depressive\ndisorder (MDD). We design a novel graph autoencoder (GAE) architecture based on\nthe graph convolutional networks (GCNs) to embed the topological structure and\nnode content of large-sized fMRI networks into low-dimensional latent\nrepresentations. In network construction, we employ the Ledoit-Wolf (LDW)\nshrinkage method to estimate the high-dimensional FC metrics efficiently from\nfMRI data. We consider both supervised and unsupervised approaches for the\ngraph embedded learning. The learned embeddings are then used as feature inputs\nfor a deep fully-connected neural network (FCNN) to discriminate MDD from\nhealthy controls. Evaluated on a resting-state fMRI MDD dataset with 43\nsubjects, results show that the proposed GAE-FCNN model significantly\noutperforms several state-of-the-art DNN methods for brain connectome\nclassification, achieving accuracy of 72.50% using the LDW-FC metrics as node\nfeatures. The graph embeddings of fMRI FC networks learned by the GAE also\nreveal apparent group differences between MDD and HC. Our new framework\ndemonstrates feasibility of learning graph embeddings on brain networks to\nprovide discriminative information for diagnosis of brain disorders.']"
101,15,101_heat_thermal_building_energy,"['heat', 'thermal', 'building', 'energy', 'collector', 'exchangers', 'wall', 'soil', 'cement', 'dissipation']","['This work establishes a closed-form analysis for two-dimensional transient\nheat transfer in an absorber plate based on the thermal wave model when the\ncollector starts to work. The method of separation of variables is employed to\ndetermine the exact thermal response during the starting of a solar collector.\nThe thermal field is represented by 2-D thermal contours and 3-D surface plots\nto provide an accurate prediction compared to the 1-D analysis using the\ntime-lag theory. An influence of Fourier number on the heat flow in the\nabsorber plate is studied for different thermal relaxation times. The present\nanalysis highlights that the magnitude of plate temperature depends\nsignificantly on the physical geometry. Differences in two models of heat\npropagation, viz. classical and thermal wave, in the absorber plate have an\nalteration in predicted collector performances. The present study also develops\nanother design situation for heat transfer in the absorber plate when the\ncollector shutdowns to the ambient state from its steady condition. Laplace\nTransform Method (LTM) in combination with the product solution method uses to\ndetermine the temperature with the initial condition represented as a steady\ncase. Due to the lagging nature of thermal behavior, a significant difference\nbetween the classical and thermal wave models for the propagation of heat was\nobserved, and also this study determines the exact time to be taken to reach\nthe dead state for the complete shutdown of the collector.', ""Building energy performance is one of the key features in performance-based\nbuilding design decision making. Building envelope materials can play a key\nrole in improving building energy performance. The thermal properties of\nbuilding materials determine the level of heat transfer through building\nenvelope, thus the annual thermal energy performance of the building. This\nresearch applies the Linear Discriminant Analysis (LDA) method to study the\neffects of materials' thermal properties on building thermal loads. Two\napproaches are adopted for feature selection including the Principal Component\nAnalysis (PCA) and the Exhaustive Feature Selection (EFS). A hypothetical\ndesign scenario is developed with six material alternatives for an office\nbuilding in Los Angeles, California. The best design alternative is selected\nbased on the LDA results and the key input parameters are determined based on\nthe PCA and EFS methods. The PCA results confirm that among all thermal\nproperties of the materials, the four parameters including thermal\nconductivity, density, specific heat capacity, and thickness are the most\ncritical features, in terms of building thermal behavior and thermal energy\nconsumption. This result matches quite well with the assumptions of most of the\nbuilding energy simulation tools."", 'The building sector is responsible for 40% of primary energy consumption,\nwith heating/cooling covering the most significant portion. Thus, passive\nheating/cooling applications have gained significant ground during the last\nthree decades, with many research activities on the subject. Among passive\ncooling/heating applications, ground cooling (especially earth-to-air heat\nexchangers) has been highlighted as a remarkably attractive technological\nresearch subjects because of its significant contribution to the reduction of\nheating/cooling energy loads; the improvement of indoor thermal comfort\nconditions; and the amelioration of the urban environment. This paper presents\na holistic review of state-of-the-art research, methodologies, and technologies\nof earth-to-air heat exchangers that help achieve energy conservation and\nthermal comfort in the built environment. The review covers the critical\nsubject of the thermal performance of earth-to-air heat exchanger systems;\nexperimental studies and applications; parametric studies for investigating the\nimpact of their main characteristics on thermal efficiency; and recent advances\nand trends including hybrid technologies and systems. The models describing the\nthermal performance of earth-to-air heat exchangers systems were classified in\nnumerical, analytical, and data-driven; their main theoretical principles were\npresented; and experimental validation was mentioned when carried out. System\nparameters were grouped into three categories: system design, soil types, and\nsoil surface coverage. System design parameters, especially length and burial\ndepth, bore the most important influence on the thermal efficiency of the\nsystem. The paper was rounded up with an economic assessment of system\napplication, and the conclusions highlighted the need for more experimental\nwork including laboratory simulators.']"
102,14,102_catalysts_surface_adsorption_cu,"['catalysts', 'surface', 'adsorption', 'cu', 'metal', 'catalyst', 'co2', 'sites', 'ru', 'hydrogen']","['The search for highly efficient and low-cost catalysts based on earth\nabundant elements is one of the main driving forces in organic and inorganic\nchemistry. Current strategies for the heterogeneous catalyst design focus on\nincreasing the density of active regions and the optimization of Gibbs free\nenergy of chemical reaction via doping. In this work, we investigate the\nchemical properties of moire metal for hydrogen evolution reaction. We show\nthat the local shift of metallic atoms introduces a variation in surface\nhydrogen bonding strength and leads to the spatially dependent Gibbs free\nenergy for hydrogen absorption. Remarkably, the Gibbs free energy of AB stacked\nNbS2 is shown to cover the thermoneutral volcano peak, exceeding the efficiency\nof the current record platinum. The richness of local chemical environment in\nmoire structures provides a unique way to tune the reactivity for various\nchemical reactions. Our study establishes a recipe for designing superior\ncatalysts from the simple twist or stacking of formerly inactive catalytic\nmetals, and proposes structural moire as a promising nanoengineering method to\ntune surface properties for chemical adsorption.', 'Electrocatalytic hydrogen evolution reaction (HER) in alkaline media is a\npromising electrochemical energy conversion strategy. Ruthenium (Ru) is an\nefficient catalyst with a desirable cost for HER, however, the sluggish H2O\ndissociation process, due to the low H2O adsorption on its surface, currently\nhampers the performances of this catalyst in alkaline HER. Herein, we\ndemonstrate that the H2O adsorption improves significantly by the construction\nof Ru-O-Mo sites. We prepared Ru/MoO2 catalysts with Ru-O-Mo sites through a\nfacile thermal treatment process and assessed the creation of Ru-O-Mo\ninterfaces by transmission electron microscope (TEM) and extended X-ray\nabsorption fine structure (EXAFS). By using Fourier-transform infrared\nspectroscopy (FTIR) and H2O adsorption tests, we proved Ru-O-Mo sites have\ntenfold stronger H2O adsorption ability than that of Ru catalyst. The catalysts\nwith Ru-O-Mo sites exhibited a state-of-the-art overpotential of 16 mV at 10 mA\ncm-2 in 1 M KOH electrolyte, demonstrating a threefold reduction than the\nprevious bests of Ru (59 mV) and commercial Pt (31 mV) catalysts. We proved the\nstability of these performances over 40 hours without decline. These results\ncould open a new path for designing efficient and stable catalysts.', 'Electrochemical CO2 reduction is a promising strategy for utilization of CO2\nand intermittent excess electricity. Cu is the only single-metal catalyst that\ncan electrochemically convert CO2 to multi-carbon products. However, Cu has an\nundesirable selectivity and activity for C2 products, due to its insufficient\namount of CO* for C-C coupling. Considering the strong CO2 adsorption and\nultra-fast reaction kinetics of CO* formation on Pd, an intimate CuPd(100)\ninterface was designed to lower the intermediate reaction barriers and then\nimprove the efficiency of C2 products. Density functional theory (DFT)\ncalculations showed that the CuPd(100) interface has enhanced CO2 adsorption\nand decreased CO2* hydrogenation energy barrier, which are beneficial for C-C\ncoupling. The potential-determining step (PDS) barrier of CO2 to C2 products on\nCuPd(100) interface is 0.61 eV, which is lower than that on Cu(100) (0.72 eV).\nMotivated by the DFT calculation, the CuPd(100) interface catalyst was prepared\nby a facile chemical solution method and demonstrated by transmission electron\nmicroscope (TEM). The CO2 temperature programmed desorption (CO2-TPD) and gas\nsensor experiments proved the enhancements of CO2 adsorption and CO2*\nhydrogenation abilities on CuPd(100) interface catalyst. As a result, the\nobtained CuPd(100) interface catalyst exhibits a C2 Faradaic efficiency of 50.3\n(+/-) 1.2% at -1.4 VRHE in 0.1 M KHCO3, which is 2.1 times higher than\n23.6(+/-) 1.5% of Cu catalyst. This work provides a rational design of Cu-based\nelectrocatalyst for multi-carbon products by fine-tuning the intermediate\nreaction barriers.']"
103,14,103_batteries_electrode_battery_ion,"['batteries', 'electrode', 'battery', 'ion', 'electrodes', 'pore', 'charging', 'lithium', 'transport', 'density']","['Slow ionic transport and high voltage drop (IR drop) of homogeneous porous\nelectrodes are the critical causes of severe performance degradation of\nlithium-ion (Li-ion) batteries under high charging rates. Herein, we\ndemonstrate that a bio-inspired vascularized porous electrode can\nsimultaneously solve these two problems by introducing low tortuous channels\nand graded porosity. To optimize the vasculature structural parameters, we\nemploy artificial neural networks (ANNs) to accelerate the computation of\npossible structures with high accuracy. Furthermore, an inverse-design\nsearching library is compiled to find the optimal vascular structures under\ndifferent industrial fabrication and design criteria. The prototype delivers a\ncustomizable package containing optimal geometric parameters and their\nuncertainty and sensitivity analysis. Finally, the full-vascularized cell shows\na 66% improvement of charging capacity than the traditional homogeneous cell\nunder 3.2C current density. This research provides an innovative methodology to\nsolve the fast-charging problem in batteries and broaden the applicability of\ndeep learning algorithm to different scientific or engineering areas.', 'A thick electrode with high areal capacity has been developed as a strategy\nfor high-energy-density lithium-ion batteries, but thick electrodes have\ndifficulties in manufacturing and limitations in ion transport. Here, we\nreported a new manufacturing approach for ultra-thick electrode with aligned\nstructure, called structure electrode additive manufacturing or SEAM, which\naligns active materials to the through-thicknesses direction of electrodes\nusing shear flow and a designed printing path. The ultra-thick electrodes with\nhigh loading of active materials, low tortuous structure, and good structure\nstability resulting from a simple and scalable SEAM lead to rapid ion transport\nand fast electrolyte infusion, delivering a higher areal capacity than\nslurry-casted thick electrodes. SEAM shows strengths in design flexibility and\nscalability, which allows the production of practical high energy/power density\nstructure electrodes.', 'Porous electrodes are found in energy storage devices such as supercapacitors\nand pseudo-capacitors. However, the effect of electrode-pore-size distribution\nover their energy storage properties remains unclear. Here, we develop a model\nfor the charging of electrical double layers inside a cylindrical pore for\narbitrary pore size. We assume small applied potentials and perform a regular\nperturbation analysis to predict the evolution of electrical potential and ion\nconcentrations in both the radial and axial directions. We validate our\nperturbation model with direct numerical simulations of the\nPoisson-Nernst-Planck equations, and obtain quantitative agreement between the\ntwo approaches for small and moderate potentials. Our analysis yields two main\ncharacteristic features of arbitrary pore size: i) a monotonic decrease of the\ncharging timescale with an increase in relative pore size (pore size relative\nto Debye length); ii) a region of large potential gradients at the mouth of the\npore due to charge conservation. We develop a modified transmission circuit\nmodel that captures the effect of arbitrary pore sizes and demonstrate that a\ntime-dependent interfacial capacitance needs to be included in the circuit. We\nalso derive expressions for effective capacitance and charging timescale as a\nfunction of pore-size distribution, and show that the capacitance and charging\ntimescale increase for narrower and less polydisperse distributions, resulting\nin a gain of energy density at a constant power density. Overall, our results\nadvance the fundamental understanding of electrical-double-layer charging and\nwill be useful for the electrode design of energy storage devices.']"
104,14,104_ao_mirror_wfs_wavefront,"['ao', 'mirror', 'wfs', 'wavefront', 'telescope', 'instrument', 'exoplanets', 'imaging', 'deformable', 'optics']","[""The Santa Cruz Extreme AO Lab (SEAL) is a new visible-wavelength testbed\ndesigned to advance the state of the art in wavefront control for high contrast\nimaging on large, segmented, ground-based telescopes. SEAL provides multiple\noptions for simulating atmospheric turbulence, including rotating phase plates\nand a custom Meadowlark spatial light modulator that delivers phase offsets of\nup to 6pi at 635nm. A 37-segment IrisAO deformable mirror (DM) simulates the W.\nM. Keck Observatory segmented primary mirror. The adaptive optics system\nconsists of a woofer/tweeter deformable mirror system (a 97-actuator ALPAO DM\nand 1024-actuator Boston Micromachines MEMs DM, respectively), and four\nwavefront sensor arms: 1) a high-speed Shack-Hartmann WFS, 2) a reflective\npyramid WFS, designed as a prototype for the ShaneAO system at Lick\nObservatory, 3) a vector-Zernike WFS, and 4) a Fast Atmospheric Self Coherent\nCamera Technique (FAST) demonstration arm, consisting of a custom focal plane\nmask and high-speed sCMOS detector. Finally, science arms preliminarily include\na classical Lyot-style coronagraph as well as FAST (which doubles as a WFS and\nscience camera). SEAL's real time control system is based on the Compute and\nControl for Adaptive optics (CACAO) package, and is designed to support the\nefficient transfer of software between SEAL and the Keck II AO system. In this\npaper, we present an overview of the design and first light performance of\nSEAL."", ""Extreme adaptive optics (AO) is crucial for enabling the contrasts needed for\nground-based high contrast imaging instruments to detect exoplanets. Pushing\nexoplanet imaging detection sensitivities towards lower mass, closer\nseparations, and older planets will require upgrading AO wavefront sensors\n(WFSs) to be more efficient. In particular, future WFS designs will aim to\nimprove a WFS's measurement error (i.e., the wavefront level at which photon\nnoise, detector noise, and/or sky background limits a WFS measurement) and\nlinearity (i.e., the wavefront level, in the absence of photon noise, aliasing,\nand servo lag, at which an AO loop can close and the corresponding closed-loop\nresidual level). We present one such design here called the bright pyramid WFS\n(bPWFS), which improves both the linearity and measurement errors as compared\nto the non-modulated pyramid WFS (PWFS). The bPWFS is a unique design that,\nunlike other WFSs, doesn't sacrifice measurement error for linearity,\npotentially enabling this WFS to (a) close the AO loop on open loop turbulence\nutilising a tip/tilt modulation mirror (i.e., a modulated bPWFS; analogous to\nthe procedure used for the regular modulated PWFS), and (b) reach deeper\nclosed-loop residual wavefront levels (i.e., improving both linearity and\nmeasurement error) compared to the regular non-modulated PWFS. The latter\napproach could be particularly beneficial to enable improved AO performance\nusing the bWFS as a second stage AO WFS. In this paper we will present an AO\nerror budget analysis of the non-modulated bPWFS as well as supporting AO\ntestbed results from the Marseille Astrophysics Laboratory."", 'The Planetary Systems Imager (PSI) is a proposed instrument for the Thirty\nMeter Telescope (TMT) that provides an extreme adaptive optics (AO) correction\nto a multi-wavelength instrument suite optimized for high contrast science.\nPSI\'s broad range of capabilities, spanning imaging, polarimetry, integral\nfield spectroscopy, and high resolution spectroscopy from 0.6-5 microns, with a\npotential channel at 10 microns, will enable breakthrough science in the areas\nof exoplanet formation and evolution. Here, we present a preliminary optical\ndesign and performance analysis toolset for the 2-5 microns component of the\nPSI AO system, which must deliver the wavefront quality necessary to support\ninfrared high contrast science cases. PSI-AO is a two-stage system, with an\ninitial deformable mirror and infrared wavefront sensor providing a common\nwavefront correction to all PSI science instruments followed by a dichroic that\nseparates ""PSI-Red"" (2-5 microns) from ""PSI-Blue"" (0.5-1.8 microns). To meet\nthe demands of visible-wavelength high contrast science, the PSI-Blue arm will\ninclude a second deformable mirror and a visible-wavelength wavefront sensor.\nIn addition to an initial optical design of the PSI-Red AO system, we present a\npreliminary set of tools for an end-to-end AO simulation that in future work\nwill be used to demonstrate the planet-to-star contrast ratios achievable with\nPSI-Red.']"
105,14,105_tracking_frame_trackers_object,"['tracking', 'frame', 'trackers', 'object', 'appearance', 'tracker', 'target', 'mot', 'video', 'panoptic']","['Most existing trackers based on deep learning perform tracking in a holistic\nstrategy, which aims to learn deep representations of the whole target for\nlocalizing the target. It is arduous for such methods to track targets with\nvarious appearance variations. To address this limitation, another type of\nmethods adopts a part-based tracking strategy which divides the target into\nequal patches and tracks all these patches in parallel. The target state is\ninferred by summarizing the tracking results of these patches. A potential\nlimitation of such trackers is that not all patches are equally informative for\ntracking. Some patches that are not discriminative may have adverse effects. In\nthis paper, we propose to track the salient local parts of the target that are\ndiscriminative for tracking. In particular, we propose a fine-grained saliency\nmining module to capture the local saliencies. Further, we design a\nsaliency-association modeling module to associate the captured saliencies\ntogether to learn effective correlation representations between the exemplar\nand the search image for state estimation. Extensive experiments on five\ndiverse datasets demonstrate that the proposed method performs favorably\nagainst state-of-the-art trackers.', 'Benefiting from the development of Deep Neural Networks, Multi-Object\nTracking (MOT) has achieved aggressive progress. Currently, the real-time\nJoint-Detection-Tracking (JDT) based MOT trackers gain increasing attention and\nderive many excellent models. However, the robustness of JDT trackers is rarely\nstudied, and it is challenging to attack the MOT system since its mature\nassociation algorithms are designed to be robust against errors during\ntracking. In this work, we analyze the weakness of JDT trackers and propose a\nnovel adversarial attack method, called Tracklet-Switch (TraSw), against the\ncomplete tracking pipeline of MOT. Specifically, a push-pull loss and a center\nleaping optimization are designed to generate adversarial examples for both\nre-ID feature and object detection. TraSw can fool the tracker to fail to track\nthe targets in the subsequent frames by attacking very few frames. We evaluate\nour method on the advanced deep trackers (i.e., FairMOT, JDE, ByteTrack) using\nthe MOT-Challenge datasets (i.e., 2DMOT15, MOT17, and MOT20). Experiments show\nthat TraSw can achieve a high success rate of over 95% by attacking only five\nframes on average for the single-target attack and a reasonably high success\nrate of over 80% for the multiple-target attack. The code is available at\nhttps://github.com/DerryHub/FairMOT-attack .', 'In many visual systems, visual tracking often bases on RGB image sequences,\nin which some targets are invalid in low-light conditions, and tracking\nperformance is thus affected significantly. Introducing other modalities such\nas depth and infrared data is an effective way to handle imaging limitations of\nindividual sources, but multi-modal imaging platforms usually require elaborate\ndesigns and cannot be applied in many real-world applications at present.\nNear-infrared (NIR) imaging becomes an essential part of many surveillance\ncameras, whose imaging is switchable between RGB and NIR based on the light\nintensity. These two modalities are heterogeneous with very different visual\nproperties and thus bring big challenges for visual tracking. However, existing\nworks have not studied this challenging problem. In this work, we address the\ncross-modal object tracking problem and contribute a new video dataset,\nincluding 654 cross-modal image sequences with over 481K frames in total, and\nthe average video length is more than 735 frames. To promote the research and\ndevelopment of cross-modal object tracking, we propose a new algorithm, which\nlearns the modality-aware target representation to mitigate the appearance gap\nbetween RGB and NIR modalities in the tracking process. It is plug-and-play and\ncould thus be flexibly embedded into different tracking frameworks. Extensive\nexperiments on the dataset are conducted, and we demonstrate the effectiveness\nof the proposed algorithm in two representative tracking frameworks against 17\nstate-of-the-art tracking methods. We will release the dataset for free\nacademic usage, dataset download link and code will be released soon.']"
106,14,106_forgetting_continual_catastrophic_cl,"['forgetting', 'continual', 'catastrophic', 'cl', 'learning', 'old', 'classes', 'new', 'meta', 'tasks']","['Continual Learning (CL) is an emerging machine learning paradigm that aims to\nlearn from a continuous stream of tasks without forgetting knowledge learned\nfrom the previous tasks. To avoid performance decrease caused by forgetting,\nprior studies exploit episodic memory (EM), which stores a subset of the past\nobserved samples while learning from new non-i.i.d. data. Despite the promising\nresults, since CL is often assumed to execute on mobile or IoT devices, the EM\nsize is bounded by the small hardware memory capacity and makes it infeasible\nto meet the accuracy requirements for real-world applications. Specifically,\nall prior CL methods discard samples overflowed from the EM and can never\nretrieve them back for subsequent training steps, incurring loss of information\nthat would exacerbate catastrophic forgetting. We explore a novel hierarchical\nEM management strategy to address the forgetting issue. In particular, in\nmobile and IoT devices, real-time data can be stored not just in high-speed\nRAMs but in internal storage devices as well, which offer significantly larger\ncapacity than the RAMs. Based on this insight, we propose to exploit the\nabundant storage to preserve past experiences and alleviate the forgetting by\nallowing CL to efficiently migrate samples between memory and storage without\nbeing interfered by the slow access speed of the storage. We call it Carousel\nMemory (CarM). As CarM is complementary to existing CL methods, we conduct\nextensive evaluations of our method with seven popular CL methods and show that\nCarM significantly improves the accuracy of the methods across different\nsettings by large margins in final average accuracy (up to 28.4%) while\nretaining the same training efficiency.', 'Catastrophic forgetting of previously learned knowledge while learning new\ntasks is a widely observed limitation of contemporary neural networks. Although\nmany continual learning methods are proposed to mitigate this drawback, the\nmain question remains unanswered: what is the root cause of catastrophic\nforgetting? In this work, we aim at answering this question by posing and\nvalidating a set of research hypotheses related to the specificity of\nrepresentations built internally by neural models. More specifically, we design\na set of empirical evaluations that compare the robustness of representations\nin discriminative and generative models against catastrophic forgetting. We\nobserve that representations learned by discriminative models are more prone to\ncatastrophic forgetting than their generative counterparts, which sheds new\nlight on the advantages of developing generative models for continual learning.\nFinally, our work opens new research pathways and possibilities to adopt\ngenerative models in continual learning beyond mere replay mechanisms.', 'A growing body of research in continual learning is devoted to overcoming the\n""Catastrophic Forgetting"" of neural networks by designing new algorithms that\nare more robust to the distribution shifts. While the recent progress in\ncontinual learning literature is encouraging, our understanding of what\nproperties of neural networks contribute to catastrophic forgetting is still\nlimited. To address this, instead of focusing on continual learning algorithms,\nin this work, we focus on the model itself and study the impact of ""width"" of\nthe neural network architecture on catastrophic forgetting, and show that width\nhas a surprisingly significant effect on forgetting. To explain this effect, we\nstudy the learning dynamics of the network from various perspectives such as\ngradient norm and sparsity, orthogonalization, and lazy training regime. We\nprovide potential explanations that are consistent with the empirical results\nacross different architectures and continual learning benchmarks.']"
107,14,107_codes_decoding_polar_rcq,"['codes', 'decoding', 'polar', 'rcq', 'crc', 'sc', 'scma', 'list', 'scl', 'latency']","['Polar codes are normally designed based on the reliability of the\nsub-channels in the polarized vector channel. There are various methods with\ndiverse complexity and accuracy to evaluate the reliability of the\nsub-channels. However, designing polar codes solely based on the sub-channel\nreliability may result in poor Hamming distance properties. In this work, we\npropose a different approach to design the information set for polar codes and\nPAC codes where the objective is to reduce the number of codewords with minimum\nweight (a.k.a. error coefficient) of a code designed for maximum reliability.\nThis approach is based on the coset-wise characterization of the rows of polar\ntransform $\\mathbf{G}_N$ involved in the formation of the minimum-weight\ncodewords. Our analysis capitalizes on the properties of the polar transform\nbased on its row and column indices. The numerical results show that the\ndesigned codes outperform PAC codes and CRC-Polar codes at the practical block\nerror rate of $10^{-2}-10^{-3}$. Furthermore, a by-product of the combinatorial\nproperties analyzed in this paper is an alternative enumeration method of the\nminimum-weight codewords.', 'The joint transmission scheme of polar codes and sparse code multiple access\n(SCMA) has been regarded as a promising technology for future wireless\ncommunication systems. However, most of the existing polar-coded SCMA (PC-SCMA)\nsystems suffer from high latency caused by the feedback iteration and list\ndecoding. In addition, the error performance of PC-SCMA systems is\nunsatisfactory for ultra-reliable transmission. Inspired by the compelling\nbenefits of non-binary polar codes, in this paper, we design a non-binary\npolar-coded SCMA (NB-PC-SCMA) system with a free order matching strategy to\naddress the issues of delay and reliability. Specifically, we first formulate a\njoint factor graph for NB-PC-SCMA and propose a non-binary successive\ncancellation list (NB-SCL) and damping based joint iterative detection and\ndecoding (NSD-JIDD) multiuser receiver to improve the BER and latency\nperformance. Then, a lazy-search based NB-SCL (L-NB-SCL) decoding is proposed\nto reduce the computational complexity by modifying the path search pattern of\nthe list decoder. After that, we optimize the update of user nodes for SCMA\ndetection to improve the convergence error and finally propose the optimized\nNSD-JIDD (OSD-JIDD) algorithm, which can avoid redundant operations by\nexploiting L-NB-SCL decoding. Simulation results show that the proposed\nNB-PC-SCMA system achieves better bit error rate (BER) performance and\nconsiderable latency gain when compared to its counterparts. In particular, the\nproposed OSD-JIDD can achieve similar BER performance of NSD-JIDD with less\ncomplexity.', 'Mission critical Machine-type Communication, also referred to as\nUltra-reliable Low Latency Communication is primarily characterized by\ncommunication that provides ultra-high reliability and very low latency to\nconcurrently transmit short commands to a massive number of connected devices.\nWhile the reduction in PHY layer overhead and improvement in channel coding\ntechniques are pivotal in reducing latency and improving reliability, the\ncurrent wireless standards dedicated to support mcMTC rely heavily on adopting\nthe bottom layers of general-purpose wireless standards and customizing only\nthe upper layers. The mcMTC has a significant technical impact on the design of\nall layers of the communication protocol stack. In this paper, an innovative\nbottom-up approach has been proposed for mcMTC applications through PHY layer\ntargeted at improving the transmission reliability by implementing\nultra-reliable channel coding scheme in the PHY layer of IEEE 802.11a bearing\nin mind short packet transmission system. To achieve this aim, we analyzed and\ncompared the channel coding performance of convolutional codes, LDPC codes, and\npolar codes in wireless network on the condition of short data packet\ntransmission. The Viterbi decoding algorithm, logarithmic belief propagation\nalgorithm, and cyclic redundancy check - successive cancellation list decoding\nalgorithm were adopted to CC, LDPC codes, and polar codes, respectively.\nConsequently, a new PHY layer for mcMTC has been proposed. The reliability of\nthe proposed approach has been validated by simulation in terms of Bit error\nrate vs. SNR. The simulation results demonstrate that the reliability of IEEE\n802.11a standard has been significantly improved to be at PER less 10e-5 with\nthe implementation of polar codes. The results also show that the\ngeneral-purpose wireless networks are prominent in providing short packet mcMTC\nwith the modification needed.']"
108,14,108_apps_privacy_party_third,"['apps', 'privacy', 'party', 'third', 'children', 'banners', 'cookie', 'businesses', 'google', 'online']","[""Parental control apps, which are mobile apps that allow parents to monitor\nand restrict their children's activities online, are becoming increasingly\nadopted by parents as a means of safeguarding their children's online safety.\nHowever, it is not clear whether these apps are always beneficial or effective\nin what they aim to do; for instance, the overuse of restriction and\nsurveillance has been found to undermine parent-child relationship and\nchildren's sense of autonomy. In this work, we investigate this gap, asking\nspecifically: how might children's and parents' perceptions be related to how\nparental control features were designed? To investigate this question, we\nconducted an analysis of 58 top Android parental control apps designed for the\npurpose of promoting children's online safety, finding three major axes of\nvariation in how key restriction and monitoring features were realised:\ngranularity, feedback/transparency, and parent-child communications support. To\nrelate these axes to perceived benefits and problems, we then analysed 3264 app\nreviews to identify references to aspects of the each of the axes above, to\nunderstand children's and parents' views of how such dimensions related to\ntheir experiences with these apps. Our findings led towards 1) an understanding\nof how parental control apps realise their functionalities differently along\nthree axes of variation, 2) an analysis of exactly the ways that such variation\ninfluences children's and parents' perceptions, respectively of the usefulness\nor effectiveness of these apps, and finally 3) an identification of design\nrecommendations and opportunities for future apps by contextualising our\nfindings within existing digital parenting theories."", ""Online services like Google provide a variety of application programming\ninterfaces (APIs). These online APIs enable authenticated third-party services\nand applications (apps) to access a user's account data for tasks such as\nsingle sign-on (SSO), calendar integration, and sending email on behalf of the\nuser, among others. Despite their prevalence, API access could pose significant\nprivacy and security risks, where a third-party could have unexpected\nprivileges to a user's account. To gauge users' perceptions and concerns\nregarding third-party apps that integrate with online APIs, we performed a\nmulti-part online survey of Google users. First, we asked n = 432 participants\nto recall if and when they allowed third-party access to their Google account:\n89% recalled using at least one SSO and 52% remembered at least one third-party\napp. In the second survey, we re-recruited n = 214 participants to ask about\nspecific apps and SSOs they've authorized on their own Google accounts. We\ncollected in-the-wild data about users' actual SSOs and authorized apps: 86%\nused Google SSO on at least one service, and 67% had at least one third-party\napp authorized. After examining their apps and SSOs, participants expressed the\nmost concern about access to personal information like email addresses and\nother publicly shared info. However, participants were less concerned with\nbroader -- and perhaps more invasive -- access to calendars, emails, or cloud\nstorage (as needed by third-party apps). This discrepancy may be due in part to\ntrust transference to apps that integrate with Google, forming an implied\npartnership. Our results suggest opportunities for design improvements to the\ncurrent third-party management tools offered by Google; for example, tracking\nrecent access, automatically revoking access due to app disuse, and providing\npermission controls."", ""While many studies have looked at privacy properties of the Android and\nGoogle Play app ecosystem, comparatively much less is known about iOS and the\nApple App Store, the most widely used ecosystem in the US. At the same time,\nthere is increasing competition around privacy between these smartphone\noperating system providers. In this paper, we present a study of 24k Android\nand iOS apps from 2020 along several dimensions relating to user privacy. We\nfind that third-party tracking and the sharing of unique user identifiers was\nwidespread in apps from both ecosystems, even in apps aimed at children. In the\nchildren's category, iOS apps used much fewer advertising-related tracking than\ntheir Android counterparts, but could more often access children's location (by\na factor of 7). Across all studied apps, our study highlights widespread\npotential violations of US, EU and UK privacy law, including 1) the use of\nthird-party tracking without user consent, 2) the lack of parental consent\nbefore sharing PII with third-parties in children's apps, 3) the\nnon-data-minimising configuration of tracking libraries, 4) the sending of\npersonal data to countries without an adequate level of data protection, and 5)\nthe continued absence of transparency around tracking, partly due to design\ndecisions by Apple and Google. Overall, we find that neither platform is\nclearly better than the other for privacy across the dimensions we studied.""]"
109,14,109_boldsymbol_regression_estimators_estimator,"['boldsymbol', 'regression', 'estimators', 'estimator', 'mathbb', 'theta', 'beta', 'norm', 'descent', 'gaussian']","[""We study active sampling algorithms for linear regression, which aim to query\nonly a small number of entries of a target vector $b\\in\\mathbb{R}^n$ and output\na near minimizer to $\\min_{x\\in\\mathbb{R}^d}\\|Ax-b\\|$, where $A\\in\\mathbb{R}^{n\n\\times d}$ is a design matrix and $\\|\\cdot\\|$ is some loss function.\n  For $\\ell_p$ norm regression for any $0<p<\\infty$, we give an algorithm based\non Lewis weight sampling that outputs a $(1+\\epsilon)$ approximate solution\nusing just $\\tilde{O}(d^{\\max(1,{p/2})}/\\mathrm{poly}(\\epsilon))$ queries to\n$b$. We show that this dependence on $d$ is optimal, up to logarithmic factors.\nOur result resolves a recent open question of Chen and Derezi\\'{n}ski, who gave\nnear optimal bounds for the $\\ell_1$ norm, and suboptimal bounds for $\\ell_p$\nregression with $p\\in(1,2)$.\n  We also provide the first total sensitivity upper bound of\n$O(d^{\\max\\{1,p/2\\}}\\log^2 n)$ for loss functions with at most degree $p$\npolynomial growth. This improves a recent result of Tukan, Maalouf, and\nFeldman. By combining this with our techniques for the $\\ell_p$ regression\nresult, we obtain an active regression algorithm making $\\tilde\nO(d^{1+\\max\\{1,p/2\\}}/\\mathrm{poly}(\\epsilon))$ queries, answering another open\nquestion of Chen and Derezi\\'{n}ski. For the important special case of the\nHuber loss, we further improve our bound to an active sample complexity of\n$\\tilde O(d^{(1+\\sqrt2)/2}/\\epsilon^c)$ and a non-active sample complexity of\n$\\tilde O(d^{4-2\\sqrt 2}/\\epsilon^c)$, improving a previous $d^4$ bound for\nHuber regression due to Clarkson and Woodruff. Our sensitivity bounds have\nfurther implications, improving a variety of previous results using sensitivity\nsampling, including Orlicz norm subspace embeddings and robust subspace\napproximation. Finally, our active sampling results give the first sublinear\ntime algorithms for Kronecker product regression under every $\\ell_p$ norm."", ""The Gaussian noise stability of a function $f:\\mathbb{R}^n \\to \\{-1, 1\\}$ is\nthe expected value of $f(\\boldsymbol{x}) \\cdot f(\\boldsymbol{y})$ over\n$\\rho$-correlated Gaussian random variables $\\boldsymbol{x}$ and\n$\\boldsymbol{y}$. Borell's inequality states that for $-1 \\leq \\rho \\leq 0$,\nthis is minimized by the halfspace $f(x) = \\mathrm{sign}(x_1)$. In this work,\nwe generalize this result to hold for functions $f:\\mathbb{R}^n \\to S^{k-1}$\nwhich output $k$-dimensional unit vectors. Our main result shows that the\nexpected value of $\\langle f(\\boldsymbol{x}), f(\\boldsymbol{y})\\rangle$ over\n$\\rho$-correlated Gaussians $\\boldsymbol{x}$ and $\\boldsymbol{y}$ is minimized\nby the function $f(x) = x_{\\leq k} / \\Vert x_{\\leq k} \\Vert$, where $x_{\\leq k}\n= (x_1, \\ldots, x_k)$.\n  As an application, we show several hardness of approximation results for\nQuantum Max-Cut, a special case of the local Hamiltonian problem related to the\nanti-ferromagnetic Heisenberg model. Quantum Max-Cut is a natural quantum\nanalogue of classical Max-Cut and has become testbed for designing quantum\napproximation algorithms. We show the following:\n  (1) The integrality gap of the basic SDP is $0.498$, matching an existing\nrounding algorithm. Combined with existing approximation results for Quantum\nMax-Cut, this shows that the basic SDP does not achieve the optimal\napproximation ratio.\n  (2) It is Unique Games-hard (UG-hard) to compute a\n$(0.956+\\varepsilon)$-approximation to the value of the best product state,\nmatching an existing approximation algorithm. This result may be viewed as\napplying to a generalization of Max-Cut where one seeks to assign\n$3$-dimensional unit vectors to each vertex; we also give tight hardness\nresults for the analogous $k$-dimensional generalization of Max-Cut.\n  (3) It is UG-hard to compute a $(0.956+\\varepsilon)$-approximation to the\nvalue of the best (possibly entangled) state."", 'The Chebyshev or $\\ell_{\\infty}$ estimator is an unconventional alternative\nto the ordinary least squares in solving linear regressions. It is defined as\nthe minimizer of the $\\ell_{\\infty}$ objective function \\begin{align*}\n  \\hat{\\boldsymbol{\\beta}} := \\arg\\min_{\\boldsymbol{\\beta}} \\|\\boldsymbol{Y} -\n\\mathbf{X}\\boldsymbol{\\beta}\\|_{\\infty}. \\end{align*} The asymptotic\ndistribution of the Chebyshev estimator under fixed number of covariates were\nrecently studied (Knight, 2020), yet finite sample guarantees and\ngeneralizations to high-dimensional settings remain open. In this paper, we\ndevelop non-asymptotic upper bounds on the estimation error\n$\\|\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}^*\\|_2$ for a Chebyshev estimator\n$\\hat{\\boldsymbol{\\beta}}$, in a regression setting with uniformly distributed\nnoise $\\varepsilon_i\\sim U([-a,a])$ where $a$ is either known or unknown. With\nrelatively mild assumptions on the (random) design matrix $\\mathbf{X}$, we can\nbound the error rate by $\\frac{C_p}{n}$ with high probability, for some\nconstant $C_p$ depending on the dimension $p$ and the law of the design.\nFurthermore, we illustrate that there exist designs for which the Chebyshev\nestimator is (nearly) minimax optimal. In addition we show that ""Chebyshev\'s\nLASSO"" has advantages over the regular LASSO in high dimensional situations,\nprovided that the noise is uniform. Specifically, we argue that it achieves a\nmuch faster rate of estimation under certain assumptions on the growth rate of\nthe sparsity level and the ambient dimension with respect to the sample size.']"
110,13,110_mutations_cov_sars_viral,"['mutations', 'cov', 'sars', 'viral', 'virus', 'variants', 'vaccine', '19', 'covid', 'infectious']","[""SARS-CoV-2, like any other virus, continues to mutate as it spreads,\naccording to an evolutionary process. Unlike any other virus, the number of\ncurrently available sequences of SARS-CoV-2 in public databases such as GISAID\nis already several million. This amount of data has the potential to uncover\nthe evolutionary dynamics of a virus like never before. However, a million is\nalready several orders of magnitude beyond what can be processed by the\ntraditional methods designed to reconstruct a virus's evolutionary history,\nsuch as those that build a phylogenetic tree. Hence, new and scalable methods\nwill need to be devised in order to make use of the ever increasing number of\nviral sequences being collected.\n  Since identifying variants is an important part of understanding the\nevolution of a virus, in this paper, we propose an approach based on clustering\nsequences to identify the current major SARS-CoV-2 variants. Using a $k$-mer\nbased feature vector generation and efficient feature selection methods, our\napproach is effective in identifying variants, as well as being efficient and\nscalable to millions of sequences. Such a clustering method allows us to show\nthe relative proportion of each variant over time, giving the rate of spread of\neach variant in different locations -- something which is important for vaccine\ndevelopment and distribution. We also compute the importance of each amino acid\nposition of the spike protein in identifying a given variant in terms of\ninformation gain. Positions of high variant-specific importance tend to agree\nwith those reported by the USA's Centers for Disease Control and Prevention\n(CDC), further demonstrating our approach."", ""The importance of understanding SARS-CoV-2 evolution cannot be\noveremphasized. Recent studies confirm that natural selection is the dominating\nmechanism of SARS-CoV-2 evolution, which favors mutations that strengthen viral\ninfectivity. We demonstrate that vaccine-breakthrough or antibody-resistant\nmutations provide a new mechanism of viral evolution. Specifically,\nvaccine-resistant mutation Y449S in the spike (S) protein receptor-bonding\ndomain (RBD), which occurred in co-mutation [Y449S, N501Y], has reduced\ninfectivity compared to the original SARS-CoV-2 but can disrupt existing\nantibodies that neutralize the virus. By tracing the evolutionary trajectories\nof vaccine-resistant mutations in over 1.9 million SARS-CoV-2 genomes, we\nreveal that the occurrence and frequency of vaccine-resistant mutations\ncorrelate strongly with the vaccination rates in Europe and America. We\nanticipate that as a complementary transmission pathway, vaccine-resistant\nmutations will become a dominating mechanism of SARS-CoV-2 evolution when most\nof the world's population is vaccinated. Our study sheds light on SARS-CoV-2\nevolution and transmission and enables the design of the next-generation\nmutation-proof vaccines and antibody drugs."", 'The recent global surge in COVID-19 infections has been fueled by new\nSARS-CoV-2 variants, namely Alpha, Beta, Gamma, Delta, etc. The molecular\nmechanism underlying such surge is elusive due to 4,653 non-degenerate\nmutations on the spike protein, which is the target of most COVID-19 vaccines.\nThe understanding of the molecular mechanism of transmission and evolution is a\nprerequisite to foresee the trend of emerging vaccine-breakthrough variants and\nthe design of mutation-proof vaccines and monoclonal antibodies. We integrate\nthe genotyping of 1,489,884 SARS-CoV-2 genomes isolates, 130 human antibodies,\ntens of thousands of mutational data points, topological data analysis, and\ndeep learning to reveal SARS-CoV-2 evolution mechanism and forecast emerging\nvaccine-escape variants. We show that infectivity-strengthening and\nantibody-disruptive co-mutations on the S protein RBD can quantitatively\nexplain the infectivity and virulence of all prevailing variants. We\ndemonstrate that Lambda is as infectious as Delta but is more\nvaccine-resistant. We analyze emerging vaccine-breakthrough co-mutations in 20\ncountries, including the United Kingdom, the United States, Denmark, Brazil,\nand Germany, etc. We envision that natural selection through infectivity will\ncontinue to be the main mechanism for viral evolution among unvaccinated\npopulations, while antibody disruptive co-mutations will fuel the future growth\nof vaccine-breakthrough variants among fully vaccinated populations. Finally,\nwe have identified the co-mutations that have the great likelihood of becoming\ndominant: [A411S, L452R, T478K], [L452R, T478K, N501Y], [V401L, L452R, T478K],\n[K417N, L452R, T478K], [L452R, T478K, E484K, N501Y], and [P384L, K417N, E484K,\nN501Y]. We predict they, particularly the last four, will break through\nexisting vaccines. We foresee an urgent need to develop new vaccines that\ntarget these co-mutations.']"
111,13,111_quantization_bnn_binary_bit,"['quantization', 'bnn', 'binary', 'bit', 'esb', 'quantized', 'weights', 'architectures', 'networks', 'search']","['In the low-bit quantization field, training Binary Neural Networks (BNNs) is\nthe extreme solution to ease the deployment of deep models on\nresource-constrained devices, having the lowest storage cost and significantly\ncheaper bit-wise operations compared to 32-bit floating-point counterparts. In\nthis paper, we introduce Sub-bit Neural Networks (SNNs), a new type of binary\nquantization design tailored to compress and accelerate BNNs. SNNs are inspired\nby an empirical observation, showing that binary kernels learnt at\nconvolutional layers of a BNN model are likely to be distributed over kernel\nsubsets. As a result, unlike existing methods that binarize weights one by one,\nSNNs are trained with a kernel-aware optimization framework, which exploits\nbinary quantization in the fine-grained convolutional kernel space.\nSpecifically, our method includes a random sampling step generating\nlayer-specific subsets of the kernel space, and a refinement step learning to\nadjust these subsets of binary kernels via optimization. Experiments on visual\nrecognition benchmarks and the hardware deployment on FPGA validate the great\npotentials of SNNs. For instance, on ImageNet, SNNs of ResNet-18/ResNet-34 with\n0.56-bit weights achieve 3.13/3.33 times runtime speed-up and 1.8 times\ncompression over conventional BNNs with moderate drops in recognition accuracy.\nPromising results are also obtained when applying SNNs to binarize both weights\nand activations. Our code is available at https://github.com/yikaiw/SNN.', 'Gradient quantization is an emerging technique in reducing communication\ncosts in distributed learning. Existing gradient quantization algorithms often\nrely on engineering heuristics or empirical observations, lacking a systematic\napproach to dynamically quantize gradients. This paper addresses this issue by\nproposing a novel dynamically quantized SGD (DQ-SGD) framework, enabling us to\ndynamically adjust the quantization scheme for each gradient descent step by\nexploring the trade-off between communication cost and convergence error. We\nderive an upper bound, tight in some cases, of the convergence error for a\nrestricted family of quantization schemes and loss functions. We design our\nDQ-SGD algorithm via minimizing the communication cost under the convergence\nerror constraints. Finally, through extensive experiments on large-scale\nnatural language processing and computer vision tasks on AG-News, CIFAR-10, and\nCIFAR-100 datasets, we demonstrate that our quantization scheme achieves better\ntradeoffs between the communication cost and learning performance than other\nstate-of-the-art gradient quantization methods.', 'Network quantization, which aims to reduce the bit-lengths of the network\nweights and activations, has emerged for their deployments to resource-limited\ndevices. Although recent studies have successfully discretized a full-precision\nnetwork, they still incur large quantization errors after training, thus giving\nrise to a significant performance gap between a full-precision network and its\nquantized counterpart. In this work, we propose a novel quantization method for\nneural networks, Cluster-Promoting Quantization (CPQ) that finds the optimal\nquantization grids while naturally encouraging the underlying full-precision\nweights to gather around those quantization grids cohesively during training.\nThis property of CPQ is thanks to our two main ingredients that enable\ndifferentiable quantization: i) the use of the categorical distribution\ndesigned by a specific probabilistic parametrization in the forward pass and\nii) our proposed multi-class straight-through estimator (STE) in the backward\npass. Since our second component, multi-class STE, is intrinsically biased, we\nadditionally propose a new bit-drop technique, DropBits, that revises the\nstandard dropout regularization to randomly drop bits instead of neurons. As a\nnatural extension of DropBits, we further introduce the way of learning\nheterogeneous quantization levels to find proper bit-length for each layer by\nimposing an additional regularization on DropBits. We experimentally validate\nour method on various benchmark datasets and network architectures, and also\nsupport a new hypothesis for quantization: learning heterogeneous quantization\nlevels outperforms the case using the same but fixed quantization levels from\nscratch.']"
112,13,112_community_clustering_graphs_weighted,"['community', 'clustering', 'graphs', 'weighted', 'hypergraph', 'streaming', 'nodes', 'graph', 'hypergraphs', 'k_']","['Graph clustering and community detection are central problems in modern data\nmining. The increasing need for analyzing billion-scale data calls for faster\nand more scalable algorithms for these problems. There are certain trade-offs\nbetween the quality and speed of such clustering algorithms. In this paper, we\ndesign scalable algorithms that achieve high quality when evaluated based on\nground truth. We develop a generalized sequential and shared-memory parallel\nframework based on the LambdaCC objective (introduced by Veldt et al.), which\nencompasses modularity and correlation clustering. Our framework consists of\nhighly-optimized implementations that scale to large data sets of billions of\nedges and that obtain high-quality clusters compared to ground-truth data, on\nboth unweighted and weighted graphs. Our empirical evaluation shows that this\nframework improves the state-of-the-art trade-offs between speed and quality of\nscalable community detection. For example, on a 30-core machine with two-way\nhyper-threading, our implementations achieve orders of magnitude speedups over\nother correlation clustering baselines, and up to 28.44x speedups over our own\nsequential baselines while maintaining or improving quality.', 'Community is a universal structure in various complex networks, and community\ndetection is a fundamental task for network analysis. With the rapid growth of\nnetwork scale, networks are massive, changing rapidly and could naturally be\nmodeled as graph streams. Due to the limited memory and access constraint in\ngraph streams, existing non-streaming community detection methods are no longer\napplicable. This raises an emerging need for online approaches. In this work,\nwe consider the problem of uncovering the local community containing a few\nquery nodes in graph streams, termed streaming local community detection. This\nis a new problem raised recently that is more challenging for community\ndetection and only a few works address this online setting. Correspondingly, we\ndesign an online single-pass streaming local community detection approach.\nInspired by the ""local"" property of communities, our method samples the local\nstructure around the query nodes in graph streams, and extracts the target\ncommunity on the sampled subgraph using our proposed metric called the\napproximate conductance. Comprehensive experiments show that our method\nremarkably outperforms the streaming baseline on both effectiveness and\nefficiency, and even achieves similar accuracy comparing to the\nstate-of-the-art non-streaming local community detection methods that use\nstatic and complete graphs.', 'Community detection is an important research topic in graph analytics that\nhas a wide range of applications. A variety of static community detection\nalgorithms and quality metrics were developed in the past few years. However,\nmost real-world graphs are not static and often change over time. In the case\nof streaming data, communities in the associated graph need to be updated\neither continuously or whenever new data streams are added to the graph, which\nposes a much greater challenge in devising good community detection algorithms\nfor maintaining dynamic graphs over streaming data. In this paper, we propose\nan incremental community detection algorithm for maintaining a dynamic graph\nover streaming data. The contributions of this study include (a) the\nimplementation of a Distributed Weighted Community Clustering (DWCC) algorithm,\n(b) the design and implementation of a novel Incremental Distributed Weighted\nCommunity Clustering (IDWCC) algorithm, and (c) an experimental study to\ncompare the performance of our IDWCC algorithm with the DWCC algorithm. We\nvalidate the functionality and efficiency of our framework in processing\nstreaming data and performing large in-memory distributed dynamic graph\nanalytics. The results demonstrate that our IDWCC algorithm performs up to\nthree times faster than the DWCC algorithm for a similar accuracy.']"
113,13,113_qubit_qubits_squid_josephson,"['qubit', 'qubits', 'squid', 'josephson', 'circuit', 'superconducting', 'quantum', 'microwave', 'junction', 'coupling']","['A flux qubit can interact strongly when it is capacitively coupled to other\ncircuit elements. This interaction can be separated in two parts, one acting on\nthe qubit subspaces and one in which excited states mediate the interaction.\nThe first term dominates the interaction between the flux qubit and an\nLC-resonator, leading to ultrastrong couplings of the form\n$\\sigma^y(a+a^\\dagger),$ which complement the inductive\n$\\sigma^xi(a^\\dagger-a)$ coupling. However, when coupling two flux qubits\ncapacitively, all terms need to be taken into account, leading to complex\nnon-stoquastic ultrastrong interaction of the $\\sigma^y\\sigma^y$,\n$\\sigma^z\\sigma^z$ and $\\sigma^x\\sigma^x$ type. Our theory explains all these\ninteractions, describing them in terms of general circuit properties---coupling\ncapacitances, qubit gaps, inductive, Josephson and capactive energies---, that\napply to a wide variety of circuits and flux qubit designs.', 'Gate-tunable Josephson junctions embedded in a microwave environment provide\na promising platform to in-situ engineer and optimize novel superconducting\nquantum circuits. The key quantity for the circuit design is the\nphase-dependent complex admittance of the junction, which can be probed by\nsensing an rf SQUID with a tank circuit. Here, we investigate a graphene-based\nJosephson junction as a prototype gate-tunable element enclosed in a SQUID loop\nthat is inductively coupled to a superconducting resonator operating at 3 GHz.\nWith a concise circuit model that describes the dispersive and dissipative\nresponse of the coupled system, we extract the phase-dependent junction\nadmittance corrected for self-screening of the SQUID loop. We decompose the\nadmittance into the current-phase relation and the phase-dependent loss and as\nthese quantities are dictated by the spectrum and population dynamics of the\nsupercurrent-carrying Andreev bound states, we gain insight to the underlying\nmicroscopic transport mechanisms in the junction. We theoretically reproduce\nthe experimental results by considering a short, diffusive junction model that\ntakes into account the interaction between the Andreev spectrum and the\nelectromagnetic environment, from which we deduce a lifetime of ~17 ps for\nnon-equilibrium populations.', 'The superconducting fluxonium circuit is an RF-SQUID-type flux qubit that\nuses a large inductance built from an array of Josephson junctions or a high\nkinetic inductance material. This inductance suppresses charge sensitivity\nexponentially and flux sensitivity quadratically. In contrast to the transmon\nqubit, the anharmonicity of fluxonium can be large and positive, allowing for\nbetter separation between the low energy qubit manifold of the circuit and\nhigher-lying excited states. Here, we propose a tunable coupling scheme for\nimplementing two-qubit gates on fixed-frequency fluxonium qubits, biased at\nhalf flux quantum. In this system, both qubits and coupler are coupled\ncapacitively and implemented as fluxonium circuits with an additional harmonic\nmode. We investigate the performance of the scheme by simulating a universal\ntwo-qubit fSim gate. In the proposed approach, we rely on a planar on-chip\narchitecture for the whole device. Our design is compatible with existing\nhardware for transmon-based devices, with the additional advantage of lower\nqubit frequency facilitating high-precision gating.']"
114,13,114_electricity_wind_power_grid,"['electricity', 'wind', 'power', 'grid', 'energy', 'renewable', 'generative', 'generation', 'cable', 'validation']","[""Artificial intelligence-based techniques applied to the electricity\nconsumption data generated from the smart grid prove to be an effective\nsolution in reducing Non Technical Loses (NTLs), thereby ensures safety,\nreliability, and security of the smart energy systems. However, imbalanced\ndata, consecutive missing values, large training times, and complex\narchitectures hinder the real time application of electricity theft detection\nmodels. In this paper, we present EnsembleNTLDetect, a robust and scalable\nelectricity theft detection framework that employs a set of efficient data\npre-processing techniques and machine learning models to accurately detect\nelectricity theft by analysing consumers' electricity consumption patterns.\nThis framework utilises an enhanced Dynamic Time Warping Based Imputation\n(eDTWBI) algorithm to impute missing values in the time series data and\nleverages the Near-miss undersampling technique to generate balanced data.\nFurther, stacked autoencoder is introduced for dimensionality reduction and to\nimprove training efficiency. A Conditional Generative Adversarial Network\n(CTGAN) is used to augment the dataset to ensure robust training and a soft\nvoting ensemble classifier is designed to detect the consumers with aberrant\nconsumption patterns. Furthermore, experiments were conducted on the real-time\nelectricity consumption data provided by the State Grid Corporation of China\n(SGCC) to validate the reliability and efficiency of EnsembleNTLDetect over the\nstate-of-the-art electricity theft detection models in terms of various quality\nmetrics."", 'The growth of wind generation capacities in the past decades has shown that\nwind energy can contribute to the energy transition in many parts of the world.\nBeing highly variable and complex to model, the quantification of the\nspatio-temporal variation of wind power and the related uncertainty is highly\nrelevant for energy planners. Machine Learning has become a popular tool to\nperform wind-speed and power predictions. However, the existing approaches have\nseveral limitations. These include (i) insufficient consideration of\nspatio-temporal correlations in wind-speed data, (ii) a lack of existing\nmethodologies to quantify the uncertainty of wind speed prediction and its\npropagation to the wind-power estimation, and (iii) a focus on less than hourly\nfrequencies. To overcome these limitations, we introduce a framework to\nreconstruct a spatio-temporal field on a regular grid from irregularly\ndistributed wind-speed measurements. After decomposing data into temporally\nreferenced basis functions and their corresponding spatially distributed\ncoefficients, the latter are spatially modelled using Extreme Learning\nMachines. Estimates of both model and prediction uncertainties, and of their\npropagation after the transformation of wind speed into wind power, are then\nprovided without any assumptions on distribution patterns of the data. The\nmethodology is applied to the study of hourly wind power potential on a grid of\n$250\\times 250$ m$^2$ for turbines of 100 meters hub height in Switzerland,\ngenerating the first dataset of its type for the country. The potential wind\npower generation is combined with the available area for wind turbine\ninstallations to yield an estimate of the technical potential for wind power in\nSwitzerland. The wind power estimate presented here represents an important\ninput for planners to support the design of future energy systems with\nincreased wind power generation.', 'The design and operation of modern energy systems are heavily influenced by\ntime-dependent and uncertain parameters, e.g., renewable electricity\ngeneration, load-demand, and electricity prices. These are typically\nrepresented by a set of discrete realizations known as scenarios. A popular\nscenario generation approach uses deep generative models (DGM) that allow\nscenario generation without prior assumptions about the data distribution.\nHowever, the validation of generated scenarios is difficult, and a\ncomprehensive discussion about appropriate validation methods is currently\nlacking. To start this discussion, we provide a critical assessment of the\ncurrently used validation methods in the energy scenario generation literature.\nIn particular, we assess validation methods based on probability density,\nauto-correlation, and power spectral density. Furthermore, we propose using the\nmultifractal detrended fluctuation analysis (MFDFA) as an additional validation\nmethod for non-trivial features like peaks, bursts, and plateaus. As\nrepresentative examples, we train generative adversarial networks (GANs),\nWasserstein GANs (WGANs), and variational autoencoders (VAEs) on two renewable\npower generation time series (photovoltaic and wind from Germany in 2013 to\n2015) and an intra-day electricity price time series form the European Energy\nExchange in 2017 to 2019. We apply the four validation methods to both the\nhistorical and the generated data and discuss the interpretation of validation\nresults as well as common mistakes, pitfalls, and limitations of the validation\nmethods. Our assessment shows that no single method sufficiently characterizes\na scenario but ideally validation should include multiple methods and be\ninterpreted carefully in the context of scenarios over short time periods.']"
115,13,115_positioning_localization_csi_gnss,"['positioning', 'localization', 'csi', 'gnss', 'indoor', 'channel', 'seirios', 'position', 'dynoloc', 'dgnss']","['Global Navigation Satellite Systems (GNSS) provide positioning services for\nconnected and autonomous vehicles. Differential GNSS (DGNSS) has been\ndemonstrated to provide reliable, high quality range correction information\nenabling real-time navigation with sub-meter or centimeter accuracy. However,\nDGNSS requires a local reference station near each user, which for a\ncontinental or global scale implementation would require a dense network of\nreference stations whose construction and maintenance would be prohibitively\nexpensive. Precise Point Positioning (PPP) affords more flexibility as a public\nservice for GNSS receivers, but its State Space Representation (SSR) format is\nnot currently supported by most receivers. This article proposes a novel\nVirtual Network DGNSS (VN-DGNSS) design that capitalizes on the PPP\ninfrastructure to provide global coverage for real-time navigation without\nbuilding physical reference stations. Correction information is computed using\ndata from public GNSS SSR data services and transmitted to users by Radio\nTechnical Commission for Maritime Services (RTCM) Observation Space\nRepresentation (OSR) messages which are accepted by most receivers. The\nreal-time stationary and moving platform testing performance, using u-blox M8P\nand ZED-F9P receivers, surpasses the Society of Automotive Engineering (SAE)\nspecification (68% of horizontal error $\\leqslant$ 1.5 m and vertical error\n$\\leqslant$ 3 m) and shows significantly better horizontal performance than\nGNSS Open Service (OS). The moving tests also show better horizontal\nperformance than the ZEDF9P receiver with Satellite Based Augmentation Systems\n(SBAS) enabled and achieve the lane-level accuracy which requires 95% of\nhorizontal errors less than 1 meter.', 'Recent channel state information (CSI)-based positioning pipelines rely on\ndeep neural networks (DNNs) in order to learn a mapping from estimated CSI to\nposition. Since real-world communication transceivers suffer from hardware\nimpairments, CSI-based positioning systems typically rely on features that are\ndesigned by hand. In this paper, we propose a CSI-based positioning pipeline\nthat directly takes raw CSI measurements and learns features using a structured\nDNN in order to generate probability maps describing the likelihood of the\ntransmitter being at pre-defined grid points. To further improve the\npositioning accuracy of moving user equipments, we propose to fuse a\ntime-series of learned CSI features or a time-series of probability maps. To\ndemonstrate the efficacy of our methods, we perform experiments with real-world\nindoor line-of-sight (LoS) and non-LoS channel measurements. We show that CSI\nfeature learning and time-series fusion can reduce the mean distance error by\nup to 2.5$\\boldsymbol\\times$ compared to the state-of-the-art.', 'Deep Neural Networks (DNNs) are a promising tool for Global Navigation\nSatellite System (GNSS) positioning in the presence of multipath and\nnon-line-of-sight errors, owing to their ability to model complex errors using\ndata. However, developing a DNN for GNSS positioning presents various\nchallenges, such as 1) poor numerical conditioning caused by large variations\nin measurements and position values across the globe, 2) varying number and\norder within the set of measurements due to changing satellite visibility, and\n3) overfitting to available data. In this work, we address the aforementioned\nchallenges and propose an approach for GNSS positioning by applying DNN-based\ncorrections to an initial position guess. Our DNN learns to output the position\ncorrection using the set of pseudorange residuals and satellite line-of-sight\nvectors as inputs. The limited variation in these input and output values\nimproves the numerical conditioning for our DNN. We design our DNN architecture\nto combine information from the available GNSS measurements, which vary both in\nnumber and order, by leveraging recent advancements in set-based deep learning\nmethods. Furthermore, we present a data augmentation strategy for reducing\noverfitting in the DNN by randomizing the initial position guesses. We first\nperform simulations and show an improvement in the initial positioning error\nwhen our DNN-based corrections are applied. After this, we demonstrate that our\napproach outperforms a WLS baseline on real-world data. Our implementation is\navailable at github.com/Stanford-NavLab/deep_gnss.']"
116,12,116_uav_autonomous_drone_path,"['uav', 'autonomous', 'drone', 'path', 'free', 'uavs', 'aerial', 'obstacle', 'interception', 'obstacles']","['This paper presents the design, development, and testing of hardware-software\nsystems by the IISc-TCS team for Challenge 1 of the Mohammed Bin Zayed\nInternational Robotics Challenge 2020. The goal of Challenge 1 was to grab a\nball suspended from a moving and maneuvering UAV and pop balloons anchored to\nthe ground, using suitable manipulators. The important tasks carried out to\naddress this challenge include the design and development of a hardware system\nwith efficient grabbing and popping mechanisms, considering the restrictions in\nvolume and payload, design of accurate target interception algorithms using\nvisual information suitable for outdoor environments, and development of a\nsoftware architecture for dynamic multi-agent aerial systems performing complex\ndynamic missions. In this paper, a single degree of freedom manipulator\nattached with an end-effector is designed for grabbing and popping, and robust\nalgorithms are developed for the interception of targets in an uncertain\nenvironment. Vision-based guidance and tracking laws are proposed based on the\nconcept of pursuit engagement and artificial potential function. The software\narchitecture presented in this work proposes an Operation Management System\n(OMS) architecture that allocates static and dynamic tasks collaboratively\namong multiple UAVs to perform any given mission. An important aspect of this\nwork is that all the systems developed were designed to operate in completely\nautonomous mode. A detailed description of the architecture along with\nsimulations of complete challenge in the Gazebo environment and field\nexperiment results are also included in this work. The proposed\nhardware-software system is particularly useful for counter-UAV systems and can\nalso be modified in order to cater to several other applications.', 'Autonomous systems have played an important role in response to the Covid-19\npandemic. Notably, there have been multiple attempts to leverage Unmanned\nAerial Vehicles (UAVs) to disinfect surfaces. Although recent research suggests\nthat surface transmission is less significant than airborne transmission in the\nspread of Covid-19, surfaces and fomites can play, and have played, critical\nroles in the transmission of Covid-19 and many other viruses, especially in\nsettings such as child daycares, schools, offices, and hospitals. Employing\nUAVs for mass spray disinfection offers several potential advantages, including\nhigh-throughput application of disinfectant, large scale deployment, and the\nminimization of health risks to sanitation workers. Despite these potential\nbenefits and preliminary usage of UAVs for disinfection, there has been little\nresearch into their design and effectiveness. In this work, we present an\nautonomous UAV capable of effectively disinfecting indoor surfaces. We identify\nrelevant parameters such as disinfectant type and concentration, and\napplication time and distance required of the UAV to disinfect high-touch\nsurfaces such as door handles. Finally, we develop a robotic system that\nenables the fully autonomous disinfection of door handles in an unstructured\nand previously unknown environment. To our knowledge, this is the smallest\nuntethered UAV ever built with both full autonomy and spraying capabilities,\nallowing it to operate in confined indoor settings, and the first autonomous\nUAV to specifically target high-touch surfaces on an individual basis with\nspray disinfectant, resulting in more efficient use of disinfectant', 'Unmanned aerial vehicles (UAVs) have become very popular for many military\nand civilian applications including in agriculture, construction, mining,\nenvironmental monitoring, etc. A desirable feature for UAVs is the ability to\nnavigate and perform tasks autonomously with least human interaction. This is a\nvery challenging problem due to several factors such as the high complexity of\nUAV applications, operation in harsh environments, limited payload and onboard\ncomputing power and highly nonlinear dynamics. The work presented in this\nreport contributes towards the state-of-the-art in UAV control for safe\nautonomous navigation and motion coordination of multi-UAV systems. The first\npart of this report deals with single-UAV systems. The complex problem of\nthree-dimensional (3D) collision-free navigation in unknown/dynamic\nenvironments is addressed. To that end, advanced 3D reactive control strategies\nare developed adopting the sense-and-avoid paradigm to produce quick reactions\naround obstacles. A special case of navigation in 3D unknown confined\nenvironments (i.e. tunnel-like) is also addressed. General 3D kinematic models\nare considered in the design which makes these methods applicable to different\nUAV types in addition to underwater vehicles. Moreover, different\nimplementation methods for these strategies with quadrotor-type UAVs are also\ninvestigated considering UAV dynamics in the control design. Practical\nexperiments and simulations were carried out to analyze the performance of the\ndeveloped methods. The second part of this report addresses safe navigation for\nmulti-UAV systems. Distributed motion coordination methods of multi-UAV systems\nfor flocking and 3D area coverage are developed. These methods offer good\ncomputational cost for large-scale systems. Simulations were performed to\nverify the performance of these methods considering systems with different\nsizes.']"
117,12,117_game_games_randomness_player,"['game', 'games', 'randomness', 'player', 'ai', 'persona', 'human', 'apf', 'agents', 'agent']","[""Video Games are boring when they are too easy, and frustrating when they are\ntoo hard. In terms of providing game experience such as enjoyment to the player\nby match players with different levels of ability to player ability, We assume\nthat implementing DDA for providing matches between player ability and overall\ngame difficulty to the game, especially the modern game, has limitations in\nterms of increasing computational cost and complexities in the design of\nmodeling the difficulty in modern games. To overcome limitations underlying the\nmethod of providing static difficulty changes to player, and DDA, we proposed a\nnovel idea, Player Domination adjustment (PDA). The proposed idea is that to\ncontrol the AI's actions based on the player's inputs so as to adjust the\nplayer's dominant power (e.g. the AI recognizes the player's attack actions but\ndefends it in a wrong side to let the player incur damage to itself), which was\nproved as it leads to promotion of game-related self-efficacy in our work.\nSeveral pieces of research on were conducted on a social deduction game and a\nfighting game respectively, show our proposed idea has its potential of\npromoting User Experience(UX). As in an another study, outperforms DDA in two\nconducted experiments in terms of health promotion."", 'Randomness is an important factor in games, so much so that some games rely\nalmost purely on it for its outcomes and increase players\' engagement with\nthem. However, randomness can affect the game experience depending on when it\noccurs in a game, altering the chances of planning for a player. In this paper,\nwe refer to it as ""input-output randomness"". Input-output randomness is a\ncornerstone of collectable card games like Hearthstone, in which cards are\ndrawn randomly (input randomness) and have random effects when played (output\nrandomness). While the topic might have been commonly discussed by game\ndesigners and be present in many games, few empirical studies have been\nperformed to evaluate the effects of these different kinds of randomness on the\nplayers\' satisfaction. This research investigates the effects of input-output\nrandomness on collectable card games across four input-output randomness\nconditions. We have developed our own collectable card game and experimented\nwith the different kinds of randomness with the game. Our results suggest that\ninput randomness can significantly impact game satisfaction negatively.\nOverall, our results present helpful considerations on how and when to apply\nrandomness in game design when aiming for players\' satisfaction.', 'Currently, the development of computer games has shown a tremendous surge.\nThe ease and speed of internet access today have also influenced the\ndevelopment of computer games, especially computer games that are played\nonline. Internet technology has allowed computer games to be played in\nmultiplayer mode. Interaction between players in a computer game can be built\nin several ways, one of which is by providing balanced opponents. Opponents can\nbe developed using intelligent agents. On the other hand, research on\ndeveloping intelligent agents is also growing rapidly. In computer game\ndevelopment, one of the easiest ways to measure the performance of an\nintelligent agent is to develop a virtual environment that allows the\nintelligent agent to interact with other players. In this research, we try to\ndevelop an intelligent agent and virtual environment for the board game. To be\neasily accessible, the intelligent agent and virtual environment are then\ndeveloped into an Application Programming Interface (API) service called\nGapoera API. The Gapoera API service that is built is expected to help game\ndevelopers develop a game without having to think much about the artificial\nintelligence that will be embedded in the game. This service provides a basic\nmultilevel intelligent agent that can provide users with playing board games\ncommonly played in Indonesia. Although the Gapoera API can be used for various\ntypes of games, in this paper, we will focus on the discussion on a popular\ntraditional board game in Indonesia, namely Mancala. The test results conclude\nthat the multilevel agent concept developed has worked as expected. On the\nother hand, the development of the Gapoera API service has also been\nsuccessfully accessed on several game platforms.']"
118,12,118_precipitation_forecasting_fade_rain,"['precipitation', 'forecasting', 'fade', 'rain', 'prediction', 'forecasts', 'solar', 'apsim', 'nowcasting', 'emulation']","['Time series forecasting has always been a hot spot in scientific research.\nWith the development of artificial intelligence, new time series forecasting\nmethods have obtained better forecasting effects and forecasting performance\nthrough bionic research and improvements to the past methods. Visibility Graph\n(VG) algorithm is often used for time series prediction in previous research,\nbut the prediction effect is not as good as deep learning prediction methods\nsuch as Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and\nLong Short-Term Memory Network (LSTM) prediction. The VG algorithm contains a\nwealth of network information, but previous studies did not effectively use the\nnetwork information to make predictions, resulting in relatively large\nprediction errors. In order to solve this problem, this paper proposes the Deep\nVisibility Series (DVS) module through the bionic design of VG and the\nexpansion of the past research, which is the first time to combine VG with\nbionic design and deep network. By applying the bionic design of biological\nvision to VG, the time series of DVS has obtained superior forecast accuracy,\nwhich has made a contribution to time series forecasting. At the same time,\nthis paper applies the DVS forecasting method to the construction cost index\nforecast, which has practical significance.', ""Hydroelectricity is one of the renewable energy source, has been used for\nmany years in Turkey. The production of hydraulic power plants based on water\nreservoirs varies based on different parameters. For this reason, the\nestimation of hydraulic production gains importance in terms of the planning of\nelectricity generation. In this article, the estimation of Turkey's monthly\nhydroelectricity production has been made with the long-short-term memory\n(LSTM) network-based deep learning model. The designed deep learning model is\nbased on hydraulic production time series and future production planning for\nmany years. By using real production data and different LSTM deep learning\nmodels, their performance on the monthly forecast of hydraulic electricity\ngeneration of the next year has been examined. The obtained results showed that\nthe use of time series based on real production data for many years and deep\nlearning model together is successful in long-term prediction. In the study, it\nis seen that the 100-layer LSTM model, in which 120 months (10 years)\nhydroelectric generation time data are used according to the RMSE and MAPE\nvalues, are the highest model in terms of estimation accuracy, with a MAPE\nvalue of 0.1311 (13.1%) in the annual total and 1.09% as the monthly average\ndistribution. In this model, the best results were obtained for the 100-layer\nLSTM model, in which the time data of 144 months (12 years) hydroelectric\ngeneration data are used, with a RMSE value of 29,689 annually and 2474.08 in\nmonthly distribution. According to the results of the study, time data covering\nat least 120 months of production is recommended to create an acceptable\nhydropower forecasting model with LSTM."", 'Accurate and timely estimation of precipitation is critical for issuing\nhazard warnings (e.g., for flash floods or landslides). Current remotely sensed\nprecipitation products have a few hours of latency, associated with the\nacquisition and processing of satellite data. By applying a robust nowcasting\nsystem to these products, it is (in principle) possible to reduce this latency\nand improve their applicability, value, and impact. However, the development of\nsuch a system is complicated by the chaotic nature of the atmosphere, and the\nconsequent rapid changes that can occur in the structures of precipitation\nsystems In this work, we develop two approaches (hereafter referred to as\nNowcasting-Nets) that use Recurrent and Convolutional deep neural network\nstructures to address the challenge of precipitation nowcasting. A total of\nfive models are trained using Global Precipitation Measurement (GPM) Integrated\nMulti-satellitE Retrievals for GPM (IMERG) precipitation data over the Eastern\nContiguous United States (CONUS) and then tested against independent data for\nthe Eastern and Western CONUS. The models were designed to provide forecasts\nwith a lead time of up to 1.5 hours and, by using a feedback loop approach, the\nability of the models to extend the forecast time to 4.5 hours was also\ninvestigated. Model performance was compared against the Random Forest (RF) and\nLinear Regression (LR) machine learning methods, and also against a persistence\nbenchmark (BM) that used the most recent observation as the forecast.\nIndependent IMERG observations were used as a reference, and experiments were\nconducted to examine both overall statistics and case studies involving\nspecific precipitation events. Overall, the forecasts provided by the\nNowcasting-Net models are superior, with the Convolutional Nowcasting Network\nwith Residual Head (CNC-R) achieving 25%, 28%, and 46% improvement in the test\n...']"
119,12,119_frames_sensing_sampling_rate,"['frames', 'sensing', 'sampling', 'rate', 'sequences', 'window', 'coherence', 'bandwidth', 'tight', 'sequence']","['The correlation properties of sequences form a focal point in the design of\nmultiple access systems of communications. Such a system must be able to serve\na number of simultaneous users while keeping interference low. A popular choice\nfor the set of sequences to deploy is the quasi-complementary sequence set\n(QCSS). Its large set size enables the system to accommodate a lot of users.\nThe set has low nontrivial correlation magnitudes within a zone around the\norigin. This keeps undue interference among users under control. A QCSS\nperforms better than the perfect complementary sequence set (PCSS) does in\nschemes with fractional delays.\n  The optimality of a set of periodic sequences is measured by its maximum\nperiodic correlation magnitude, for which there is an established lower bound\nto aim at. For a fixed period, optimal sets are known only for very restricted\nparameters. Efforts have therefore been centered around the constructions of\nasymptotically optimal sets. Their periods are allowed to be as large as\nsufficient to establish optimality. In this paper we share an insight that a\nsequence set that asymptotically attains the Welch bound generates an\nasymptotically optimal periodic QCSS by interleaving. One can simply use known\nfamilies of such sequence sets to construct the desired QCSSs. Seven families\nof QCSSs with specific parameters are shown as examples of this general\nconstruction. We build upon the insight to propose two new direct constructions\nof asymptotically optimal QCSSs with very flexible parameters without\ninterleaving. The flexibility enhances their appeal for practical\nimplementation. The mathematical tools come from the theory of groups in the\nform of additive and multiplicative characters of finite fields.', 'Digital filters with variable bandwidth can be used for a variety of\napplications. Arbitrary change in the bandwidth of a digital Finite Impulse\nResponse (FIR) filter can be acquired using sampling rate converters. In this\npaper, a sampling rate converter is proposed which is generated from Pascal\nstructure, a fractional delay filter having low hardware complexity and high\nmodularity. The proposed sampling rate converter requires lesser number of\nmultipliers for implementation when compared with the sampling rate converters\nin the literature. A low pass filter having a single bandwidth sandwiched\nbetween two sampling rate converters can contribute multiple bandwidths in such\na way that each bandwidth is an arbitrary variation of the original bandwidth.\nA two stage Frequency response masking approach (FRM) is used for the hardware\nefficient design of the original low pass filter. A low complexity and high\nmodular novel design for a continuously varying bandwidth of a digital FIR\nfilter is proposed in this paper using the proposed sampling rate converter.\nThe modularity of the Pascal structure can be used to control both pass band\nripple as well as stop band attenuation of the continuously variable bandwidth\nFIR filter design. Different communication standards in a Software defined\nradio (SDR) channelizer is realized using the proposed design of continuously\nvariable bandwidth filter.', 'An Equiangular tight frame (ETF) - also known as the Welch-bound-equality\nsequences - consists of a sequence of unit norm vectors whose absolute inner\nproduct is identical and minimal. Due to this unique property, these frames are\npreferred in different applications such as in constructing sensing matrices\nfor compressed sensing systems, robust transmission, and quantum computing.\nConstruction of ETFs involves solving a challenging non-convex minimax\noptimization problem, and only a few methods were successful in constructing\nthem, albeit only for smaller dimensions. In this paper, we propose an\niterative algorithm named TEchnique to devise Large dimensional Equiangular\nTight-frames (TELET-frames) based on the majorization minimization (MM)\nprocedure - in which we design and minimize a tight upper bound for the ETF\ncost function at every iteration. Since TELET is designed using the MM\napproach, it inherits useful properties of MM such as monotonicity and\nguaranteed convergence to a stationary point. Subsequently, we use the derived\nframes to construct optimized sensing matrix for compressed sensing systems. In\nthe numerical simulations, we show that the proposed algorithm can generate\ncomplex and real frames (in the order of hundreds) with very low mutual\ncoherence value when compared to the state-of-the-art algorithm, with a slight\nincrease in computational cost. Experiments using synthetic data and real\nimages reveal that the optimized sensing matrix obtained through the frames\nconstructed by TELET performs better, in terms of image reconstruction\naccuracy, than the sensing matrix constructed using state-of-the-art methods.']"
120,12,120_label_labels_noisy_ssl,"['label', 'labels', 'noisy', 'ssl', 'labeled', 'noise', 'samples', 'labeling', 'decaf', 'deep']","['Noisy labels are commonly found in real-world data, which cause performance\ndegradation of deep neural networks. Cleaning data manually is labour-intensive\nand time-consuming. Previous research mostly focuses on enhancing\nclassification models against noisy labels, while the robustness of deep metric\nlearning (DML) against noisy labels remains less well-explored. In this paper,\nwe bridge this important gap by proposing Probabilistic Ranking-based Instance\nSelection with Memory (PRISM) approach for DML. PRISM calculates the\nprobability of a label being clean, and filters out potentially noisy samples.\nSpecifically, we propose three methods to calculate this probability: 1)\nAverage Similarity Method (AvgSim), which calculates the average similarity\nbetween potentially noisy data and clean data; 2) Proxy Similarity Method\n(ProxySim), which replaces the centers maintained by AvgSim with the proxies\ntrained by proxy-based method; and 3) von Mises-Fisher Distribution Similarity\n(vMF-Sim), which estimates a von Mises-Fisher distribution for each data class.\nWith such a design, the proposed approach can deal with challenging DML\nsituations in which the majority of the samples are noisy. Extensive\nexperiments on both synthetic and real-world noisy dataset show that the\nproposed approach achieves up to 8.37% higher Precision@1 compared with the\nbest performing state-of-the-art baseline approaches, within reasonable\ntraining time.', 'Long-tailed learning has attracted much attention recently, with the goal of\nimproving generalisation for tail classes. Most existing works use supervised\nlearning without considering the prevailing noise in the training dataset. To\nmove long-tailed learning towards more realistic scenarios, this work\ninvestigates the label noise problem under long-tailed label distribution. We\nfirst observe the negative impact of noisy labels on the performance of\nexisting methods, revealing the intrinsic challenges of this problem. As the\nmost commonly used approach to cope with noisy labels in previous literature,\nwe then find that the small-loss trick fails under long-tailed label\ndistribution. The reason is that deep neural networks cannot distinguish\ncorrectly-labeled and mislabeled examples on tail classes. To overcome this\nlimitation, we establish a new prototypical noise detection method by designing\na distance-based metric that is resistant to label noise. Based on the above\nfindings, we propose a robust framework,~\\algo, that realizes noise detection\nfor long-tailed learning, followed by soft pseudo-labeling via both label\nsmoothing and diverse label guessing. Moreover, our framework can naturally\nleverage semi-supervised learning algorithms to further improve the\ngeneralisation. Extensive experiments on benchmark and real-world datasets\ndemonstrate the superiority of our methods over existing baselines. In\nparticular, our method outperforms DivideMix by 3\\% in test accuracy. Source\ncode will be released soon.', 'The scarcity of labeled data is a critical obstacle to deep learning.\nSemi-supervised learning (SSL) provides a promising way to leverage unlabeled\ndata by pseudo labels. However, when the size of labeled data is very small\n(say a few labeled samples per class), SSL performs poorly and unstably,\npossibly due to the low quality of learned pseudo labels. In this paper, we\npropose a new SSL method called DP-SSL that adopts an innovative data\nprogramming (DP) scheme to generate probabilistic labels for unlabeled data.\nDifferent from existing DP methods that rely on human experts to provide\ninitial labeling functions (LFs), we develop a multiple-choice learning~(MCL)\nbased approach to automatically generate LFs from scratch in SSL style. With\nthe noisy labels produced by the LFs, we design a label model to resolve the\nconflict and overlap among the noisy labels, and finally infer probabilistic\nlabels for unlabeled samples. Extensive experiments on four standard SSL\nbenchmarks show that DP-SSL can provide reliable labels for unlabeled data and\nachieve better classification performance on test sets than existing SSL\nmethods, especially when only a small number of labeled samples are available.\nConcretely, for CIFAR-10 with only 40 labeled samples, DP-SSL achieves 93.82%\nannotation accuracy on unlabeled data and 93.46% classification accuracy on\ntest data, which are higher than the SOTA results.']"
121,12,121_chains_stochastic_markov_inflation,"['chains', 'stochastic', 'markov', 'inflation', 'lmm', 'parisian', 'multiplicative', 'chain', 'pdmps', 'awh']","['Time-irreversible stochastic processes are frequently used in natural\nsciences to explain non-equilibrium phenomena and to design efficient\nstochastic algorithms. Our main goal in this thesis is to analyse their\ndynamics by means of large deviation theory. We focus on processes that become\ndeterministic in a certain limit, and characterize their fluctuations around\nthat deterministic limit by Lagrangian rate functions. Our main techniques for\nestablishing these characterizations rely on the connection between large\ndeviations and Hamilton-Jacobi equations. We sketch this connection with\nexamples in the introductory parts of this thesis.\n  The second part of the thesis is devoted to irreversible processes that are\nmotivated from molecular motors, Markov chain Monte Carlo (MCMC) methods and\nstochastic slow-fast systems. We characterize the asymptotic dynamics of\nmolecular motors by Hamiltonians defined in terms of principal-eigenvalue\nproblems. From our results about the zig-zag sampler used in MCMCs, we learn\nthat maximal irreversibility corresponds to an optimal rate of convergence. In\nstochastic slow-fast systems, our main theoretical contributions are techniques\nto work with the variational formulas of Hamiltonians that one encounters in\nmean-field systems coupled to fast diffusions.\n  In the final part of the thesis, we study a family of Fokker-Planck equations\nwhose solutions become singular in a certain limit. The associated\ngradient-flow structures do not converge since the relative entropies diverge\nin the limit. To remedy this, we propose to work with a different variational\nformulation that takes fluxes into account, which is motivated by density-flux\nlarge deviations.', ""When using Markov chain Monte Carlo (MCMC) algorithms, we can increase the\nnumber of samples either by running longer chains or by running more chains.\nPractitioners often prefer the first approach because chains need an initial\n``warmup'' phase to forget their initial states; the number of operations\nneeded for warmup is constant with respect to chain length but increases\nlinearly with the number of chains. However, highly parallel hardware\naccelerators such as GPUs may allow us to run many chains in parallel almost as\nquickly as a single chain. This makes it more attractive to run many chains\nwith a short sampling phase. Unfortunately, existing diagnostics are not\ndesigned for the ``many short chains'' regime. This is notably the case for the\npopular $\\hat R$ statistic which claims convergence only if the effective\nsample size \\textit{per chain} is sufficiently large. We present $\\mathfrak\nn\\hat R$, a generalization of $\\hat R$, which does not conflate short chains\nand poor mixing, and offers a useful diagnostic provided we run enough chains\nand meet certain initialization conditions. We define what constitutes a proper\nwarmup in the many-chains regime and recommend a threshold for $\\mathfrak n\n\\hat R$. Furthermore we use $\\mathfrak n \\hat R$ to construct a warmup scheme\nwith an adaptive length, allowing users to avoid running their MCMC algorithms\nlonger than necessary."", ""We study contractions of Markov chains on general metric spaces with respect\nto some carefully designed distance-like functions, which are comparable to the\ntotal variation and the standard $L^p$-Wasserstein distances for $p \\ge 1$. We\npresent explicit lower bounds of the corresponding contraction rates. By\nemploying the refined basic coupling and the coupling by reflection, the\nresults are applied to Markov chains whose transitions include additive\nstochastic noises that are not necessarily isotropic. This can be useful in the\nstudy of Euler schemes for SDEs driven by L\\'evy noises. In particular,\nmotivated by recent works on the use of heavy tailed processes in Markov Chain\nMonte Carlo, we show that chains driven by the $\\alpha$-stable noise can have\nbetter contraction rates than corresponding chains driven by the Gaussian\nnoise, due to the heavy tails of the $\\alpha$-stable distribution.""]"
122,11,122_emotion_recognition_emotions_fbp,"['emotion', 'recognition', 'emotions', 'fbp', 'stimuli', 'emotional', 'vea', 'eeg', 'visual', 'features']","[""Visual emotion analysis (VEA) has attracted great attention recently, due to\nthe increasing tendency of expressing and understanding emotions through images\non social networks. Different from traditional vision tasks, VEA is inherently\nmore challenging since it involves a much higher level of complexity and\nambiguity in human cognitive process. Most of the existing methods adopt deep\nlearning techniques to extract general features from the whole image,\ndisregarding the specific features evoked by various emotional stimuli.\nInspired by the \\textit{Stimuli-Organism-Response (S-O-R)} emotion model in\npsychological theory, we proposed a stimuli-aware VEA method consisting of\nthree stages, namely stimuli selection (S), feature extraction (O) and emotion\nprediction (R). First, specific emotional stimuli (i.e., color, object, face)\nare selected from images by employing the off-the-shelf tools. To the best of\nour knowledge, it is the first time to introduce stimuli selection process into\nVEA in an end-to-end network. Then, we design three specific networks, i.e.,\nGlobal-Net, Semantic-Net and Expression-Net, to extract distinct emotional\nfeatures from different stimuli simultaneously. Finally, benefiting from the\ninherent structure of Mikel's wheel, we design a novel hierarchical\ncross-entropy loss to distinguish hard false examples from easy ones in an\nemotion-specific manner. Experiments demonstrate that the proposed method\nconsistently outperforms the state-of-the-art approaches on four public visual\nemotion datasets. Ablation study and visualizations further prove the validity\nand interpretability of our method."", 'Multimodal emotion recognition is a challenging task in emotion computing as\nit is quite difficult to extract discriminative features to identify the subtle\ndifferences in human emotions with abstract concept and multiple expressions.\nMoreover, how to fully utilize both audio and visual information is still an\nopen problem. In this paper, we propose a novel multimodal fusion attention\nnetwork for audio-visual emotion recognition based on adaptive and multi-level\nfactorized bilinear pooling (FBP). First, for the audio stream, a fully\nconvolutional network (FCN) equipped with 1-D attention mechanism and local\nresponse normalization is designed for speech emotion recognition. Next, a\nglobal FBP (G-FBP) approach is presented to perform audio-visual information\nfusion by integrating selfattention based video stream with the proposed audio\nstream. To improve G-FBP, an adaptive strategy (AG-FBP) to dynamically\ncalculate the fusion weight of two modalities is devised based on the\nemotion-related representation vectors from the attention mechanism of\nrespective modalities. Finally, to fully utilize the local emotion information,\nadaptive and multi-level FBP (AMFBP) is introduced by combining both\nglobal-trunk and intratrunk data in one recording on top of AG-FBP. Tested on\nthe IEMOCAP corpus for speech emotion recognition with only audio stream, the\nnew FCN method outperforms the state-ofthe-art results with an accuracy of\n71.40%. Moreover, validated on the AFEW database of EmotiW2019 sub-challenge\nand the IEMOCAP corpus for audio-visual emotion recognition, the proposed\nAM-FBP approach achieves the best accuracy of 63.09% and 75.49% respectively on\nthe test set.', 'Visual Emotion Analysis (VEA) aims at finding out how people feel emotionally\ntowards different visual stimuli, which has attracted great attention recently\nwith the prevalence of sharing images on social networks. Since human emotion\ninvolves a highly complex and abstract cognitive process, it is difficult to\ninfer visual emotions directly from holistic or regional features in affective\nimages. It has been demonstrated in psychology that visual emotions are evoked\nby the interactions between objects as well as the interactions between objects\nand scenes within an image. Inspired by this, we propose a novel Scene-Object\ninterreLated Visual Emotion Reasoning network (SOLVER) to predict emotions from\nimages. To mine the emotional relationships between distinct objects, we first\nbuild up an Emotion Graph based on semantic concepts and visual features. Then,\nwe conduct reasoning on the Emotion Graph using Graph Convolutional Network\n(GCN), yielding emotion-enhanced object features. We also design a Scene-Object\nFusion Module to integrate scenes and objects, which exploits scene features to\nguide the fusion process of object features with the proposed scene-based\nattention mechanism. Extensive experiments and comparisons are conducted on\neight public visual emotion datasets, and the results demonstrate that the\nproposed SOLVER consistently outperforms the state-of-the-art methods by a\nlarge margin. Ablation studies verify the effectiveness of our method and\nvisualizations prove its interpretability, which also bring new insight to\nexplore the mysteries in VEA. Notably, we further discuss SOLVER on three other\npotential datasets with extended experiments, where we validate the robustness\nof our method and notice some limitations of it.']"
123,11,123_satellite_constellations_satellites_constellation,"['satellite', 'constellations', 'satellites', 'constellation', 'ngso', 'leo', 'haps', 'space', 'terrestrial', 'gso']","['Digital connectivity has become the foundation of prosperity and an essential\nneed for functioning societies. Despite this dependence, limitation on Internet\naccess remains a prevalent issue, largely hinged on socioeconomic and\ngeographic factors. A promising solution to attain global access equality is\nbased on integrated terrestrial-satellite networks that rely on low Earth orbit\n(LEO) mega constellations. While the benefits of LEO constellations complement\nthe shortcomings of terrestrial networks, their incorporation impacts the\nnetwork design, adding complexity and challenges. This article presents a\nsystematic analysis of next generation LEO mega satellite constellations and\noutlines opportunities by virtue of the many benefits these constellations can\nprovide and highlights the major challenges. Furthermore, it provides a\nsynopsis of analytic models and underscores modern simulation approaches for\nnext generation mega satellite constellations. This provides network designers\nwith the necessary insights into satellite network performance.', 'Satellite networks are expected to support global connectivity and services\nvia future integrated 6G space-terrestrial networks (STNs), as well as private\nnon-geostationary satellite orbit (NGSO) constellations. In the past few years,\nmany such private constellations have been launched or are in planning, e.g.\nSpaceX and OneWeb to name a few. In this article we take a closer look at the\nprivate constellations and give a comprehensive overview of their features. We\nthen discuss major technical challenges resulting from their design and briefly\nreview the recent literature addressing these challenges. Studying the emerging\nprivate constellations gives us useful insights for engineering the future\nSTNs. To this end, we study the satellite mobility and evaluate the impact of\ntwo handover strategies on the space-to-ground link performance of four real\nprivate NGSO constellations. We show that the link capacity, delay, and\nhandover rate vary across the constellations, so the optimal handover strategy\ndepends on the constellation design. Consequently, the communications solutions\nof future STNs should be compliant with the constellation specifics, and the\nSTN standards need to be flexible enough to support satellite operation with\nthe large parameter space observed in the emerging private constellations.', 'The high number of objects in the LEO is a risk that collisions between\nsub-orbital or escape velocity objects with an orbiting object of satellites\noccur when two satellites collide while orbiting the earth. One of the\napproaches to avoid collisions is a robotic configuration of satellite\nconstellations. Satellite constellations should not be confused with satellite\nclusters, which are groups of satellites moving in close proximity to each\nother in nearly identical orbits; nor with satellite series or satellite\nprograms, which are generations of satellites launched successively; nor with\nsatellite fleets, which are groups of satellites from the same manufacturer or\noperator that operate an independent system. CfEOS constellations designed for\ngeospatial applications and Earth observation. Unlike a single satellite, a\nconstellation can provide permanent global or near-global coverage anywhere on\nEarth. CfEOS constellations are configured in sets of complementary orbital\nplanes and connect to ground stations located around the globe. This paper\ndescribes the GNC analysis, the orbit propagation and robotic systems\nconfiguration for Collision-free Earth observation satellites (CfEOS)\nconstellations.']"
124,11,124_ssl_lt_learning_cues,"['ssl', 'lt', 'learning', 'cues', 'supervised', 'view', 'negative', 'head', 'ipmc', 'pairs']","['Real-world visual recognition problems often exhibit long-tailed\ndistributions, where the amount of data for learning in different categories\nshows significant imbalance. Standard classification models learned on such\ndata distribution often make biased predictions towards the head classes while\ngeneralizing poorly to the tail classes. In this paper, we present two\neffective modifications of CNNs to improve network learning from long-tailed\ndistribution. First, we present a Class Activation Map Calibration (CAMC)\nmodule to improve the learning and prediction of network classifiers, by\nenforcing network prediction based on important image regions. The proposed\nCAMC module highlights the correlated image regions across data and reinforces\nthe representations in these areas to obtain a better global representation for\nclassification. Furthermore, we investigate the use of normalized classifiers\nfor representation learning in long-tailed problems. Our empirical study\ndemonstrates that by simply scaling the outputs of the classifier with an\nappropriate scalar, we can effectively improve the classification accuracy on\ntail classes without losing the accuracy of head classes. We conduct extensive\nexperiments to validate the effectiveness of our design and we set new\nstate-of-the-art performance on five benchmarks, including ImageNet-LT,\nPlaces-LT, iNaturalist 2018, CIFAR10-LT, and CIFAR100-LT.', 'Contrastive self-supervised learning (SSL) has achieved great success in\nunsupervised visual representation learning by maximizing the similarity\nbetween two augmented views of the same image (positive pairs) and\nsimultaneously contrasting other different images (negative pairs). However,\nthis type of methods, such as SimCLR and MoCo, relies heavily on a large number\nof negative pairs and thus requires either large batches or memory banks. In\ncontrast, some recent non-contrastive SSL methods, such as BYOL and SimSiam,\nattempt to discard negative pairs by introducing asymmetry and show remarkable\nperformance. Unfortunately, to avoid collapsed solutions caused by not using\nnegative pairs, these methods require sophisticated asymmetry designs. In this\npaper, we argue that negative pairs are still necessary but one is sufficient,\ni.e., triplet is all you need. A simple triplet-based loss can achieve\nsurprisingly good performance without requiring large batches or asymmetry.\nMoreover, we observe that unsupervised visual representation learning can gain\nsignificantly from randomness. Based on this observation, we propose a simple\nplug-in RandOm MApping (ROMA) strategy by randomly mapping samples into other\nspaces and enforcing these randomly projected samples to satisfy the same\ncorrelation requirement. The proposed ROMA strategy not only achieves the\nstate-of-the-art performance in conjunction with the triplet-based loss, but\nalso can further effectively boost other SSL methods.', 'One-stage long-tailed recognition methods improve the overall performance in\na ""seesaw"" manner, i.e., either sacrifice the head\'s accuracy for better tail\nclassification or elevate the head\'s accuracy even higher but ignore the tail.\nExisting algorithms bypass such trade-off by a multi-stage training process:\npre-training on imbalanced set and fine-tuning on balanced set. Though\nachieving promising performance, not only are they sensitive to the\ngeneralizability of the pre-trained model, but also not easily integrated into\nother computer vision tasks like detection and segmentation, where pre-training\nof classifiers solely is not applicable. In this paper, we propose a one-stage\nlong-tailed recognition scheme, ally complementary experts (ACE), where the\nexpert is the most knowledgeable specialist in a sub-set that dominates its\ntraining, and is complementary to other experts in the less-seen categories\nwithout being disturbed by what it has never seen. We design a\ndistribution-adaptive optimizer to adjust the learning pace of each expert to\navoid over-fitting. Without special bells and whistles, the vanilla ACE\noutperforms the current one-stage SOTA method by 3-10% on CIFAR10-LT,\nCIFAR100-LT, ImageNet-LT and iNaturalist datasets. It is also shown to be the\nfirst one to break the ""seesaw"" trade-off by improving the accuracy of the\nmajority and minority categories simultaneously in only one stage. Code and\ntrained models are at https://github.com/jrcai/ACE.']"
125,11,125_action_temporal_recognition_oad,"['action', 'temporal', 'recognition', 'oad', 'video', 'ien', 'rgb', 'short', 'long', 'spatio']","['Online action detection (OAD) is a task that receives video segments within a\nstreaming video as inputs and identifies ongoing actions within them. It is\nimportant to retain past information associated with a current action. However,\nlong short-term memory (LSTM), a popular recurrent unit for modeling temporal\ninformation from videos, accumulates past information from the previous hidden\nand cell states and the extracted visual features at each timestep without\nconsidering the relationships between the past and current information.\nConsequently, the forget gate of the original LSTM can lose the accumulated\ninformation relevant to the current action because it determines which\ninformation to forget without considering the current action. We introduce a\nnovel information elevation unit (IEU) that lifts up and accumulate the past\ninformation relevant to the current action in order to model the past\ninformation that is especially relevant to the current action. To the best of\nour knowledge, our IEN is the first attempt that considers the computational\noverhead for the practical use of OAD. Through ablation studies, we design an\nefficient and effective OAD network using IEUs, called an information elevation\nnetwork (IEN). Our IEN uses visual features extracted by a fast action\nrecognition network taking only RGB frames because extracting optical flows\nrequires heavy computation overhead. On two OAD benchmark datasets, THUMOS-14\nand TVSeries, our IEN outperforms state-of-the-art OAD methods using only RGB\nframes. Furthermore, on the THUMOS-14 dataset, our IEN outperforms the\nstate-of-the-art OAD methods using two-stream features based on RGB frames and\noptical flows.', 'Temporal Action Detection (TAD) is an essential and challenging topic in\nvideo understanding, aiming to localize the temporal segments containing human\naction instances and predict the action categories. The previous works greatly\nrely upon dense candidates either by designing varying anchors or enumerating\nall the combinations of boundaries on video sequences; therefore, they are\nrelated to complicated pipelines and sensitive hand-crafted designs. Recently,\nwith the resurgence of Transformer, query-based methods have tended to become\nthe rising solutions for their simplicity and flexibility. However, there still\nexists a performance gap between query-based methods and well-established\nmethods. In this paper, we identify the main challenge lies in the large\nvariants of action duration and the ambiguous boundaries for short action\ninstances; nevertheless, quadratic-computational global attention prevents\nquery-based methods to build multi-scale feature maps. Towards high-quality\ntemporal action detection, we introduce Sparse Proposals to interact with the\nhierarchical features. In our method, named SP-TAD, each proposal attends to a\nlocal segment feature in the temporal feature pyramid. The local interaction\nenables utilization of high-resolution features to preserve action instances\ndetails. Extensive experiments demonstrate the effectiveness of our method,\nespecially under high tIoU thresholds. E.g., we achieve the state-of-the-art\nperformance on THUMOS14 (45.7% on mAP@0.6, 33.4% on mAP@0.7 and 53.5% on\nmAP@Avg) and competitive results on ActivityNet-1.3 (32.99% on mAP@Avg). Code\nwill be made available at https://github.com/wjn922/SP-TAD.', 'In this paper, we place the atomic action detection problem into a Long-Short\nTerm Context (LSTC) to analyze how the temporal reliance among video signals\naffect the action detection results. To do this, we decompose the action\nrecognition pipeline into short-term and long-term reliance, in terms of the\nhypothesis that the two kinds of context are conditionally independent given\nthe objective action instance. Within our design, a local aggregation branch is\nutilized to gather dense and informative short-term cues, while a high order\nlong-term inference branch is designed to reason the objective action class\nfrom high-order interaction between actor and other person or person pairs.\nBoth branches independently predict the context-specific actions and the\nresults are merged in the end. We demonstrate that both temporal grains are\nbeneficial to atomic action recognition. On the mainstream benchmarks of atomic\naction detection, our design can bring significant performance gain from the\nexisting state-of-the-art pipeline. The code of this project can be found at\n[this url](https://github.com/TencentYoutuResearch/ActionDetection-LSTC)']"
126,11,126_estimator_wage_inequality_income,"['estimator', 'wage', 'inequality', 'income', 'regression', 'estimators', 'earnings', 'worked', 'workers', 'variance']","['Truncated conditional expectation functions are objects of interest in a wide\nrange of economic applications, including income inequality measurement,\nfinancial risk management, and impact evaluation. They typically involve\ntruncating the outcome variable above or below certain quantiles of its\nconditional distribution. In this paper, based on local linear methods, a\nnovel, two-stage, nonparametric estimator of such functions is proposed. In\nthis estimation problem, the conditional quantile function is a nuisance\nparameter that has to be estimated in the first stage. The proposed estimator\nis insensitive to the first-stage estimation error owing to the use of a\nNeyman-orthogonal moment in the second stage. This construction ensures that\ninference methods developed for the standard nonparametric regression can be\nreadily adapted to conduct inference on truncated conditional expectations. As\nan extension, estimation with an estimated truncation quantile level is\nconsidered. The proposed estimator is applied in two empirical settings: sharp\nregression discontinuity designs with a manipulated running variable and\nrandomized experiments with sample selection.', ""Income inequality is a distributional phenomenon. This paper examines the\nimpact of U.S governor's party allegiance (Republican vs Democrat) on ethnic\nwage gap. A descriptive analysis of the distribution of yearly earnings of\nWhites and Blacks reveals a divergence in their respective shapes over time\nsuggesting that aggregate analysis may mask important heterogeneous effects.\nThis motivates a granular estimation of the comparative causal effect of\ngovernors' party affiliation on labor market outcomes. We use a regression\ndiscontinuity design (RDD) based on marginal electoral victories and samples of\nquantiles groups by wage and hours worked. Overall, the distributional causal\nestimations show that the vast majority of subgroups of black workers earnings\nare not affected by democrat governors' policies, suggesting the possible\nexistence of structural factors in the labor markets that contribute to create\nand keep a wage trap and/or hour worked trap for most of the subgroups of black\nworkers. Democrat governors increase the number of hours worked of black\nworkers at the highest quartiles of earnings. A bivariate quantiles groups\nanalysis shows that democrats decrease the total hours worked for black workers\nwho have the largest number of hours worked and earn the least. Black workers\nearning more and working fewer hours than half of the sample see their number\nof hours worked increase under a democrat governor."", ""During recent crisis, wage subsidies played a major role in sheltering firms\nand households from economic shocks. During COVID-19, most workers were\naffected and many liberal welfare states introduced new temporary wage\nsubsidies to protected workers' earnings and employment (OECD, 2021). New wage\nsubsidies marked a departure from the structure of traditional income support\npayments and required reform. This paper uses simulated datasets to assess the\nstructure and incentives of the Irish COVID-19 wage subsidy scheme (CWS) under\nfive designs. We use a nowcasting approach to update 2017 microdata, producing\na near real time picture of the labour market at the peak of the crisis. Using\nmicrosimulation modelling, we assess the impact of different designs on income\nreplacement, work incentives and income inequality. Our findings suggest that\npro rata designs support middle earners more and flat rate designs support low\nearners more. We find evidence for strong work disincentives under all designs,\nthough flat rate designs perform better. Disincentives are primarily driven by\ngenerous unemployment payments and work related costs. The impact of design on\nincome inequality depends on the generosity of payments. Earnings related pro\nrata designs were associated to higher market earnings inequality. The\ndifference in inequality levels falls once benefits, taxes and work related\ncosts are considered. In our discussion, we turn to transaction costs, the\nrationale for reform and reintegration of CWS. We find some support for the\nclaim that design changes were motivated by political considerations. We\nsuggest that establishing permanent wage subsidies based on sectorial turnover\nrules could offer enhanced protection to middle-and high-earners and reduce\nuncertainty, the need for reform, and the risk of politically motivated\ndesigns.""]"
127,11,127_aerodynamic_airfoil_wings_stall,"['aerodynamic', 'airfoil', 'wings', 'stall', 'reynolds', 'flow', 'swept', 'trailing', 'unsteady', 'drag']","[""Figuring out the right airfoil is a crucial step in the preliminary stage of\nany aerial vehicle design, as its shape directly affects the overall\naerodynamic characteristics of the aircraft or rotorcraft. Besides being a\nmeasure of performance, the aerodynamic coefficients are used to design\nadditional subsystems such as a flight control system, or predict complex\ndynamic phenomena such as aeroelastic instability. The coefficients in question\ncan either be obtained experimentally through wind tunnel testing or, depending\nupon the accuracy requirements, by numerically simulating the underlying\nfundamental equations of fluid dynamics. In this paper, the feasibility of\napplying Artificial Neural Networks (ANNs) to estimate the aerodynamic\ncoefficients of differing airfoil geometries at varying Angle of Attack, Mach\nand Reynolds number is investigated. The ANNs are computational entities that\nhave the ability to learn highly nonlinear spatial and temporal patterns.\nTherefore, they are increasingly being used to approximate complex real-world\nphenomenon. However, despite their significant breakthrough in the past few\nyears, ANNs' spreading in the field of Computational Fluid Dynamics (CFD) is\nfairly recent, and many applications within this field remain unexplored. This\nstudy thus compares different network architectures and training datasets in an\nattempt to gain insight as to how the network perceives the given airfoil\ngeometries, while producing an acceptable neuronal model for faster and easier\nprediction of lift, drag and moment coefficients in steady state,\nincompressible flow regimes. This data-driven method produces sufficiently\naccurate results, with the added benefit of saving high computational and\nexperimental costs."", 'Forward-swept wings offer unique advantages in the aerodynamic performance of\nair vehicles. However, the low-Reynolds-number characteristics of such wings\nhave not been explored in the past. In this work, we numerically study laminar\nseparated flows over forward-swept wings with semi aspect ratios $sAR=0.5$ to 2\nat a chord-based Reynolds number of 400. Forward-swept wings generate wakes\nthat are significantly different from those of backward-swept wings. For\nlow-aspect-ratio forward wings, the wakes remain steady due to the strong\ndownwash effects induced by the tip vortices. For larger aspect ratio, the\ndownwash effects weaken over the inboard regions of the wing, allowing unsteady\nvortex shedding to occur. Further larger aspect ratio allows for the formation\nof streamwise vortices for highly-swept wings, stabilizing the flow.\nForward-swept wings can generate enhanced lift at high angles of attack than\nthe unswept and backward-swept wings, with the cost of high drag. We show\nthrough force element analysis that the increased lift of forward-swept wings\nis attributed to the vortical structure that is maintained by the\ntip-vortex-induced downwash over the outboard region of wing span. The current\nfindings offer a detailed understanding of the sweep effects on laminar\nseparated flows over forward-swept wings, and invite innovative designs of\nhigh-lift devices.', 'Morphing wings comprised of fixed leading sections with piezocomposite\ntrailing control surfaces have emerged as a novel active control technique for\nunmanned aerial vehicles. However, the wake dynamics and aerodynamic\nperformance of such hybrid airfoil configuration has not been thoroughly\ninvestigated. In this paper, direct numerical simulations of two-dimensional\nflows over hybrid airfoils comprised of NACA 0012 leading sections with\npiezocomposite trailing control surfaces are performed at a fixed Reynolds\nnumber of 1000. The effects of length and camber of the trailing control\nsurface on the laminar aerodynamic characteristics are studied over a wide\nrange of angle of attack. It is shown that the flow behind the airfoil exhibits\ndifferent features, including steady flow, periodic vortex shedding, and\nquasi-periodic vortex shedding for different configurations. The transition\nbetween these wake states occurs at slightly smaller angles of attack compared\nto the nominal NACA 0012 airfoil. While the drag coefficient remains close to\neach other at a fixed angle of attack, the lift coefficient of the hybrid\nairfoil is positively affected by the length and camber of the trailing control\nsurface. The mechanism of lift generation is examined by surface pressure\ndistributions and a force element analysis. It is revealed that with increased\ncamber of the trailing control surface, the flow on both the suction and\npressure sides of the airfoil are modified in a beneficial way to enhance lift.\nIncreasing the length ratio only significantly modifies the flow near the aft\nsection on the pressure side. The results herein provide a laminar aerodynamic\ncharacterization of hybrid airfoils with trailing control surfaces, and could\npotentially aid the design of control techniques for next-generation small\nunmanned aerial vehicles.']"
128,11,128_fermi_quantum_rydberg_superfluid,"['fermi', 'quantum', 'rydberg', 'superfluid', 'ising', 'bosons', 'gauge', 'fermions', 'site', 'emergent']","['We design a quantum battery made up of bosons or fermions in an ultracold\natom setup, described by Fermi-Hubbard (FH) and Bose-Hubbard (BH) models\nrespectively. We compare the performance of bosons as well as fermions and\ncheck which can act more efficiently as a quantum battery for a given on-site\ninteraction and temperature of the initial state. The performance of a quantum\nbattery is quantified by the maximum power generated over the time evolution\nunder an on-site charging Hamiltonian. We report that when the initial battery\nstate is in the ground state, fermions outperform bosons in a certain\nconfiguration over a large range of on-site interactions which are shown\nanalytically for a smaller number of lattice sites and numerically for a\nconsiderable number of sites. Bosons take the lead when the temperature is\ncomparatively high in the initial state for a longer range of on-site\ninteraction. We perform the study of a number of up and down fermions as well\nas the number of bosons per site to find the optimal filling factor for\nmaximizing the power of the battery. We also introduce disorder in both on-site\nand hopping parameters and demonstrate that the maximum power is robust against\nimpurities. Moreover, we identify a range of tuning parameters in the fermionic\nas well as bosonic systems where the disorder-enhanced power is observed.', ""We develop a theory to describe the dynamics of a driven-dissipative\nmany-body Fermi system, to pursue our proposal to realize exotic quantum states\nbased on reservoir engineering. Our idea is to design the shape of a Fermi\nsurface so as to have multiple Fermi edges, by properly attaching multiple\nreservoirs with different chemical potentials to a fermionic system. These\nemerged edges give rise to additional scattering channels that can destabilize\nthe system into unconventional states, which is exemplified in this work by\nconsidering a driven-dissipative attractively interacting Fermi gas. By\nformulating a quantum kinetic equation using the Nambu-Keldysh Green's function\ntechnique, we determine the nonequilibrium steady state of this system and\nassess its stability. We find that, besides the BCS-type isotropic pairing\nstate, an anisotropic superfluid state being accompanied by Cooper pairs with\nnon-zero center-of-mass momentum exists as a stable solution, even in the\nabsence of a magnetic Zeeman field. Our result implies a great potential of\nrealizing quantum matter beyond the equilibrium paradigm, by engineering the\nshape and topology of Fermi surfaces in both electronic and atomic systems."", 'We theoretically propose an idea based on reservoir engineering to process\nthe structure of a Fermi edge to split into multiple Fermi edges, so as to be\nsuitable for the state which we want to realize. When one appropriately tunes\nthe chemical-potential difference between two reservoirs being coupled with the\nsystem, the system is shown to be in the non-equilibrium steady state with the\nmomentum distribution having a two-edge structure. We argue that these edges\nplay similar roles to two Fermi surfaces, which can be designed to realize\nexotic quantum many-body states. To demonstrate this, we consider a model\ndriven-dissipative two-component Fermi gas with an attractive interaction as a\nparadigmatic example and show that it exhibits an unconventional Fermi\nsuperfluid. While the superfluid order parameter of this state has the same\nform as that in the Fulde-Ferrell state discussed in metallic superconductivity\nunder an external magnetic field, the former non-equilibrium pairing state is\nnot accompanied by any spin imbalance. Our proposed reservoir engineering to\nprocess the Fermi momentum distribution would provide further possibilities of\nmany-body quantum phenomena beyond the thermal equilibrium case.']"
129,10,129_hash_table_indexes_index,"['hash', 'table', 'indexes', 'index', 'columns', 'alex', 'learned', 'iceberg', 'tables', 'queries']","[""We revisit the problem of building static hash tables on the GPU and design\nand build three bucketed hash tables that use different probing schemes. Our\nimplementations are lock-free and offer efficient memory access patterns; thus,\nonly the probing scheme is the factor affecting the performance of the hash\ntable's different operations. Our results show that a bucketed cuckoo hash\ntable that uses three hash functions (BCHT) outperforms alternative methods\nthat use power-of-two choices, iceberg hashing, and a cuckoo hash table that\nuses a bucket size one. At high load factors as high as 0.99, BCHT enjoys an\naverage probe count of 1.43 during insertion. Using three hash functions only,\npositive and negative queries require at most 1.39 and 2.8 average probes per\nkey, respectively."", 'Despite being one of the oldest data structures in computer science, hash\ntables continue to be the focus of a great deal of both theoretical and\nempirical research. A central reason for this is that many of the fundamental\nproperties that one desires from a hash table are difficult to achieve\nsimultaneously; thus many variants offering different trade-offs have been\nproposed.\n  This paper introduces Iceberg hashing, a hash table that simultaneously\noffers the strongest known guarantees on a large number of core properties.\nIceberg hashing supports constant-time operations while improving on the state\nof the art for space efficiency, cache efficiency, and low failure probability.\nIceberg hashing is also the first hash table to support a load factor of up to\n$1 - o(1)$ while being stable, meaning that the position where an element is\nstored only ever changes when resizes occur. In fact, in the setting where keys\nare $\\Theta(\\log n)$ bits, the space guarantees that Iceberg hashing offers,\nnamely that is uses at most $\\log \\binom{|U|}{n} + O(n \\log \\log n)$ bits to\nstore $n$ items from a universe $U$, matches a lower bound by Demaine et al.\nthat applies to any stable hash table.\n  Iceberg hashing introduces new general-purpose techniques for some of the\nmost basic aspects of hash-table design. Notably, our indirection-free\ntechnique for dynamic resizing, which we call waterfall addressing, and our\ntechniques for achieving stability and very-high probability guarantees, can be\napplied to any hash table that makes use of the front-yard/backyard paradigm\nfor hash table design.', ""Hash table is a fundamental data structure for quick search and retrieval of\ndata. It is a key component in complex graph analytics and AI/ML applications.\nState-of-the-art parallel hash table implementations either make some\nsimplifying assumptions such as supporting only a subset of hash table\noperations or employ optimizations that lead to performance that is highly data\ndependent and in the worst case can be similar to a sequential implementation.\nIn contrast, in this work we develop a dynamic hash table that supports all the\nhash table queries - search, insert, delete, update, while allowing us to\nsupport 'p' parallel queries (p>1) per clock cycle via p processing engines\n(PEs) in the worst case i.e. the performance is data agnostic. We achieve this\nby implementing novel XOR based multi-ported block memories on FPGAs.\nAdditionally, we develop a technique to optimize the memory requirement of the\nhash table if the ratio of search to insert/update/delete queries is known\nbeforehand. We implement our design on state-of-the-art FPGA devices. Our\ndesign is scalable to 16 PEs and supports throughput up to 5926 MOPS. It\nmatches the throughput of the state-of-the-art hash table design - FASTHash,\nwhich only supports search and insert operations. Comparing with the best FPGA\ndesign that supports the same set of operations, our hash table achieves up to\n12.3x speedup.""]"
130,10,130_person_reid_modality_re,"['person', 'reid', 'modality', 're', 'id', 'clothing', 'modalities', 'mmd', 'search', 'features']","['Person reidentification (ReID) is a very hot research topic in machine\nlearning and computer vision, and many person ReID approaches have been\nproposed; however, most of these methods assume that the same person has the\nsame clothes within a short time interval, and thus their visual appearance\nmust be similar. However, in an actual surveillance environment, a given person\nhas a great probability of changing clothes after a long time span, and they\nalso often take different personal belongings with them. When the existing\nperson ReID methods are applied in this type of case, almost all of them fail.\nTo date, only a few works have focused on the cloth-changing person ReID task,\nbut since it is very difficult to extract generalized and robust features for\nrepresenting people with different clothes, their performances need to be\nimproved. Moreover, visual-semantic information is often ignored. To solve\nthese issues, in this work, a novel multigranular visual-semantic embedding\nalgorithm (MVSE) is proposed for cloth-changing person ReID, where visual\nsemantic information and human attributes are embedded into the network, and\nthe generalized features of human appearance can be well learned to effectively\nsolve the problem of clothing changes. Specifically, to fully represent a\nperson with clothing changes, a multigranular feature representation scheme\n(MGR) is employed to focus on the unchanged part of the human, and then a cloth\ndesensitization network (CDN) is designed to improve the feature robustness of\nthe approach for the person with different clothing, where different high-level\nhuman attributes are fully utilized. Moreover, to further solve the issue of\npose changes and occlusion under different camera perspectives, a partially\nsemantically aligned network (PSA) is proposed to obtain the visual-semantic\ninformation that is used to align the human attributes.', 'Person search is an extended task of person re-identification (Re-ID).\nHowever, most existing one-step person search works have not studied how to\nemploy existing advanced Re-ID models to boost the one-step person search\nperformance due to the integration of person detection and Re-ID. To address\nthis issue, we propose a faster and stronger one-step person search framework,\nthe Teacher-guided Disentangling Networks (TDN), to make the one-step person\nsearch enjoy the merits of the existing Re-ID researches. The proposed TDN can\nsignificantly boost the person search performance by transferring the advanced\nperson Re-ID knowledge to the person search model. In the proposed TDN, for\nbetter knowledge transfer from the Re-ID teacher model to the one-step person\nsearch model, we design a strong one-step person search base framework by\npartially disentangling the two subtasks. Besides, we propose a Knowledge\nTransfer Bridge module to bridge the scale gap caused by different input\nformats between the Re-ID model and one-step person search model. During\ntesting, we further propose the Ranking with Context Persons strategy to\nexploit the context information in panoramic images for better retrieval.\nExperiments on two public person search datasets demonstrate the favorable\nperformance of the proposed method.', ""Clothing changes and lack of data labels are both crucial challenges in\nperson ReID. For the former challenge, people may occur multiple times at\ndifferent locations wearing different clothing. However, most of the current\nperson ReID research works focus on the benchmarks in which a person's clothing\nis kept the same all the time. For the last challenge, some researchers try to\nmake model learn information from a labeled dataset as a source to an unlabeled\ndataset. Whereas purely unsupervised training is less used. In this paper, we\naim to solve both problems at the same time. We design a novel unsupervised\nmodel, Sync-Person-Cloud ReID, to solve the unsupervised clothing change person\nReID problem. We developer a purely unsupervised clothing change person ReID\npipeline with person sync augmentation operation and same person feature\nrestriction. The person sync augmentation is to supply additional same person\nresources. These same person's resources can be used as part supervised input\nby same person feature restriction. The extensive experiments on clothing\nchange ReID datasets show the out-performance of our methods.""]"
